[
["index.html", "Intro to Data Science for the Social Sector Welcome", " Intro to Data Science for the Social Sector Updated September 10, 2018 Welcome Welcome to the course text for Data Science for the Social Sector. This course introduces students to the field of data science and its applications in the public sector. Modern performance management and evaluation processes require strong data literacy and the ability to combine and analyze data from a variety of sources to inform managerial processes. We offer a practical, tools-based approach that is designed to build strong foundations for people that want to work as policy analysts or data-driven managers. We will cover data programming fundamentals, visualization, text analysis, automated reporting, and dynamic reporting using dashboards. The course is analytically rigorous, but no prior programming experience is assumed. The Data Science Process (Wickham &amp; Grolemund, 2016): A data scientist is a person who should be able to leverage existing data sources, and create new ones as needed in order to extract meaningful information and actionable insights. These insights can be used to drive business decisions and changes intended to achieve business goals… ‘The Perfect Data Scientist’ is the individual who is equally strong in business, programming, statistics, and communication. From: “What Is Data Science, and What Does a Data Scientist Do?” "],
["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 Navigation 1.2 Commenting Code 1.3 Help 1.4 Install Programs (packages) 1.5 Accessing Built-In Datasets in R", " Chapter 1 Introduction to R This lecture introduces you to basic operations when you first start using R such as navigation, the object-oriented framework, loading a package, and creating some data vectors. 1.1 Navigation You need to know a few operations to help you maneuver the R work environment, such as listing objects (datasets and functions) that are active, changing your working directory, listing available files, and finding help. 1.1.1 Setting Your Working Directory When you are ready to load data, R needs to know where to look for your files. You can check what is avaiable in the current directory (i.e. folder) by asking to list all of the current files using dir(). dir() If the file that you need is located in a different folder, you can change directories easily in R Studio by Session -&gt; Set working director -&gt; Choose directory (or Ctrl + Shift + H). If you are writing a script, you want to keep track of this step so that it can be reproduced. Use the function get.wd() to check your current working directory, and set.wd() to change. You need to specify your path as an argument to this function, such as. setwd( &quot;C:/user/projects/financial model&quot; ) NOTE! R uses unix style notation with forward slashes, so if you copy and paste from Windows it will look like this, with back slashes: setwd( &quot;C:\\user\\projects\\financial model&quot; ) You will need to change them around for it to work. It is best to save all of your steps in your scripts so that the analysis can be reproduced by yourself or others. In some cases you are doing exploratory or summary work, and you may want to find a file a quickly. You can use the file.choose() function to open a GUI to select your file directly. This function is used as an argument inside of a load data function. my.dat &lt;- read.csv( file.choose() ) 1.2 Commenting Code Most computer languages have a special character that is used to “comment out” lines so that it is not run by the program. It is used for two important purposes. First, we can add text to document our functions and it will not interfere with the program. And two, we can use it to run a program while ignoring some of the code, often for debugging purposes. The # hash tag is used for comments in R. ##============================================== ## ## Here is some documentation for this script ## ##============================================== x &lt;- 1:10 sum( x ) ### [1] 55 # y &lt;- 1:25 # not run # sum( y ) # not run 1.3 Help You will use the help functions frequently to figure out what arguments and values are needed for specific functions. Because R is very customizable, you will find that many functions have several or dozens of arguments, and it is difficult to remember the correct syntax and values. But don’t worry, to look them up all you need is the function name and a call for help: help( dotchart ) # opens an external helpfile If you just need to remind yourself which arguments are defined in a function, you can use the args() command: args( dotchart ) ### function (x, labels = NULL, groups = NULL, gdata = NULL, cex = par(&quot;cex&quot;), ### pt.cex = cex, pch = 21, gpch = 21, bg = par(&quot;bg&quot;), color = par(&quot;fg&quot;), ### gcolor = par(&quot;fg&quot;), lcolor = &quot;gray&quot;, xlim = range(x[is.finite(x)]), ### main = NULL, xlab = NULL, ylab = NULL, ...) ### NULL If you can’t recall a function name, you can list all of the functions from a specific package as follows: help( package=“stats” ) # lists all functions in stats package 1.4 Install Programs (packages) When you open R by default it will launch a core set of programs, called “packages” in R speak, that are use for most data operations. To see which packages are currently active use the search() function. search() ### [1] &quot;.GlobalEnv&quot; &quot;package:maps&quot; &quot;package:ggplot2&quot; ### [4] &quot;package:reshape2&quot; &quot;package:tidyr&quot; &quot;package:ggvis&quot; ### [7] &quot;package:skimr&quot; &quot;package:stargazer&quot; &quot;package:Lahman&quot; ### [10] &quot;package:bindrcpp&quot; &quot;package:scales&quot; &quot;package:pander&quot; ### [13] &quot;package:dplyr&quot; &quot;package:bookdown&quot; &quot;package:rmarkdown&quot; ### [16] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ### [19] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ### [22] &quot;Autoloads&quot; &quot;package:base&quot; These programs manage the basic data operations, run the core graphics engine, and give you basic statistical methods. The real magic for R comes from the over 7,000 contributed packages available on the CRAN: https://cran.r-project.org/web/views/ A package consists of custom functions and datasets that are generated by users. They are packaged together so that they can be shared with others. A package also includes documentation that describes each function, defines all of the arguments, and documents any datasets that are included. If you know a package name, it is easy to install. In R Studio you can select Tools -&gt; Install Packages and a list of available packages will be generated. But it is easier to use the install.packages() command. We will use the Lahman Package in this course, so let’s install that now. Description This package provides the tables from Sean Lahman’s Baseball Database as a set of R data.frames. It uses the data on pitching, hitting and fielding performance and other tables from 1871 through 2013, as recorded in the 2014 version of the database. See the documentation here: https://cran.r-project.org/web/packages/Lahman/Lahman.pdf install.packages( &quot;Lahman&quot; ) You will be asked to select a “mirror”. In R speak this just means the server from which you will download the package (choose anything nearby). R is a community of developers and universities that create code and maintain the infrastructure. A couple of dozen universities around the world host servers that contain copies of the R packages so that they can be easily accessed everywhere. If the package is successfully installed you will get a message similar to this: package ‘Lahman’ successfully unpacked and MD5 sums checked Once a new program is installed you can now open (“load” in R speak) the package using the library() command: library( &quot;Lahman&quot; ) If you now type search() you can see that Lahman has been added to the list of active programs. We can now access all of the functions and data that are available in the Lahman package. 1.5 Accessing Built-In Datasets in R One nice feature of R is that is comes with a bunch of built-in datasets that have been contributed by users are are loaded automatically. You can see the list of available datasets by typing: data() This will list all of the default datasets in core R packages. If you want to see all of the datasets available in installed packages as well use: data( package = .packages(all.available = TRUE) ) 1.5.1 Basic Data Operations Let’s ignore the underlying data structure right now and look at some ways that we might interact with data. We will use the USArrests dataset available in the core files. To access the data we need to load it into working memory. Anything that is active in R will be listed in the environment, which you can check using the ls() command. We will load the dataset using the data() command. remove( list=ls() ) ls() # nothing currently available ### character(0) data( &quot;USArrests&quot; ) ls() # data is now avaible for use ### [1] &quot;USArrests&quot; Now that we have loaded a dataset, we can start to access the variables and analyze relationships. Let’s get to know our dataset. names( USArrests ) # what variables are in the dataset? ### [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; nrow( USArrests ) # how many observations are there? ### [1] 50 dim( USArrests ) # a quick way to see rows and columns - the dimensions of the dataset ### [1] 50 4 row.names( head( USArrests ) ) # what are the obsevations (rows) in our data ### [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; ### [6] &quot;Colorado&quot; summary( USArrests ) # summary statistics of variables ### Murder Assault UrbanPop Rape ### Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 ### 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 ### Median : 7.250 Median :159.0 Median :66.00 Median :20.10 ### Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 ### 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 ### Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 We can see that the dataset consists of four variables: Murder, Assault, UrbanPop, and Rape. We also see that our unit of analysis is the state. But where does the data come from, and how are these variables measured? To see the documentation for a specific dataset you will need to use the help() function: help( &quot;USArrests&quot; ) We get valuable information about the source and metrics: Description This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas. Format A data frame with 50 observations on 4 variables. Murder: numeric Murder arrests (per 100,000) Assault: numeric Assault arrests (per 100,000) UrbanPop: numeric Percent urban population Rape: numeric Rape arrests (per 100,000) To access a specific variable inside of a dataset, you will use the $ operator between the dataset name and the variable name: summary( USArrests$Murder ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 0.800 4.075 7.250 7.788 11.250 17.400 summary( USArrests$Assault ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 45.0 109.0 159.0 170.8 249.0 337.0 # Is there a relationship between urban density and crime? plot( USArrests$UrbanPop, USArrests$Murder ) abline( lm( USArrests$Murder ~ USArrests$UrbanPop ), col=&quot;red&quot; ) 1.5.2 Using the Lahman Data Let’s take a look at some of the data available in the Lahman package. data( package = &quot;Lahman&quot; ) # All datasets in package ‘Lahman’: TABLE NAME DEFITION AllstarFull AllstarFull table Appearances Appearances table AwardsManagers AwardsManagers table AwardsPlayers AwardsPlayers table AwardsShareManagers AwardsShareManagers table AwardsSharePlayers AwardsSharePlayers table Batting Batting table BattingPost BattingPost table CollegePlaying CollegePlaying table Fielding Fielding table FieldingOF FieldingOF table FieldingPost FieldingPost data HallOfFame Hall of Fame Voting Data LahmanData Lahman Datasets Managers Managers table ManagersHalf ManagersHalf table Master Master table Pitching Pitching table PitchingPost PitchingPost table Salaries Salaries table Schools Schools table SeriesPost SeriesPost table Teams Teams table TeamsFranchises TeamFranchises table TeamsHalf TeamsHalf table battingLabels Variable Labels fieldingLabels Variable Labels pitchingLabels Variable Labels We see that we have lots of datasets to choose from here. I will use the Master dataset, which is a list of all of the Major League Baseball players over the past century, and their personal information. library( Lahman ) # loads Lahman package data( Master ) head( Master ) Here are some common functions for exploring datasets: names( Master ) # variable names nrow( Master ) # 18,354 players included summary( Master ) # descriptive statistics of variables We can use help(Master) to get information about the dataset, including a data dictionary. help( Master ) MASTER TABLE Description Master table - Player names, DOB, and biographical info. This file is to be used to get details about players listed in the Batting, Pitching, and other files where players are identified only by playerID. Usage data(Master) Format A data frame with 19105 observations on the following 26 variables. playerID: A unique code asssigned to each player. The playerID links the data in this file with records on players in the other files. birthYear: Year player was born birthMonth: Month player was born birthDay: Day player was born birthCountry: Country where player was born birthState: State where player was born birthCity: City where player was born deathYear: Year player died deathMonth: Month player died deathDay: Day player died deathCountry: Country where player died deathState: State where player died deathCity: City where player died nameFirst: Player’s first name nameLast: Player’s last name nameGiven: Player’s given name (typically first and middle) weight: Player’s weight in pounds height: Player’s height in inches bats: a factor: Player’s batting hand (left (L), right (R), or both (B)) throws: a factor: Player’s throwing hand (left(L) or right(R)) debut: Date that player made first major league appearance finalGame: Date that player made first major league appearance (blank if still active) retroID: ID used by retrosheet, http://www.retrosheet.org/ bbrefID: ID used by Baseball Reference website, http://www.baseball-reference.com/ birthDate: Player’s birthdate, in as.Date format deathDate: Player’s deathdate, in as.Date format Details debut, finalGame were converted from character strings with as.Date. Source Lahman, S. (2016) Lahman’s Baseball Database, 1871-2015, 2015 version, http://www.seanlahman.com/baseball-archive/statistics/ 1.5.3 Example Analysis Perhaps I am curious about some of the data. I see that we have information on the birth month of professional baseball players. If you have read Malcolm Gladwell’s book Outliers you know there is an interesting cumulative advantage phenomenon that can occur with atheletes as they are young. If you are born near the end of the cutoff, you are on average six months older than other players in your league, and therefore slightly larger physically and more coordinated on average. Six months does not sound like much, but the slight size and coordination advantage means more playing time, which also improves skill. Over time, this small difference accumulates so that those lucky enough to be born near the cutoff become the best players. Gladwell looked at studies of hockey. Do we see this in baseball? table( Master$birthMonth ) tab &lt;- prop.table( table( Master$birthMonth ) ) names(tab) &lt;- c(&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;,&quot;Dec&quot;) dotchart( tab, pch=19, xlab = &quot;Percent of Players&quot;, ylab = &quot;Birth Month&quot; ) "],
["functions.html", "Chapter 2 Functions 2.1 Key Concepts 2.2 Computer Programs as Recipes 2.3 Example Function 2.4 Default Argument Values 2.5 Assignment", " Chapter 2 Functions 2.1 Key Concepts Figure 2.1: Anatomy of a function Figure 2.2: Assignment of output values After reading this chapter you should be able to define the following: function argument object assignment 2.2 Computer Programs as Recipes Computer programs are powerful because they allow us to codify recipes for complex tasks, save them, share them, and build upon them. In the simplest form, a computer program is like a recipe. We have inputs, steps, and outputs. Ingredients: 1/3 cup butter 1/2 cup sugar 1/4 cup brown sugar 2 teaspoons vanilla extract 1 large egg 2 cups all-purpose flour 1/2 teaspoon baking soda 1/2 teaspoon kosher salt 1 cup chocolate chips Instructions: Preheat the oven to 375 degrees F. In a large bowl, mix butter with the sugars until well-combined. Stir in vanilla and egg until incorporated. Addflour, baking soda, and salt. Stir in chocolate chips. Bake for 10 minutes. In R, the recipe would look something like this: function( butter=0.33, sugar=0.5, eggs=1, flour=2, temp=375 ) { dry.goods &lt;- combine( flour, sugar ) batter &lt;- mix( dry.goods, butter, eggs ) cookies &lt;- bake( batter, temp, time=10 ) return( cookies ) } Note that this function to make cookies relies on other functions for each step, combine(), mix(), and bake(). Each of these functions would have to be defined as well, or more likely someone else in the open source community has already written a package called “baking” that contains simple functions for cooking so that you can use them for more complicated recipes. You will find that R allows you to conduct powerful analysis primarily because you can build on top of and extend a lot of existing functionality. 2.3 Example Function As you get started in R you will be working with existing functions, not writing your own. It is, however, constructive to see how one is created. This example demonstrates the use of a mortgage calculator that will take a loan size, term, and interest rate and return a monthly payment. calcMortgage &lt;- function( principal, years, APR ) { months &lt;- years * 12 int.rate &lt;- APR / 12 # amortization formula monthly.payment &lt;- ( principal * int.rate ) / (1 - (1 + int.rate)^(-months) ) monthly.payment &lt;- round( monthly.payment, 2 ) return( monthly.payment ) } Let’s then see what the payments will be for a: $100,000 loan 30-year mortgage 5% annual interest rate calcMortgage( principal=100000, years=30, APR=0.05 ) ### [1] 536.82 2.4 Default Argument Values Note that the loan function needs all three of the input values in order to calculate the loan size. If we were to omit one required value, we would get an error. calcMortgage( principal=100000 ) # Error in calcMortgage(APR = 0.05, principal = 1e+05): # argument &quot;years&quot; is missing, with no default When creating functions, we might have a good idea of typical use cases. If true, we can try to guess at reasonable user parameters. For example, perhaps we are working at a bank where most of the customers take out 30-year mortgages, and interest rates have been stable at 5 percent. We can set these as default values when we create the function. calcMortgage &lt;- function( principal, years=30, APR=0.05 ) ... We can now run the function while omitting arguments, as long as they have defaults assigned. calcMortgage( principal=100000 ) ### [1] 536.82 2.5 Assignment When we call a function in R, the default behavior of the function is typically to print the results on the screen: calcMortgage( principal=100000 ) ### [1] 536.82 If we are creating a script, however, we often need to save the function outputs at each step. We can do this by assigning output to a new variable. payments.15.year &lt;- calcMortgage( years=15, principal=100000 ) payments.30.year &lt;- calcMortgage( years=30, principal=100000 ) These values are then stored, and can be used later or printed by typing the object name: payments.15.year ### [1] 790.79 payments.30.year ### [1] 536.82 Note that variable names can include periods or underscores. They can also include numbers, but they cannot start with a number. Like everything in R, they will be case sensitive. "],
["vectors.html", "VECTORS", " VECTORS Vectors are the building blocks of data programming in R, so they are extremely important concepts. This section will cover basic principles of working with vectors in the R language, including the different types of vectors (data types or classes), and common functions used on vectors. "],
["data-types.html", "Chapter 3 Data Types 3.1 Key Concepts 3.2 Vectors 3.3 Common Vectors Functions 3.4 The Combine Function 3.5 Casting 3.6 Numeric Vectors 3.7 Character Vectors 3.8 Factors 3.9 Logical Vectors 3.10 Generating Vectors 3.11 Variable Transformations 3.12 Missing Values: NA’s 3.13 Datasets", " Chapter 3 Data Types 3.1 Key Concepts Figure 3.1: Components of a Vector Figure 3.2: Basic data types in R 3.2 Vectors Generally speaking a vector is a set of numbers, words, or other values stored sequentially: [ 1, 2, 3] [ apple, orange, pear ] [ TRUE, FALSE, FALSE ] In social sciences, a vector usually represents a variable in a dataset, often as a column in a spreadsheet. There are four primary vector types (“classes”) in R: Class Description numeric Typical variable of only numbers character A vector of letters or words, always enclosed with quotes factor Categories which represent groups, like treatment and control logical A vector of TRUE and FALSE to designate which observations fit a criteria Each vector or dataset has a “class” that tells R the data type. These different vectors can be combined into three different types of datasets (data frames, matrices, and lists), which will be discussed below. x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) x4 &lt;- c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) x5 &lt;- x4 == &quot;treatment&quot; dat &lt;- data.frame( name=x2, sex=x3, treat=x4, is.treat=x5, strength=x1 ) name sex treat is.treat strength adam male treatment TRUE 167 jamal male control FALSE 185 linda female treatment TRUE 119 sriti female control FALSE 142 R keeps track of the data type of each object, which can be ascertained using the class() function. class( x ) ### $name ### [1] &quot;character&quot; ### ### $sex ### [1] &quot;factor&quot; ### ### $treat ### [1] &quot;character&quot; ### ### $is.treat ### [1] &quot;logical&quot; ### ### $strength ### [1] &quot;numeric&quot; class( dat ) ### [1] &quot;data.frame&quot; 3.3 Common Vectors Functions You will spend a lot of time creating data vectors, transforming variables, generating subsets, cleaning data, and adding new observations. These are all accomplished through functions() that act on vectors. We often need to know how many elements belong to a vector, which we find with the length() function. x1 ### [1] 167 185 119 142 length( x1 ) ### [1] 4 3.4 The Combine Function We often need to combine several elements into a single vector, or combine two vectors to form one. This is done using the c() function. c(1,2,3) # create a numeric vector ### [1] 1 2 3 c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) # create a character vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Combining two vectors: x &lt;- 1:5 y &lt;- 10:15 z &lt;- c(x,y) z ### [1] 1 2 3 4 5 10 11 12 13 14 15 Combining two vectors of different data types: x &lt;- c(1,2,3) y &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) z &lt;- c(x,y) z ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; 3.5 Casting You can easily move from one data type to another by casting a specific type as another type: x &lt;- 1:5 x ### [1] 1 2 3 4 5 as.character(x) ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; y &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) y ### [1] TRUE FALSE TRUE TRUE FALSE as.numeric( y ) ### [1] 1 0 1 1 0 as.character( y ) ### [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; &quot;TRUE&quot; &quot;FALSE&quot; But in some cases it might not make sense to cast one variable type as another. z &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) z ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; as.numeric( z ) ### [1] NA NA NA Casting will often be induced automatically when you try to combine different types of data. For example, when you add a character element to a numeric vector, the whole vector will be cast as a character vector. x1 &lt;- 1:5 x1 ### [1] 1 2 3 4 5 x1 &lt;- c( x1, &quot;a&quot; ) # a vector can only have one data type x1 # all numbers silently recast as characters ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;a&quot; If you consider the example above, when a numeric and character vector are combined all elements are re-cast as strings because numbers can be represented as characters but not vice-versa. R tries to select a reasonable default type, but sometimes casting will create some strange and unexpected behaviors. Consider some of these examples. What do you think each will produce? x1 &lt;- c(1,2,3) # numeric x2 &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) # character x3 &lt;- c(TRUE,FALSE,TRUE) # logical x4 &lt;- factor( c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) ) # factor case1 &lt;- c( x1, x3 ) case2 &lt;- c( x2, x3 ) case3 &lt;- c( x1, x4 ) case4 &lt;- c( x2, x4 ) The answers to case1 and case2 are somewhat intuitive. case1 # combine a numeric and logical vector ### [1] 1 2 3 1 0 1 case2 # combine a character and logical vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; Recall that TRUE and FALSE are often represented as 1 and 0 in datasets, so they can be recast as numeric elements. The numbers 2 and 3 have no meaning in a logical vector, so we can’t cast a numeric vector as a logical vector. case3 and case4 are a little more nuanced. See the section on factors below to make sense of them. case3 # combine a numeric and factor vector ### [1] 1 2 3 1 2 3 case4 # combine a character and factor vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; TIP: When you read data in from outside sources, the input functions often will cast character or numeric vectors as factors if they contain a low number of elements. See the section on factors below for special instructions on moving from factors to numeric vectors. 3.6 Numeric Vectors There are some specific things to note about each vector type. Math operators will only work on numeric vectors. summary( x1 ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 1.0 1.5 2.0 2.0 2.5 3.0 Note that if we try to run this mathematical function we get an error: sum( x2 ) # Error in sum(x2) : invalid &#39;type&#39; (character) of argument Many functions in R are sensitive to the data type of vectors. Mathematical functions, for example, do not make sense when applied to text (character vectors). In many cases R will give an error. In some cases R will silently re-cast the variable, then perform the operation. Be watchful for when silent re-casting occurs because it might have unwanted side effects, such as deleting data or re-coding group levels in the wrong way. 3.6.1 Integers Are Simple Numeric Vectors The integer vector is a special type of numeric vector. It is used to save memory since integers require less space than numbers that contain decimals points (you need to allocate space for the numbers to the left and the numbers to the right of the decimal). Google “computer memory allocation” if you are interested in the specifics. If you are doing advanced programming you will be more sensitive to memory allocation and the speed of your code, but in the intro class we will not differentiate between the two types of number vectors. In most cases they result in the same results, unless you are doing advanced numerical analysis where rounding errors matter. n &lt;- 1:5 n ### [1] 1 2 3 4 5 class( n ) ### [1] &quot;integer&quot; n[ 2 ] &lt;- 2.01 n # all elements converted to decimals ### [1] 1.00 2.01 3.00 4.00 5.00 class( n ) ### [1] &quot;numeric&quot; 3.7 Character Vectors The most important rule to remember with this data type: when creating character vectors, all text must be enclosed by quotation marks. This one works: c( &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ) # this works ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; This one will not: c( a, b, c ) # Error: object &#39;a&#39; not found When you type characters surrounded by quotes then R knows you are creating new text (“strings” in programming speak). When you type characters that are not surrounded by quotes, R thinks that you are looking for an object in the environment, like the variables we have already created. It gets confused when it doesn’t find the object that you typed. In generate, you will use quotes when you are creating character vectors, and for arguments in functions. You do not use quotes when you are referencing an active object. An active object is typically a dataset or vector that you have imported or created. You can print a list of all active objects with the ls() function. 3.7.1 Quotes in Arguments When you first start using R it can be confusing about when quotes are needed around arguments. Take the following example of the color argument (col=) in the plot() function. strength &lt;- c(167,185,119,142) name &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) group &lt;- factor( c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) ) plot( strength, col=&quot;blue&quot;, ... ) plot( strength, col=group, ... ) In the first example we are using a text string as an argument to specify a color (col=&quot;blue&quot;), so it must be enclosed by quotes because it is text. In the second example R selects the color based upon group membership specified by the factor called ‘group’ (treatment or control). Since the argument is now referencing an object (col=group), we do not use quotes. The exception here is when your argument requires a number. Numbers are not passed with quotes, or they would be cast as text. 3.8 Factors When there are categorical variables within our data, or groups, then we use a special vector to keep track of these groups. We could just use numbers (1=female, 0=male) or characters (“male”,“female”), but factors are useful for two reasons. First, it saves memory. Text is very “expensive” in terms of memory allocation and processing speed, so using simpler data structure makes R faster. Second, when a variable is set as a factor, R recognizes that it represents a group and it can deploy object-oriented functionality. When you use a factor in analysis, R knows that you want to split the analysis up by groups. height &lt;- c( 70, 68, 62, 64, 72, 69, 58, 63 ) strength &lt;- c(167,185,119,142,175,204,124,117) sex &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot; ) ) plot( height, strength ) # simple scatter plot plot( sex, strength ) # box and whisker plot Factors are more memory efficient than character vectors because they store the underlying data as a numeric vector instead of a categorical (text) vector. Each group in the data is assigned a number, and when printing items the program only has to remember which group corresponds to which number: sex ### [1] male male female female male male female female ### Levels: female male as.numeric( sex ) ### [1] 2 2 1 1 2 2 1 1 # male = 2 # female = 1 If you print a factor, the computer just replaces each category designation with its name (2 would be replaced with “male” in this example). These replacements can be done in real time without clogging the memory of your computer as they don’t need to be saved. In some instances a categorical variable might be represented by numbers. For example, grades 9-12 for high school kids. The very important rule to remember with factors is you can’t move directly from the factor to numeric using the as.numeric() casting function. This will give you the underlying data structure, but will not give you the category names. To get these, you need the as.character casting function. grades &lt;- sample( x=9:12, size=10, replace=T ) grades ### [1] 11 9 10 12 12 9 11 9 10 9 grades &lt;- as.factor( grades ) grades ### [1] 11 9 10 12 12 9 11 9 10 9 ### Levels: 9 10 11 12 as.numeric( grades ) ### [1] 3 1 2 4 4 1 3 1 2 1 as.character( grades ) ### [1] &quot;11&quot; &quot;9&quot; &quot;10&quot; &quot;12&quot; &quot;12&quot; &quot;9&quot; &quot;11&quot; &quot;9&quot; &quot;10&quot; &quot;9&quot; # to get back to the original numeric vector as.numeric( as.character( grades )) ### [1] 11 9 10 12 12 9 11 9 10 9 Note that when sub-setting a factor, it will retain all of the original levels, even when they are not in use. In this example, there are 37 teams in the Lahman dataset (some of them defunct) and 16 teams in the National League in 2002. But after applying the year and league subsets you will still have 37 levels. # there are only 16 teams in the NL in 2002 sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; After applying a subset, in order to remove the unused factor levels you need to apply either droplevels(), or else recast your factor as a new factor. For example: sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; # fix in one of two equivalent ways: # # nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) # nl.sals$teamID &lt;- factor( nl.sals$teamID ) levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) levels( nl.sals$teamID ) ### [1] &quot;ARI&quot; &quot;ATL&quot; &quot;CHN&quot; &quot;CIN&quot; &quot;COL&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;LAN&quot; &quot;MIL&quot; &quot;MON&quot; &quot;NYN&quot; ### [12] &quot;PHI&quot; &quot;PIT&quot; &quot;SDN&quot; &quot;SFN&quot; &quot;SLN&quot; TIP: When reading data from Excel spreadsheets (usually saved in the comma separated value or CSV format), remember to include the following argument to prevent the creation of factors, which can produce some annoying behaviors. dat &lt;- read.csv( &quot;filename.csv&quot;, stringsAsFactors=F ) 3.9 Logical Vectors Logical vectors are collections of a set of TRUE and FALSE statements. Logical statements allow us to define groups based upon criteria, then decide whether observations belong to the group. A logical statement is one that contains a logical operator, and returns only TRUE, FALSE, or NA values. Logical vectors are important because organizing data into these sets is what drives all of the advanced data analytics (set theory is at the basis of mathematics and computer science). name sex treat strength adam male treatment 167 jamal male control 185 linda female treatment 119 sriti female control 142 dat$name == &quot;sriti&quot; ### [1] FALSE FALSE FALSE TRUE dat$sex == &quot;male&quot; ### [1] TRUE TRUE FALSE FALSE dat$strength &gt; 180 ### [1] FALSE TRUE FALSE FALSE When defining logical vectors, you can use the abbreviated versions of T for TRUE and F for FALSE. z1 &lt;- c(T,T,F,T,F,F) z1 ### [1] TRUE TRUE FALSE TRUE FALSE FALSE Typically logical vectors are used in combination with subset operators to identify specific groups in the data. # isolate data on all of the females in the dataset dat[ dat$sex == &quot;female&quot; , ] ### name sex treat strength ### 3 linda female treatment 119 ### 4 sriti female control 142 See the next chapter for more details on subsets. 3.10 Generating Vectors You will often need to generate vectors for data transformations or simulations. Here are the most common functions that will be helpful. # repeat a number, or series of numbers rep( x=9, times=5 ) ### [1] 9 9 9 9 9 rep( x=c(5,7), times=5 ) ### [1] 5 7 5 7 5 7 5 7 5 7 rep( x=c(5,7), each=5 ) ### [1] 5 5 5 5 5 7 7 7 7 7 rep( x=c(&quot;treatment&quot;,&quot;control&quot;), each=5 ) # also works to create categories ### [1] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; ### [6] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; # create a sequence of numbers seq( from=1, to=15, by=1 ) ### [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 seq( from=1, to=15, by=3 ) ### [1] 1 4 7 10 13 1:15 # shorthand if by=1 ### [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # create a random sample hat &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;f&quot;) sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;f&quot; &quot;a&quot; &quot;c&quot; sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;b&quot; &quot;f&quot; &quot;b&quot; sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;a&quot; &quot;f&quot; &quot;c&quot; # for multiple samples use replacement sample( x=hat, size=10, replace=TRUE ) ### [1] &quot;f&quot; &quot;f&quot; &quot;c&quot; &quot;f&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; # create data that follows a normal curve iq &lt;- rnorm( n=1000, mean=100, sd=15 ) hist( iq, col=&quot;gray&quot; ) 3.11 Variable Transformations When we create a new variable from existing variables, it is called a ‘transformation’. This is very common in data science. Crime is measures by the number of assaults per 100,000 people, for example (crime / pop). A batting average is the number of hits divided by the number of at bats. In R, mathematical operations are vectorized, which means that operations are performed on the entire vector all at once. This makes transformations fast and easy. x &lt;- 1:10 x + 5 ### [1] 6 7 8 9 10 11 12 13 14 15 x * 5 ### [1] 5 10 15 20 25 30 35 40 45 50 R uses a convention called “recycling”, which means that it will re-use elements of a vector if necessary. In the example below the x vector has 10 elements, but the y vector only has 5 elements. When we run out of y, we just start over from the beginning. This is powerful in some instances, but can be dangerous in others if you don’t realize that that it is happening. x &lt;- 1:10 y &lt;- 1:5 x + y ### [1] 2 4 6 8 10 7 9 11 13 15 x * y ### [1] 1 4 9 16 25 6 14 24 36 50 # the colors are recycled plot( 1:5, 1:5, col=c(&quot;red&quot;,&quot;blue&quot;), pch=19, cex=3 ) Here is an example of recycling gone wrong: x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x4 &lt;- c( &quot;treatment&quot;,&quot;contro&quot;,&quot;treatment&quot;,&quot;control&quot; ) dat &lt;- data.frame( name=x2, sex=x3, treat=x4, strength=x1 ) # create a subset of data of all female study participants dat$sex == &quot;female&quot; ### [1] FALSE FALSE TRUE TRUE these &lt;- dat$sex == &quot;female&quot; dat[ these, ] # correct subset ### name sex treat strength ### 3 linda female treatment 119 ### 4 sriti female control 142 # same thing, but i mess is up - the female element is recycled in the overwrite dat$sex = &quot;female&quot; # whoops just over-wrote my data! should be double equal these &lt;- dat$sex == &quot;female&quot; dat[ these , ] ### name sex treat strength ### 1 adam female treatment 167 ### 2 jamal female contro 185 ### 3 linda female treatment 119 ### 4 sriti female control 142 3.12 Missing Values: NA’s Missing values are coded differently in each data analysis program. SPSS uses a period, for example. In R, missing values are coded as “NA”. The important thing to note is that R wants to make sure you know there are missing values if you are conducting analysis. As a result, it will give you the answer of “NA” when you try to do math with a vector that includes a missing value. You have to ask it explicitly to ignore the missing value. x5 &lt;- c( 1, 2, 3, 4 ) x5 ### [1] 1 2 3 4 sum( x5 ) ### [1] 10 mean( x5 ) ### [1] 2.5 x5 &lt;- c( 1, 2, NA, 4 ) x5 ### [1] 1 2 NA 4 # should missing values be treated as zeros or dropped? sum( x5 ) ### [1] NA mean( x5 ) ### [1] NA sum( x5, na.rm=T ) # na.rm=T argument drops missing values ### [1] 7 mean( x5, na.rm=T ) # na.rm=T argument drops missing values ### [1] 2.333333 You cannot use the == operator to identify missing values in a dataset. There is a special is.na() function to locate all of the missing values in a vector. x5 ### [1] 1 2 NA 4 x5 == NA # this does not do what you want ### [1] NA NA NA NA is.na( x5 ) # much better ### [1] FALSE FALSE TRUE FALSE ! is.na( x5 ) # if you want to create a selector vector to drop missing values ### [1] TRUE TRUE FALSE TRUE x5[ ! is.na(x5) ] ### [1] 1 2 4 x5[ is.na(x5) ] &lt;- 0 # replace missing values with zero 3.13 Datasets When multiple vectors represent data from a single sample, they are typically combined into a dataset. There are three main types of datasets that we will use in this class. Class Description data frame A typical data set comprised of several variables matrix A data set comprised of only numbers, used for matrix math list The grab bag of data structures - several vectors held together 3.13.1 Data Frames The most familiar spreadsheet-type data structure is called a data frame in R. It consists of rows, which represent observations, and columns, which represent variables. data( USArrests ) dim( USArrests ) # number of rows by number of columns ### [1] 50 4 names( USArrests ) # variable names or column names ### [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; row.names( USArrests ) ### [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ### [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; ### [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; ### [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ### [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; ### [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; ### [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; ### [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; ### [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ### [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; ### [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; ### [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; ### [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; head( USArrests ) # print first six rows of the data ### Murder Assault UrbanPop Rape ### Alabama 13.2 236 58 21.2 ### Alaska 10.0 263 48 44.5 ### Arizona 8.1 294 80 31.0 ### Arkansas 8.8 190 50 19.5 ### California 9.0 276 91 40.6 ### Colorado 7.9 204 78 38.7 3.13.2 Matrices A matrix is also a rectangular data object that consists of collections of vectors, but it is special in the sense that it only has numeric vectors and no variable names. mat &lt;- matrix( 1:20, nrow=5 ) mat ### [,1] [,2] [,3] [,4] ### [1,] 1 6 11 16 ### [2,] 2 7 12 17 ### [3,] 3 8 13 18 ### [4,] 4 9 14 19 ### [5,] 5 10 15 20 names( mat ) ### NULL dim( mat ) ### [1] 5 4 as.data.frame( mat ) # creates variable names ### V1 V2 V3 V4 ### 1 1 6 11 16 ### 2 2 7 12 17 ### 3 3 8 13 18 ### 4 4 9 14 19 ### 5 5 10 15 20 These are used almost exclusively for matrix algebra operations, which are fundamental to mathematical statistics. We will not use matrices in this course. 3.13.3 Lists The list is the most flexible data structure. It is created by sticking a bunch of unrelated vectors or datasets together. For example, when you run a regression you generate a bunch of interesting information. This information is saved as a list. x &lt;- 1:100 y &lt;- 2*x + rnorm( 100, 0, 10) m.01 &lt;- lm( y ~ x ) names( m.01 ) ### [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ### [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ### [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; m.01$coefficients ### (Intercept) x ### -0.9362161 2.0213077 m.01$residuals ### 1 2 3 4 5 6 ### -8.3875752 1.0181768 -2.4214285 -2.4483039 -19.2465148 -8.8284056 ### 7 8 9 10 11 12 ### -10.2519094 -7.0106545 3.5941154 -3.2142271 8.8546563 7.9061131 ### 13 14 15 16 17 18 ### 0.3326668 18.1313099 14.9453399 -10.6530265 0.6281780 -0.8942993 ### 19 20 21 22 23 24 ### 18.3402836 -15.1167900 -10.3303517 21.3789646 5.3369067 -17.4744280 ### 25 26 27 28 29 30 ### -4.5531498 0.7289505 9.2013568 -7.4731564 -6.2985462 7.1097706 ### 31 32 33 34 35 36 ### 12.2027234 -5.3961302 -0.3805555 5.9211272 11.8225322 14.1559171 ### 37 38 39 40 41 42 ### 3.9876217 5.4280754 -24.4390667 7.3613538 -2.3940344 -1.1606768 ### 43 44 45 46 47 48 ### 0.5407144 3.9878019 12.8595187 -13.0265375 0.8198567 -3.2099075 ### 49 50 51 52 53 54 ### -1.7838466 -11.0521456 2.7377232 17.7900021 5.2203449 4.4017586 ### 55 56 57 58 59 60 ### 3.6867389 15.8357735 -11.7083430 -18.8093574 -5.6782256 -16.1119781 ### 61 62 63 64 65 66 ### 10.3139383 6.7859675 3.5007625 -16.7609657 9.1213619 5.2162334 ### 67 68 69 70 71 72 ### 3.2935996 -9.0946826 -3.0339111 9.1942549 5.0416238 0.1974878 ### 73 74 75 76 77 78 ### 17.6893574 -11.9757249 7.4621632 -4.5988461 9.0349718 -22.4424810 ### 79 80 81 82 83 84 ### 5.6397995 10.2346179 8.9636728 -9.8245592 -8.8718576 -11.4299797 ### 85 86 87 88 89 90 ### -18.2178957 3.0373671 14.7205506 -7.1494402 -4.9101638 -21.2454113 ### 91 92 93 94 95 96 ### -7.7326962 5.2042157 -3.6713248 -6.1768795 29.7516377 -2.4582717 ### 97 98 99 100 ### -8.2679878 7.7935447 1.5174345 7.6357152 m.01$call ### lm(formula = y ~ x) These output are all related to the model we have run, so they are kept organized by the list so they can be used for various further steps like comparing models or checking for model fit. A data frame is a bit more rigid that a list in that you cannot combine elements that do not have the same dimensions. # new.dataframe &lt;- data.frame( m.01$coefficients, m.01$residuals, m.01$call ) # # these will fail because the vectors have different lengths "],
["logical-statements.html", "Chapter 4 Logical Statements 4.1 Key Concepts 4.2 Operators 4.3 Selector Vectors 4.4 Usefulness of Selector Vectors 4.5 Compound Logical Statements 4.6 NAs in Logical Statements 4.7 Subsets", " Chapter 4 Logical Statements th { text-align: left; } td { text-align: left; } 4.1 Key Concepts Figure 4.1: Logical statements define group membership Logical statements are used to translate regular language into computer code. Many of our data analysis problems start by defining our sub-groups of interest. What percentage of men over 30 are bald? these &lt;- gender == &quot;male&quot; &amp; age &gt; 30 &amp; hair == FALSE mean( these ) Do bearded men earn more than men without beards? We need to make sure statements are correct. For example, the complement of “men with beards” is not men without beards, it is men without beards OR women (with or without beards). Figure 4.2: Compound statements can be tricky bearded.men &lt;- gender == &quot;male&quot; &amp; beard == TRUE proper.gentlemen &lt;- gender == &quot;male&quot; &amp; beard == FALSE mean( salary[ bearded.men ] ) mean( salary[ ! bearded.men ] ) # this is incorrect !!! mean( salary[ proper.gentlemen ] ) 4.2 Operators Logical operators are the most basic type of data programming and the core of many types of data analysis. Most of the time we are not conducting fancy statistics, we just want to identify members of a group (print all of the females from the study), or describe things that belong to a subset of the data (compare the average price of houses with garages to houses without garages). In order to accomplish these simple tasks we need to use logic statements. A logic statement answers the question, does an observation belong to a group. Many times groups are simple. Show me all of the professions that make over $100k a year, for example. Sometimes groups are complex. Identify the African American children from a specific zip code in Chicago that live in households with single mothers. You will use nine basic logical operators: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to x | y x OR y x &amp; y x AND y ! opposite of [ ] subset Logical operators create logical vectors, a vector that contains only TRUE or FALSE. The TRUE means that the observation belongs to the group, FALSE means it does not. x1 &lt;- c( 7, 9, 1, 2 ) x1 &gt; 7 ### [1] FALSE TRUE FALSE FALSE x1 &gt;= 7 ### [1] TRUE TRUE FALSE FALSE x1 == 9 | x1 == 1 ### [1] FALSE TRUE TRUE FALSE gender &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) gender == &quot;female&quot; ### [1] FALSE FALSE TRUE TRUE Note that the logical statement for “equals” is written with two equal signs. This is important to remember, because using a single equal sign can introduce subtle errors into your analysis. x1 &lt;- c( 7, 9, 1, 2 ) x1 == 9 ### [1] FALSE TRUE FALSE FALSE x1 = 9 # don&#39;t use a single equals operator! it overwrites your variable x1 ### [1] 9 We can write compound logical statements using the AND and OR operators: gender &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) study.group &lt;- c(&quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) gender == &quot;female&quot; &amp; study.group == &quot;treatment&quot; ### [1] FALSE FALSE TRUE FALSE gender == &quot;female&quot; | study.group == &quot;treatment&quot; ### [1] TRUE FALSE TRUE TRUE 4.3 Selector Vectors Note that we use operators to create logical vectors where TRUE designates observation that belong to the defined group, and FALSE designates observations outside the group. The term “selector vector” is a useful way to remember this purpose. After you have defined a group by composing a logical statement, then the vector can be used to count group members and take subsets of other variables to calculate group statistics. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 these.female &lt;- dat$gender == &quot;female&quot; sum( these.female ) # number of women in the study ### [1] 2 mean( these.female ) # proportion of the study that is women ### [1] 0.5 dat[ these.female , ] # all data belonging to women ### name gender group strength ### 2 wanda female treat 43 ### 4 nancy female control 58 mean( dat$strength[ these.female ] ) # average outcome for women in the study ### [1] 50.5 I will consistently name my logical vectors “these.GROUP” throughout the chapters, where GROUP represents the group label. For example, I selected women above, so the selector vector is called “these.female”. 4.4 Usefulness of Selector Vectors Selector vectors, i.e. logical vectors that were created by defining a group, have three main uses in our analysis. ONE: Logical vectors give us an easy way to count things within defined groups. We can apply a sum() function to a logical vector, and the result will be a tally of all of the TRUE cases. The mean() function will give us the proportion of the sample that belongs to our defined group. # how many females do we have in our study? sum( gender == &quot;female&quot; ) ### [1] 2 # how many females do we have in our treatment group? sum( gender == &quot;female&quot; &amp; study.group == &quot;treatment&quot; ) ### [1] 0 # what proportion of our study are men? mean( gender == &quot;male&quot; ) ### [1] 0.5 TWO: We can create a selector variable that is used for subsets. A selector vector used in a subset operator will drop all observations that are FALSE, isolating data belonging to the group: these.female &lt;- gender == &quot;female&quot; name[ these.female ] ### [1] &quot;wanda&quot; &quot;nancy&quot; strength[ these.female ] ### [1] 43 58 Or we can create a subset of the full dataset: dat[ these.female , ] name gender group strength 2 wanda female treat 43 4 nancy female control 58 THREE: We use selector variables to replace observations with new values using the assignment operator. This is similar to a find and replace operation. animals &lt;- c( &quot;mole&quot;, &quot;mouse&quot;, &quot;shrew&quot;, &quot;mouse&quot;, &quot;rat&quot;, &quot;shrew&quot; ) # the lab assistant incorrectly identified the shrews animals ### [1] &quot;mole&quot; &quot;mouse&quot; &quot;shrew&quot; &quot;mouse&quot; &quot;rat&quot; &quot;shrew&quot; animals[ animals == &quot;shrew&quot; ] &lt;- &quot;possum&quot; animals ### [1] &quot;mole&quot; &quot;mouse&quot; &quot;possum&quot; &quot;mouse&quot; &quot;rat&quot; &quot;possum&quot; We don’t know if linda received the treatment: name study.group adam treatment jamal control linda treatment sriti control study.group[ name == &quot;linda&quot; ] &lt;- NA study.group ### [1] &quot;treatment&quot; &quot;control&quot; NA &quot;control&quot; The ! operator is a special case, where it is not used to define a new logical vector, but rather it swaps the values of an existing logical vector. x1 &lt;- c(7,9,1,2) these &lt;- x1 &gt; 5 these ### [1] TRUE TRUE FALSE FALSE ! these ### [1] FALSE FALSE TRUE TRUE ! TRUE ### [1] FALSE ! FALSE ### [1] TRUE 4.5 Compound Logical Statements We can combine multiple logical statements using the AND, OR, and NOT operators ( &amp;, |, ! ). This functionality gives us an incredible ability to specify very granular groups within our analysis. This will be important as we begin to construct analysis in a way that we search for apples to apples comparisons within our data in order to make inferences about program effectiveness. These statements require some precision, however. Use care when applying that AND, OR, and NOT operators as to not include unintended data in your sample. In the example above, the statement “NOT bearded men” does not mean men without beards. It means all people outside of the category of men without beards (the “complement”), which includes women with or without beards as well. ! ( gender == &quot;male&quot; &amp; beard == TRUE ) Also note that parentheses matter. Compare this statement to the statement above: ! gender == &quot;male&quot; &amp; beard == TRUE Because we excluded the parentheses this statement now defines the group “NOT men AND with beards”, or bearded women. Figure 4.3: Examples of group construction with compound statements 4.6 NAs in Logical Statements Recall that missing values are an extremely important concept in statistics. If one-third of our survey sample reports that they never smoked pot, one-third reports they have smoked pot, and one-third did not answer the question, then what do we report for the proportion of the population that has smoked pot? We might prefer to be cautious and count only the people that have confirmed they have smoked pot, resulting in an estimate of 33.3%. If we throw out the missing data, then 50% of respondents have smoked pot. If we assume those that refuse to answer have likely smoked pot, our estimate might be 66.6% of the sample. These different results are a function of how we treat the missing data in our survey, so it is important that we can keep track of missing values, especially during subset operations. Note how NAs effect compound logical statements: TRUE &amp; TRUE ### [1] TRUE TRUE &amp; FALSE ### [1] FALSE TRUE &amp; NA ### [1] NA FALSE &amp; NA ### [1] FALSE To make sense of these rules consider the following: If one condition is already FALSE, the missing value does not matter because under the &amp; condition BOTH must be TRUE for the observation to belong to our defined group. After we know that one of the conditions is FALSE the missing value is irrelevant. For example, if we want to select all women in the treatment group, and we have a man with an unclear treatment group status, he is still excluded from the group because he is a man. On the other hand, if one condition is TRUE, and another is NA, R does not want to throw out the data because the state of the missing value is unclear. As a result, it will preserve the observation, but it will replace all of the data with missing values to signal the lack of certainty associated with that observation. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 keep.these &lt;- c(T,F,NA,F) dat[ keep.these , ] ### name gender group strength ### 1 frank male treat 27 ### NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA To remove these rows with missing values in your selector vector, replace all NAs with FALSE: keep.these[ is.na(keep.these) ] &lt;- FALSE dat[ keep.these , ] ### name gender group strength ### 1 frank male treat 27 4.7 Subsets The subset operators [ ] are one of the most common you will use in R. The primary rule of subsets is to use a data operator to create a logical selector vector, and use that to generate subsets. Any observation that corresponds to TRUE will be retained, any observation that corresponds to FALSE will be dropped. For vectors, you need to specify a single dimension. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 these.treat &lt;- dat$group == &quot;treat&quot; name[ these.treat ] ### [1] &quot;frank&quot; &quot;wanda&quot; strength[ these.treat ] ### [1] 27 43 For data frames, you need two dimensions (rows and columns). The two dimensions are seperated by a comma, and if you leave one blank you will not drop anything. dat[ row position , column position ] these.control &lt;- dat$group == &quot;control&quot; dat[ these.control , ] # all data in the control group ### name gender group strength ### 3 sanjay male control 19 ### 4 nancy female control 58 dat[ , c(&quot;name&quot;,&quot;gender&quot;) ] # select two columns of data ### name gender ### 1 frank male ### 2 wanda female ### 3 sanjay male ### 4 nancy female # to keep a subset as a separate dataset dat.women &lt;- dat[ dat$gender == &quot;female&quot; , ] dat.women ### name gender group strength ### 2 wanda female treat 43 ### 4 nancy female control 58 Note the rules listed above about subsetting factors. After applying a subset, they will retain all of the original levels, even when they are not longer useful. You need to drop the unused levels if you would like them to be omitted from functions that use the factor levels for analysis. df &lt;- data.frame( letters=LETTERS[1:5], numbers=seq(1:5) ) levels( df$letters ) ### [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; sub.df &lt;- df[ 1:3, ] sub.df$letters ### [1] A B C ### Levels: A B C D E levels( sub.df$letters ) ### [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; droplevels( sub.df$letters ) ### [1] A B C ### Levels: A B C sub.df$letters &lt;- droplevels( sub.df$letters ) "],
["data-verbs.html", "DATA VERBS", " DATA VERBS In this section we are moving from a focus on vector operations to a focus on dataset operations. Some of the most popular packages for building and manipulating datasets come from what is affectionately referred to as the “tidyverse”, a set of packages developed by Hadley Wickam and the R Studio team. They are designed to make basic dataset operations more intuitive and robust. These packages were created with a common framework for data design called “tidy data”. When datasets are built as or wrangled into the tidy format they are much easier to analyze, and they will play nicely with a large set of packages the follow the same philosophy of data design. The core R packages contain all of the functions necessary to wrangle datasets, but they are sometimes counter-intuitive or clumbsy in the sense that they might start with a data frame and return a table or a list, making them hard to use in data recipes. All of the “data verbs” that we will cover in this section start with a data frame and return a data frame, making them easier to use as data steps within a larger project. We will begin with some extremely useful verbs contained within the dplyr package. Figure 4.4: A data verb requires a dataset as input, and returns a transformed dataset. "],
["the-dplyr-package.html", "Chapter 5 The dplyr Package 5.1 Packages Used in This Chapter 5.2 Key Concepts 5.3 The dplyr Package 5.4 Use filter() to Subset Rows 5.5 select() Columns 5.6 arrange() Sorts Data 5.7 Variable Transforms with mutate() 5.8 rename() Variables 5.9 summarize() Variables", " Chapter 5 The dplyr Package th { text-align: left; } td { text-align: left; } 5.1 Packages Used in This Chapter library( dplyr ) library( pander ) 5.2 Key Concepts Figure 5.1: Data verbs are functions that require a dataframe as the primary argument, perform some transformation on the data, then return a new dataframe. For a nice overview of all of the dataset verbs in dplyr check out The dplyr Cheatsheet. 5.3 The dplyr Package This chapter will demonstrate a few basic dataset functions contained within the dplyr package. There are a few things to note as you get started: dplyr functions are all data verbs that accept a dataset as the argument, transform the data, and return a new dataset. The first argument is always the dataset name, and variables (columns) can be referenced directly by name without quotation marks. dplyr functions will return data as a “tibble” (tbl_df class), which is a regular data frame wrapped in a nice print method that includes metadata in the printout. For example, here is the regular data frame preview: ### weight group ### 1 4.17 ctrl ### 2 5.58 ctrl ### 3 5.18 ctrl ### 4 6.11 ctrl ### 5 4.50 ctrl ### 6 4.61 ctrl ### 7 5.17 ctrl ### 8 4.53 ctrl ### 9 5.33 ctrl ### 10 5.14 ctrl The tibble will print the first few rows and columns of a dataset, and includes dataset dimensions and vector classes: ### # A tibble: 30 x 2 ### weight group ### &lt;dbl&gt; &lt;fct&gt; ### 1 4.17 ctrl ### 2 5.58 ctrl ### 3 5.18 ctrl ### 4 6.11 ctrl ### 5 4.5 ctrl ### 6 4.61 ctrl ### # ... with 24 more rows We will cover the following dplyr functions in this section of the textbook: DATA VERB ACTION filter() Select rows select() Select columns arrage() Sort the dataset by one or more columns mutate() Create a new variable by transforming an existing variable or variables summarize() Create summary statistics for specified variables group_by() Split the dataset (implicitly) into a separate dataset for each group 5.4 Use filter() to Subset Rows In the last chapter we learned how to use operators to translate from plain English questions to data queries. As an example, a city manager might want to know the average amount owed on a delinquent property tax. tax.id amount.owed 1 $0 2 $5,549 3 $0 4 $1,709 5 $0 6 $634 7 $0 8 $0 9 $0 10 $9,353 We could write the query as follows: Define the group. Select the data that belongs to the group. Analyze the group subset. these.late &lt;- taxdat$amount.owed &gt; 0 # 1. Define group overdue.amounts &lt;- taxdat$amount.owed[ these.late ] # 2. Select data overdue.amounts ### [1] 5549 1709 634 9353 1366 mean( overdue.amounts ) # 3. Analyze data ### [1] 3722.2 The filter() function in the dplyr package is a slightly more elegant verb for selecting the group and subsetting the data by rows. filter( dataset name , logical expression ) filter( taxdat, amount.owed &gt; 0 ) ### tax.id amount.owed ### 1 2 5549 ### 2 4 1709 ### 3 6 634 ### 4 10 9353 ### 5 12 1366 Note that we do not need to reference the dat$ references inside dplyr functions. 5.5 select() Columns In the core R operators, we select colums from a dataset using the subset function: Murder Assault UrbanPop Rape Alabama 13.2 236 58 21.2 Alaska 10 263 48 44.5 Arizona 8.1 294 80 31 Arkansas 8.8 190 50 19.5 California 9 276 91 40.6 Colorado 7.9 204 78 38.7 USArrests[ , c(&quot;Murder&quot;,&quot;Assault&quot;) ] Murder Assault Alabama 13.2 236 Alaska 10 263 Arizona 8.1 294 Arkansas 8.8 190 California 9 276 Colorado 7.9 204 The select() function converts this oeration into a data verb: select( USArrests, Murder, Assault ) Murder Assault Alabama 13.2 236 Alaska 10 263 Arizona 8.1 294 Arkansas 8.8 190 California 9 276 Colorado 7.9 204 The select() function adds a lot of additional arguments that make it easy to quickly identify and keep only the necessary variables. We can demonstrate a few using the built-in iris dataset in R. Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Exclude a column with the negative sign: select( iris, -Species ) Sepal.Length Sepal.Width Petal.Length Petal.Width 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 Select by variable names: select( iris, ends_with( &quot;Length&quot; ) ) Sepal.Length Petal.Length 5.1 1.4 4.9 1.4 4.7 1.3 4.6 1.5 5 1.4 5.4 1.7 select( iris, starts_with( &quot;Petal&quot; ) ) Petal.Length Petal.Width 1.4 0.2 1.4 0.2 1.3 0.2 1.5 0.2 1.4 0.2 1.7 0.4 select( iris, matches(&quot;pal&quot;) ) Sepal.Length Sepal.Width 5.1 3.5 4.9 3 4.7 3.2 4.6 3.1 5 3.6 5.4 3.9 Or we can select by a range of variables by placing a colon between the first and last: select( iris, Sepal.Length:Petal.Width ) Petal.Length Petal.Width Species 1.4 0.2 setosa 1.4 0.2 setosa 1.3 0.2 setosa 1.5 0.2 setosa 1.4 0.2 setosa 1.7 0.4 setosa 5.6 arrange() Sorts Data The arrange() function sorts a dataset by one or more columns. By default, it sorts from smallest to largest. arrange( PlantGrowth, weight ) weight group 3.59 trt1 3.83 trt1 4.17 ctrl 4.17 trt1 4.32 trt1 If we prefer the dataset be sorted from largest to smallest, we can applyr the descending function desc() to the sort variable. arrange( PlantGrowth, desc(weight) ) weight group 6.31 trt2 6.15 trt2 6.11 ctrl 6.03 trt1 5.87 trt1 Or alternatively we can use the shortcut syntax of adding a negative sign in front of the variable: arrange( PlantGrowth, -weight ) We can also sort by multiple columns at once: arrange( PlantGrowth, group, weight ) group weight ctrl 5.17 ctrl 5.18 ctrl 5.33 ctrl 5.58 ctrl 6.11 trt1 4.69 trt1 4.81 trt1 4.89 trt1 5.87 trt1 6.03 NOTE, the equivalent core R functions would use subset[] and order() functions together. You might see examples on Stack Overflow written like this: PlantGrowth[ order(PlantGrowth$weight, decreasing=TRUE) , ] weight group 21 6.31 trt2 28 6.15 trt2 4 6.11 ctrl 17 6.03 trt1 15 5.87 trt1 29 5.8 trt2 As you can see, the dplyr versions are typically more intuitive and concise! 5.7 Variable Transforms with mutate() One of the most common operations in data analysis is to create a new variable from one or more existing variables, a “variable transformation”. Some examples include: x_squared &lt;- x * x celsius &lt;- ( fereinheit - 32 ) * ( 5/9 ) body.mass.index &lt;- kg / meters^2 per.capita.income &lt;- income / population The mutate() function creates a new transformed variable from the forumala you specify and adds it to the original dataset. As an example, perhaps we have data on the number of nonprofits located in each US city. If we look at the raw count of nonprofits, it makes it look as though the large cities have the most vibrant nonprofit sectors: city nonprofits NEW YORK 26503 LOS ANGELES 17417 WASHINGTON 15701 SAN FRANCISCO 12149 BOSTON 10536 CHICAGO 10247 PHILADELPHIA 8538 DALLAS 6008 SEATTLE 5830 ATLANTA 5438 But these numbers may be misleading. Once we account for the population size through a new nonprofit density metric (nonprofits per 1,000 residents), we can see that some smaller cities have higher densities per capita. dat.npos &lt;- mutate( dat.npos, density = nonprofits / (pop/1000) ) city density PORTLAND 2.65 MADISON 2.415 ANCHORAGE 2.156 SANTA BARBARA 1.928 LINCOLN 1.886 WASHINGTON 1.866 ASHEVILLE 1.805 TALLAHASSEE 1.785 BOSTON 1.692 ALBANY 1.685 5.8 rename() Variables More often than not you will read in a dataset that has strange or meaningless variable names: x1 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x2 &lt;- c(&quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) dat &lt;- data.frame( x1, x2 ) x1 x2 male treatment male control female treatment female control The core R functions make it a little awkward to rename these variables. names( dat ) &lt;- c(&quot;gender&quot;,&quot;study.group&quot;) gender study.group male treatment male control female treatment female control The rename() function provides a more intuitive syntax: rename( dat, x1=gender, x2=study.group ) ### x1 x2 ### 1 male treatment ### 2 male control ### 3 female treatment ### 4 female control 5.9 summarize() Variables The next chapter will cover descriptive statistics in more depth, including some useful packages and functions for generating statistics for a variety of variable types and and reporting nice tables. Most descriptive functions, however, are not data verbs in the sense that they accept a data frame as the input and return a transformed data frame or tibble. The dplyr function summarize() is the primary function that will be used in data recipes (see the next chapter). Like other data verbs, the first argument will be the input dataset. In this case, there is no pre-determined set of descriptive statistics. The user needs to specify the desired metrics. group gender strength control male 72 treatment male 108 treatment male 95 control male 92 control female 126 treatment female 112 summarize( dat, n=n(), min=min(strength), mean=mean(strength), max=max(strength) ) ### n min mean max ### 1 100 56 98.47 145 Similarly, the native table() function is useful, but returns a table object. The dplyr count() function will function almost identically, but it will return a data frame. dplyr::count( dat, group, gender ) ### # A tibble: 4 x 3 ### group gender n ### &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ### 1 control female 23 ### 2 control male 22 ### 3 treatment female 18 ### 4 treatment male 37 The real power of this function is the ability to use it with the group_by() function to analyze outcomes for many data subsets at once. grouped.dat &lt;- group_by( dat, group, gender ) dplyr::summarize( grouped.dat, n=n(), min=min(strength), mean=round( mean(strength), 1 ), max=max(strength) ) ### # A tibble: 4 x 6 ### # Groups: group [?] ### group gender n min mean max ### &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ### 1 control female 23 75 112. 143 ### 2 control male 22 56 80.8 112 ### 3 treatment female 18 94 120. 145 ### 4 treatment male 37 63 89.8 117 We will discuss this functionality in-depth two chapters from now. "],
["descriptive-statistics.html", "Chapter 6 Descriptive Statistics 6.1 Useful Packages", " Chapter 6 Descriptive Statistics 6.1 Useful Packages library( stargazer ) # publication quality tables library( skimr ) # quick and comprehensive descriptive stats Descriptive statistics are hugely important for any analysis, but they can be challenging to produce because different classes of variable require different tables or statistics to be meaningful. The most general core R summary() function prints some basic descriptives about variables in a dataset, reporting statistics based upon data type: name group gender height weight strength adam treatment male 73 180 167 jamal control male 67 190 185 linda treatment female 62 130 119 sriti control female 65 140 142 summary( dat ) ### name group gender height ### Length:4 control :2 female:2 Min. :62.00 ### Class :character treatment:2 male :2 1st Qu.:64.25 ### Mode :character Median :66.00 ### Mean :66.75 ### 3rd Qu.:68.50 ### Max. :73.00 ### weight strength ### Min. :130.0 Min. :119.0 ### 1st Qu.:137.5 1st Qu.:136.2 ### Median :160.0 Median :154.5 ### Mean :160.0 Mean :153.2 ### 3rd Qu.:182.5 3rd Qu.:171.5 ### Max. :190.0 Max. :185.0 These are not pretty enough to include in a report. Fortunately there are some functions that produce nice tables for R Markdown reports. We will use the stargazer package extensively for regression results and descriptive statistics. library( stargazer ) dat.numeric &lt;- select_if( dat, is.numeric ) stargazer( dat.numeric, type=&quot;html&quot;, digits=2, summary.stat = c(&quot;n&quot;,&quot;min&quot;,&quot;median&quot;,&quot;mean&quot;,&quot;max&quot;,&quot;sd&quot;) ) Statistic N Min Median Mean Max St. Dev. height 4 62 66 66.75 73 4.65 weight 4 130 160 160.00 190 29.44 strength 4 119 154.5 153.25 185 28.85 In many instances we will be working with a large dataset with many variables that are non-numeric. For example, the Lahman package contains a Master data frame with the demographic information of all Major League baseball players in the League’s 100-year history. Variables contained in the Master data frame in the Lahman package: VARIABLE CLASS DESCRIPTION playerID factor A unique code asssigned to each player. The playerID links the data in this file with records on players in the other files. birthYear numeric Year player was born birthMonth numeric Month player was born birthDay numeric Day player was born birthCountry character Country where player was born birthState character State where player was born birthCity character City where player was born deathYear numeric Year player died deathMonth numeric Month player died deathDay numeric Day player died deathCountry character Country where player died deathState character State where player died deathCity character City where player died nameFirst character Player’s first name nameLast character Player’s last name nameGiven character Player’s given name (typically first and middle) weight numeric Player’s weight in pounds height numeric Player’s height in inches bats factor a factor: Player’s batting hand (left (L), right (R), or both (B)) throws factor a factor: Player’s throwing hand (left(L) or right(R)) debut character Date that player made first major league appearance finalGame character Date that player made first major league appearance (blank if still active) retroID character ID used by retrosheet, http://www.retrosheet.org/ bbrefID character ID used by Baseball Reference website, http://www.baseball-reference.com/ birthDate date Player’s birthdate, in as.Date format deathDate date Player’s deathdate, in as.Date format In these cases, many of the summary functions will be of limited use. The skimr package was developed for large datasets like these. It will automatically create a set of summary tables for a variety of data types, and the default statistics are reasonable and informative: library( skimr ) skim( Master ) ### Skim summary statistics ### n obs: 19105 ### n variables: 26 ### ### -- Variable type:character ------------------------------------------------ ### variable missing complete n min max empty n_unique ### bbrefID 2 19103 19105 5 9 0 19103 ### birthCity 180 18925 19105 3 26 0 4745 ### birthCountry 69 19036 19105 3 14 0 53 ### birthState 571 18534 19105 2 22 0 260 ### deathCity 9674 9431 19105 2 28 0 2578 ### deathCountry 9669 9436 19105 3 14 0 25 ### deathState 9715 9390 19105 2 20 0 101 ### debut 195 18910 19105 10 10 0 10155 ### finalGame 195 18910 19105 10 10 0 9138 ### nameFirst 37 19068 19105 2 12 0 2353 ### nameGiven 37 19068 19105 2 43 0 12616 ### nameLast 0 19105 19105 2 14 0 9831 ### playerID 0 19105 19105 5 9 0 19105 ### retroID 56 19049 19105 8 8 0 19049 ### ### -- Variable type:Date ----------------------------------------------------- ### variable missing complete n min max median ### birthDate 449 18656 19105 1820-04-17 1996-08-12 1939-01-01 ### deathDate 9666 9439 19105 1872-03-17 2017-02-19 1967-03-06 ### n_unique ### 15404 ### 8458 ### ### -- Variable type:factor --------------------------------------------------- ### variable missing complete n n_unique ### bats 1185 17920 19105 3 ### throws 979 18126 19105 3 ### top_counts ordered ### R: 11788, L: 4957, NA: 1185, B: 1175 FALSE ### R: 14472, L: 3653, NA: 979, S: 1 FALSE ### ### -- Variable type:integer -------------------------------------------------- ### variable missing complete n mean sd p0 p25 p50 p75 p100 ### birthDay 449 18656 19105 15.61 8.75 1 8 16 23 31 ### birthMonth 302 18803 19105 6.63 3.47 1 4 7 10 12 ### birthYear 132 18973 19105 1931.44 41.56 1820 1895 1937 1969 1996 ### deathDay 9666 9439 19105 15.57 8.78 1 8 15 23 31 ### deathMonth 9665 9440 19105 6.48 3.53 1 3 6 10 12 ### deathYear 9664 9441 19105 1964.29 31.81 1872 1942 1967 1990 2017 ### height 785 18320 19105 72.27 2.6 43 71 72 74 83 ### weight 854 18251 19105 186.38 21.52 65 170 185 200 320 ### hist ### &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt; ### &lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2586&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2587&gt; ### &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt; ### &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt; ### &lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt; ### &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2586&gt; ### &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt; ### &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; For more functionality see: vignette( &quot;Using_skimr&quot;, package = &quot;skimr&quot; ) There are many additional packages and tricks for producing descriptive statistics. Note, though, that most produce a print-out of summary statistics but do not return a useful “tidy” dataset that can be used in subsequent steps. For most data recipes, we will rely on the summarize() function in the dplyr package. It’s utility will become obvious in the next two chapters. "],
["data-recipes.html", "Chapter 7 Data Recipes 7.1 Packages Used in This Chapter 7.2 Key Concepts 7.3 The Pipe Operator %&gt;% 7.4 Building Data Recipes 7.5 Conclusion", " Chapter 7 Data Recipes th { text-align: left; } td { text-align: left; } 7.1 Packages Used in This Chapter library( dplyr ) library( pander ) library( ggvis ) 7.2 Key Concepts Figure 7.1: Data verbs use data frames as the primary input and the output value. Figure 7.2: When we chain data verbs with the pipe operator we only reference the dataset name once, and all other times it’s implicitly called through through piping. Figure 7.3: Chaining allows us to write data recipes. 7.3 The Pipe Operator %&gt;% The idea of functions() was first introduced using a metaphor of a cookie recipe that has ingredients (data and arguments) and requires that each step of the process building on the results of the previous step. The pipe operator allows us to follow this same model to build “data recipes”, a stylized way of writing a program as a series of data verbs chained together to wrangle and analyze the data. The pipe operator passes the data from one verb to the next without having to name it directly. Figure 7.4: The pipe operator allows us to pass a transformed dataset forward in the recipe. 7.4 Building Data Recipes Data recipes are simple scripts that follow a series of steps, just like a recipe. This chapter demonstrates how data verbs and pipe operators can be used to write recipes to generate interesting insights. To demonstrate the idea, we will use a dataset of US Baby Names released by the Social Security Administration. This version was downloaded by Ryan Burge and posted on Kaggle. I’ve re-posted it on GitHub so it can be read directly into R easily: URL &lt;- &quot;https://github.com/DS4PS/Data-Science-Class/blob/master/DATA/BabyNames.rds?raw=true&quot; names &lt;- readRDS( gzcon( url( URL ))) names %&gt;% head() %&gt;% pander() Id Name Year Gender Count 1 Mary 1880 F 7065 2 Anna 1880 F 2604 3 Emma 1880 F 2003 4 Elizabeth 1880 F 1939 5 Minnie 1880 F 1746 6 Margaret 1880 F 1578 Let’s start by building a recipe to identify the top 10 male names for Baby Boomers. Create a subset of data for men born between 1946 and 1964. Sort by the annual count of each name in the subset. Keep only the most popular year for each name. Identify the top 10 most popular during this period. Print the results in a nice table that includes name and peak year data. The recipe will look something like this: names %&gt;% filter( Gender ==&quot;M&quot; &amp; Year &gt;= 1946 &amp; Year &lt;= 1964 ) %&gt;% arrange( desc( Count ) ) %&gt;% distinct( Name, .keep_all=T ) %&gt;% top_n( 10, Count ) %&gt;% select( Name, Year, Count ) %&gt;% pander() Name Year Count James 1947 94755 Michael 1957 92709 Robert 1947 91642 John 1947 88318 David 1955 86191 William 1947 66969 Richard 1946 58859 Mark 1960 58735 Thomas 1952 48617 Charles 1947 40773 There are many ways to construct a data recipe. We could have alternatively taken this approach: Create a subset of data for men born between 1946 and 1964. Count the total numer of men given each name during the period. Find the top 10 most popular names. names %&gt;% filter( Gender ==&quot;M&quot; &amp; Year &gt;= 1946 &amp; Year &lt;= 1964 ) %&gt;% group_by( Name ) %&gt;% dplyr::summarize( total=sum(Count) ) %&gt;% dplyr::arrange( desc(total) ) %&gt;% slice( 1:10 ) %&gt;% pander() Name total James 1570607 Robert 1530527 John 1524619 Michael 1463911 David 1395499 William 1072303 Richard 959321 Thomas 810160 Mark 684159 Charles 657780 We can see that these two approaches to answering our question give us slightly different results, but are pretty close. Let’s try to identify when specific female names have peaked. Create a subset of data for women. Group the data by “Name” so we can analyze each name separately. Find the year with the highest count for each name. Store this data as “peak.years”. Each name will occur once in this dataset in the year that it experienced it’s peak popularity. peak.years &lt;- names %&gt;% filter( Gender == &quot;F&quot; ) %&gt;% group_by( Name ) %&gt;% top_n( 1, Count ) %&gt;% ungroup() peak.years %&gt;% head( 5 ) %&gt;% pander() Id Name Year Gender Count 568 Manerva 1880 F 10 720 Neppie 1880 F 7 2621 Zilpah 1881 F 9 4625 Crete 1882 F 8 4750 Alwina 1882 F 6 We can then filter by years to see which names peaked in a given period. filter( peak.years, Year == 1950 ) %&gt;% arrange( desc( Count ) ) %&gt;% slice( 1:5 ) %&gt;% pander() Id Name Year Gender Count 462006 Constance 1950 F 4442 462008 Glenda 1950 F 4213 462103 Bonita 1950 F 1527 462301 Ilene 1950 F 453 462305 Marta 1950 F 445 # library( ggvis ) names %&gt;% filter( Name == &quot;Constance&quot; &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.1920 &lt;- filter( peak.years, Year == 1920 ) %&gt;% top_n( 5, Count ) top.five.1920 ### # A tibble: 5 x 5 ### Id Name Year Gender Count ### &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ### 1 169464 Ruth 1920 F 26100 ### 2 169465 Mildred 1920 F 18058 ### 3 169472 Marie 1920 F 12745 ### 4 169477 Lillian 1920 F 10050 ### 5 169481 Gladys 1920 F 8819 names %&gt;% filter( Name %in% top.five.1920$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.1975 &lt;- filter( peak.years, Year == 1975 ) %&gt;% top_n( 5, Count ) names %&gt;% filter( Name %in% top.five.1975$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.2000 &lt;- filter( peak.years, Year == 2000 ) %&gt;% top_n( 5, Count ) names %&gt;% filter( Name %in% top.five.2000$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download Ryan Burge posted a fun project on Kaggle about how to find hipster names using this historical data. He defines hipster names as those meeting the following criteria: They were popular when your grandmother was young. They were unpopular when your parent were young. They have recently become popular again. Let’s stick with women’s names. df1 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year &gt;= 1915 &amp; Year &lt;= 1935 &amp; Count &gt; 3000 ) df2 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year == 1980 &amp; Count &lt;= 1000 ) df3 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year &gt;= 2010 &amp; Count &gt; 2000) hipster.names &lt;- names %&gt;% filter( Name %in% df1$Name &amp; Name %in% df2$Name &amp; Name %in% df3$Name ) %&gt;% group_by( Name ) %&gt;% dplyr::summarize( total=sum(Count), peak=max(Count) ) %&gt;% arrange( desc( peak ) ) Here are the top 6 female hipster names: top.hipster.names &lt;- c(&quot;Emma&quot;,&quot;Evelyn&quot;,&quot;Alice&quot;,&quot;Grace&quot;,&quot;Lillian&quot;,&quot;Charlotte&quot;) names %&gt;% filter( Name %in% top.hipster.names &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download And the full list: hipster.names %&gt;% pander() Name total peak Emma 595546 22701 Evelyn 534502 14279 Grace 469034 12770 Alice 551034 11956 Lillian 421392 10050 Charlotte 312022 10048 Ella 273663 9868 Josephine 297064 8683 Eleanor 268153 8499 Ruby 340143 8407 Hazel 244069 7615 Clara 268980 5779 Eva 252465 4564 Lucy 185064 4257 Stella 155080 4165 Violet 122984 4156 Vivian 198012 4128 7.5 Conclusion The pipe operator is a little confusing when you first encounter it, but you will find that using data verbs contained in the dplyr package and the pipe operator will speed up your analysis and make your code more readable. In the next chapter we focus more on the use of groups in data science, and the applications of the group_by() function to make your job easier. "],
["analysis-with-groups.html", "Chapter 8 Analysis with Groups 8.1 Packages Used in this Chapter 8.2 Hypothetical Experimental Data 8.3 Group Structure 8.4 Analysis by Group", " Chapter 8 Analysis with Groups 8.1 Packages Used in this Chapter library( pander ) library( dplyr ) library( tidyr ) library( reshape2 ) library( scales ) library( ggplot2 ) library( Lahman ) 8.2 Hypothetical Experimental Data We will demonstrate some functions using this hypothetical dataset: head( d ) %&gt;% pander id race blood.type gender age study.group speed 1 black A female 31 treatment 596.4 2 white B male 48 treatment 536.7 3 white B female 46 treatment 698.7 4 black A female 52 treatment 576 5 white B male 35 treatment 508.4 6 black B female 36 treatment 702.5 8.3 Group Structure The two most important skills as you first learn a data programming language are: Translating English phrases into computer code using logical statements Organizing your data into groups This lecture focuses on efficienty splitting your data into groups, and then analyzing your data by group. 8.3.1 What Are Groups? A group represents a set of elements with identical characteristics - mice all belong to one group and elephants belong to another. Easy enough, right? In data analysis, it is a little more complicated because a group is defined by a set of features. Each group still represents a set of elements with identical characteristics, but when we have multiple features there is a unique group for each combination of features. The simple way to think about this is that the cross-tab of features generates a grid (table), and each cell represents a unique group: We might be interested in simple groups (treatment cases versus control cases), or complex groups (does the treatment effect women and men differently?). In previous lectures you have learned to identify a group with a logical statement, and analyze that group discretely. mean( speed[ study.group == &quot;treatment&quot; &amp; gender==&quot;female&quot; ] ) ### [1] 612.3679 In this lecture you will learn to define a group structure, then analyze all of your data using that structure. tapply( speed, INDEX = list( study.group, gender ), FUN = mean ) female male control 460 362.2 treatment 612.4 517.6 8.3.2 Main Take-Away R has been designed to do efficient data analysis by defining a group structure, then quickly applying a function to all unique members. The base R packages do this with a set of functions in the apply() family. The tapply() function allows you to specify an outcome to analyze and a group, then ask for results from a function. tapply( X=speed, INDEX=list( study.group, gender ), FUN=mean ) female male control 460 362.2 treatment 612.4 517.6 The dplyr package makes this process easier using some simple verbs and the “pipe” operator. dat %&gt;% group_by( study.group, gender ) %&gt;% summarize( ave.speed = mean(speed) ) study.group gender ave.speed control male 362.2 control female 460 treatment male 517.6 treatment female 612.4 8.3.3 Example Let’s think about a study looking at reading speed. The treatment is a workshop that teaches some speed-reading techniques. In this study we have data on: gender (male,female) race (black,white,asian) blood.type (A,B) age (from 18 to 93) Examining descriptive statistics we can see that reading speed varies by gender and the treatment group, but not by race or blood type: The question is, how many unique groups can we create with these four factors? Each individual factor contains a small number of levels (only 2 or 3 in this case), which makes the group structure look deceptively simple at first glance. When we start to examine combinations of factors we see that group structure can get complicated pretty quickly. If we look at gender alone, we have two levels: male and female. So we have two groups. If we look at our study groups alone we have two groups: treatment and control. If we look at gender and the study groups together, we now have a 2 x 2 grid, or four unique groups. If the race factor has three levels, how many unique groups will we have considering the study design, gender, and race together? We can calculate the size of the grid by multiplying number of levels for each factor. We see here we have 12 unique groups: nlevels( gender ) * nlevels( study.group ) * nlevels( race ) ### [1] 12 If we add blood type, a factor with two levels (A and B), we now have 24 unique groups: p + facet_grid( race + study.group ~ gender + blood.type) What about age? It is a continuous variable, so it’s a little more tricky. We can certainly analyze the relationship between age and speed using correlation tools. plot( age, speed, bty=&quot;n&quot;, main=&quot;Age&quot; ) But we can also incorporate this independent variable into a group structure. We can treat each distinct age as a separate group. The ages in this study range from 18 to 93, so we have 65 distinct ages represented. plot( factor(age), speed, las=2, frame.plot=F, outline=F, main=&quot;Age&quot;, xaxt=&quot;n&quot; ) If we think about the overall group structure, then, we have unique groups defined by gender, race, blood type, and study design, and another 65 age groups. So in total we now have 24 x 65 = 1,560 groups! That is getting complicated. This group design is problematic for two reasons. From a pragmatic standpoint, we can’t report results from 1,500 groups in a table. From a more substantive perspective, we although we have 1,500 distinct cells in our grid, many may not include observations that represent the unique combination of all factors. So this group design is not very practical. A similar problem arises if our data includes time. If our data includes the time of events recorded by hours, days of the week, months, and years, we can have generate complicated group structures if we try to analyze every unique combination. We can simplify our analysis by thinking about age ranges instead of ages, or in other words by binning our continuous data. If we split it into five-year ranges, for example, we have gone from 65 distinct ages to 12 distinct age groups. age.group &lt;- cut( age, breaks=seq(from=20,to=80,by=5), labels=paste( seq(from=20,to=75,by=5), &quot;to&quot;, seq(from=25,to=80,by=5) ) ) group.structure &lt;- formula( speed ~ age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group&quot; ) We have now simplified our analysis from 1,560 to 288 possible groups. Combinations of groups will also be easier: group.structure &lt;- formula( speed ~ gender * age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group by Gender&quot;, col=c(&quot;firebrick&quot;,&quot;steelblue&quot;), xaxt=&quot;n&quot; ) 8.4 Analysis by Group Let’s demonstrate some analysis of groups using the Lahman package and some dplyr verbs. Let’s do some analysis of player salaries (Salaries dataset), and start with a simple group structure - teams in the National League and time. Which team has the highest average player salary? Which team has the most players paid over $5 million a season? Which team has raised it’s pay the most over the past decade? Let’s start by thinking about group structure. We have teams, and we have seasons. Teams is stored as a factor, and seasons as a numeric value, so we can consider group for each by counting levels and unique values: nlevels( Salaries$teamID ) ### [1] 46 length( unique( Salaries$yearID ) ) ### [1] 32 So we can potentially calculate 32 x 46 = 1,472 average player salaries. 8.4.1 Highest Ave Player Salary For our first question, we will select only teams from the National League. Let’s use the most recent year of data to calculate average pay. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID) %&gt;% summarize( Ave_Salary = mean(salary) ) ### # A tibble: 15 x 2 ### teamID Ave_Salary ### &lt;fct&gt; &lt;dbl&gt; ### 1 ARI 3363041. ### 2 ATL 2362010. ### 3 CHC 5312678. ### 4 CIN 3066899. ### 5 COL 3413487 ### 6 LAD 6322525. ### # ... with 9 more rows Since the salaries are large, they are a little hard to read. Let’s clean up the table a bit. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID ) %&gt;% summarize( Ave_Salary=dollar( mean(salary,na.rm=T) ) ) %&gt;% arrange( desc(Ave_Salary) ) %&gt;% pander() teamID Ave_Salary SFG $6,890,151 LAD $6,322,525 WSN $5,448,179 CHC $5,312,678 NYM $4,958,857 STL $4,614,629 SDP $3,756,475 PIT $3,706,387 COL $3,413,487 ARI $3,363,041 CIN $3,066,899 MIA $2,761,222 ATL $2,362,010 MIL $2,292,508 PHI $2,033,793 8.4.2 Most Players Paid Over $5 Million This question requires you to utilize a logical statement in order to translate from the question to code. We need to inspect each salary, determine whether it is over the $5m threshold, then count all of the cases. The operation will look something like this: sum( Salaries$salary &gt; 5000000 ) ### [1] 3175 It gets a little trickier when we want to do the operation simultaneously across groups. Our team group structure is already defined, so let’s define our logical vector and count cases that match: dat.NL &lt;- filter( Salaries, yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() gt.5m &lt;- dat.NL$salary &gt; 5000000 table( dat.NL$teamID, gt.5m ) ### gt.5m ### FALSE TRUE ### ARI 23 3 ### ATL 21 6 ### CHN 19 8 ### CIN 21 5 ### COL 23 6 ### FLO 23 4 ### HOU 24 4 ### LAN 20 7 ### MIL 25 4 ### NYN 19 9 ### PHI 18 10 ### PIT 27 0 ### SDN 25 1 ### SFN 21 7 ### SLN 19 6 ### WAS 26 4 This solution works, but the table provides too much information. We can use dply to simplify and format the table nicely for our report: Salaries %&gt;% filter( yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID ) %&gt;% summarise( gt_five_million = sum( salary &gt; 5000000 ) ) %&gt;% arrange( desc(gt_five_million) ) %&gt;% pander teamID gt_five_million PHI 10 NYN 9 CHN 8 LAN 7 SFN 7 ATL 6 COL 6 SLN 6 CIN 5 FLO 4 HOU 4 MIL 4 WAS 4 ARI 3 SDN 1 PIT 0 8.4.3 Fielding Positions Which fielding position is the highest paid? merge( Salaries, Fielding ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( POS ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander POS Mean_Salary 1B $5,570,032 2B $3,162,075 3B $3,579,088 C $2,521,903 OF $3,546,115 P $3,401,676 SS $2,510,833 8.4.4 Country of Birth Which country has produced the highest paid baseball players? merge( Salaries, Master ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( birthCountry ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander birthCountry Mean_Salary Aruba $650,000 Australia $523,400 Brazil $1,548,792 CAN $7,854,167 Colombia $3,125,289 Cuba $5,532,484 Curacao $5,724,167 D.R. $5,102,318 Germany $511,500 Japan $8,247,012 Mexico $4,617,038 Netherlands $2,425,000 Nicaragua $2,375,000 P.R. $3,241,378 Panama $2,946,550 Saudi Arabia $522,500 South Korea $5,326,190 Taiwan $6,750,000 USA $4,189,640 V.I. $507,500 Venezuela $4,521,051 8.4.5 Pay Raises To examine pay raises, we will now use more than one year of data. Since the question asks about pay raises over the past decade, we will filter the last ten years of data. And how since we are looking at patterns over teams and over time, we need to define a group structure with two variables: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean= dollar(mean(salary)) ) %&gt;% head( 20 ) %&gt;% pander teamID yearID mean ARI 2007 $1,859,555 ARI 2008 $2,364,383 ARI 2009 $2,812,141 ARI 2010 $2,335,314 ARI 2011 $1,986,660 ARI 2012 $2,733,512 ARI 2013 $3,004,400 ARI 2014 $3,763,904 ARI 2015 $2,034,250 ARI 2016 $3,363,041 ATL 2007 $3,117,530 ATL 2008 $3,412,189 ATL 2009 $3,335,385 ATL 2010 $3,126,802 ATL 2011 $3,346,257 ATL 2012 $2,856,205 ATL 2013 $3,254,501 ATL 2014 $4,067,042 ATL 2015 $2,990,885 ATL 2016 $2,362,010 This might seem like an odd format. We might expect something that looks more like our grid structure: dat.NL &lt;- filter( Salaries, yearID &gt; 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() tapply( dat.NL$salary, INDEX=list(dat.NL$teamID, dat.NL$yearID), FUN=mean, na.rm=T ) %&gt;% pander() 2011 2012 2013 2014 2015 2016 ARI 1986660 2733512 3004400 3763904 2034250 3363041 ATL 3346257 2856205 3254501 4067042 2990885 2362010 CHC NA NA NA NA NA 5312678 CHN 5001893 3392194 3867989 2426759 4138547 NA CIN 2531571 2935843 4256178 3864911 4187862 3066899 CLE NA NA NA 4500000 NA NA COL 3390310 2692054 2976363 3180117 3827544 3413487 FLO 2190154 NA NA NA NA NA HOU 2437724 2332731 NA NA NA NA LAD NA NA NA NA NA 6322525 LAN 3472967 3171453 6980069 6781706 7441103 NA MIA NA 4373259 1400079 1549515 2835688 2761222 MIL 2849911 3755921 3077881 3748778 3477586 2292508 NYM NA NA NA NA NA 4958857 NYN 4401752 3457555 1648278 3168777 3870667 NA PHI 5765879 5817965 6533200 5654530 4295885 2033793 PIT 1553345 2248286 2752214 2756357 3065259 3706387 SDN 1479650 1973025 2342339 2703061 4555435 NA SDP NA NA NA NA NA 3756475 SFG NA NA NA NA NA 6890151 SFN 4377716 3920689 5006441 5839649 6100056 NA SLN 3904947 3939317 3295004 4310464 4586212 NA STL NA NA NA NA NA 4614629 WAS 2201963 2695171 4548131 4399456 5365085 NA WSN NA NA NA NA NA 5448179 Later on we will look at the benefits of “tidy data”, but the basic idea is that you can “facet” your analysis easily when your groups are represented as factors instead of arranged as a table. For example, here is a time series graph that is faceted by teams: Salaries %&gt;% filter( yearID &gt; 2000 &amp; lgID == &quot;AL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( Mean_Player_Salary=mean(salary) ) -&gt; t1 qplot( data=t1, x=yearID, y=Mean_Player_Salary, geom=c(&quot;point&quot;, &quot;smooth&quot;) ) + facet_wrap( ~ teamID, ncol=5 ) Now you can quickly see that Detroit is the team that has raised salaries most aggressively. If we need to, we can easily convert a tidy dataset into something that looks like a table using the spread() function: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=yearID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% na.omit() %&gt;% pander teamID yearID_2007 yearID_2008 yearID_2009 yearID_2010 yearID_2011 ARI $1,859,555 $2,364,383 $2,812,141 $2,335,314 $1,986,660 ATL $3,117,530 $3,412,189 $3,335,385 $3,126,802 $3,346,257 CHN $3,691,494 $4,383,179 $5,392,360 $5,429,963 $5,001,893 CIN $2,210,483 $2,647,061 $3,198,196 $2,760,059 $2,531,571 COL $2,078,500 $2,640,596 $2,785,222 $2,904,379 $3,390,310 FLO $984,097 $660,955 $1,315,500 $2,112,212 $2,190,154 HOU $3,250,333 $3,293,719 $3,814,682 $3,298,411 $2,437,724 LAN $3,739,811 $4,089,260 $4,016,584 $3,531,778 $3,472,967 MIL $2,629,130 $2,790,948 $3,083,942 $2,796,837 $2,849,911 NYN $3,841,055 $4,593,113 $5,334,785 $4,800,819 $4,401,752 PHI $2,980,940 $3,495,710 $4,185,335 $5,068,871 $5,765,879 PIT $1,427,327 $1,872,684 $1,872,808 $1,294,185 $1,553,345 SDN $2,235,022 $2,376,697 $1,604,952 $1,453,819 $1,479,650 SFN $3,469,964 $2,641,190 $2,965,230 $3,522,905 $4,377,716 SLN $3,224,529 $3,018,923 $3,278,830 $3,741,630 $3,904,947 WAS $1,319,554 $1,895,207 $2,140,286 $2,046,667 $2,201,963 Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=teamID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% pander yearID teamID_ARI teamID_ATL teamID_CHC teamID_CHN teamID_CIN 2007 $1,859,555 $3,117,530 NA $3,691,494 $2,210,483 2008 $2,364,383 $3,412,189 NA $4,383,179 $2,647,061 2009 $2,812,141 $3,335,385 NA $5,392,360 $3,198,196 2010 $2,335,314 $3,126,802 NA $5,429,963 $2,760,059 2011 $1,986,660 $3,346,257 NA $5,001,893 $2,531,571 2012 $2,733,512 $2,856,205 NA $3,392,194 $2,935,843 2013 $3,004,400 $3,254,501 NA $3,867,989 $4,256,178 2014 $3,763,904 $4,067,042 NA $2,426,759 $3,864,911 2015 $2,034,250 $2,990,885 NA $4,138,547 $4,187,862 2016 $3,363,041 $2,362,010 $5,312,678 NA $3,066,899 "],
["data-wrangling.html", "DATA WRANGLING", " DATA WRANGLING "],
["merging-data.html", "Chapter 9 Merging Data 9.1 Packages Used in This Chapter 9.2 Relational Databases 9.3 Set Theory 9.4 Merging Data 9.5 Non-Unique Observations in ID Variables 9.6 The %in% function 9.7 The Match Function", " Chapter 9 Merging Data 9.1 Packages Used in This Chapter library( pander ) library( dplyr ) library( maps ) 9.2 Relational Databases Modern databases are huge - think about the amount of information stored at Amazon in the history of each transation, the database where Google logs every single search from every person around the world, or Twitter’s database of all of the tweets (millions each day). When databases become large, flat spreadsheet style formats are not useful because they create a lot of redundant information, are large to store, and are not efficient to search. Large datasets are instead stored in relational databases - sets of tables that contain unique IDs that allow them to be joined when necessary. For example, consider a simple customer database. We don’t want to store customer info with our transactions because we would be repeating their name and street address every time they make a new purchase. As a result, we store customer information and transaction information separately. Customer Database CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE 178 Alvaro Jaurez 123 Park Ave 57701 934 Janette Johnson 456 Candy Ln 57701 269 Latisha Shane 1600 Penn Ave 20500 Transactions Database CUSTOMER.ID PRODUCT PRICE 178 video 5.38 178 shovel 12 269 book 3.99 269 purse 8 934 mirror 7.64 If we want to make the information actionable then we need to combine these datasets. For example, perhaps we want to know the average purchase amount from an individual in the 57701 zip code. We cannot answer that question with either dataset since the zip code is in one dataset, and the price is in another. We need to merge the data. merge( customer.info, purchases ) ### CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE ### 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 ### 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 ### 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 ### 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 ### 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 full.dat &lt;- merge( customer.info, purchases ) full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] ### [1] 5.38 12.00 7.64 mean( full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] ) ### [1] 8.34 In reality, each purchase would have a purchase ID that is linked to shipping addresses, customer complaints, seller ratings, etc. Each seller would have their own data table with info. Each purchase would be tied to a payment type, which has its own data table. The system gets quite complex, which is why it is important to pay attention to the details of putting the data back together again. Figure 9.1: Example of a relational database schema We will cover a few details of data merges that will help you avoid common and very subtle mistakes that can lead to incorrect inferences. 9.3 Set Theory In order to merge data correctly you need to understand some very basic principles of set theory. 9.3.1 Set Theory Functions Let’s assume we have two sets: set1=[A,B], set2=[B,C]. Each element in this set represents a group of observations that occurs in the dataset. So B represents people that occur in both datasets, A represents people that occur only in the first dataset, and C represents people that only occur in the second dataset. We can then describe membership through three operations: Figure 9.2: Membership defined by two sets Operation Description union: X OR Y The universe of all elements across all both sets: [A,B,C] intersection: X &amp; Y The elements shared by both sets: [B] difference: X &amp; ! Y The elements in my first set, not in my second [A] or [C] Let’s see how this might work in practice with an example of members of a study: name group gender frank treat male wanda treat female sanjay control male nancy control female For this example let’s define set 1 as the treatment group, and set 2 as all women in the study. Note that set membership is always defined as binary (you are in the set or out), but it can include multiple criteria (the set of animals can contains cats, dogs, and mice). treated &lt;- name[ group == &quot;treat&quot; ] treated ### [1] &quot;frank&quot; &quot;wanda&quot; females &lt;- name[ gender == &quot;female&quot; ] females ### [1] &quot;wanda&quot; &quot;nancy&quot; Now we can specify group belonging using some convenient set theory functions: union(), setdiff(), and intersect(). union( treated, females ) ### [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; intersect( treated, females ) ### [1] &quot;wanda&quot; setdiff( treated, females ) ### [1] &quot;frank&quot; setdiff( females, treated ) ### [1] &quot;nancy&quot; It is very important to note that union() and intersect() are symmetric functions, meaning intersect(x,y) will give you the same result as intersect(y,x). The setdiff() function is not symmetric, however. 9.3.2 Set Theory Using Logical Operators Typically you will define your groups using logical operators, which perform the exact same funciton as set theory functions but are a little more expressive and flexible. Let’s use the same example above where x=“treatment” and y=“female”, then consider these cases: Who belongs in each group? name group gender frank treat male wanda treat female sanjay control male nancy control female # x name[ group == &quot;treat&quot; ] ### [1] &quot;frank&quot; &quot;wanda&quot; # x &amp; y name[ group == &quot;treat&quot; &amp; gender == &quot;female&quot; ] ### [1] &quot;wanda&quot; # x &amp; ! y name[ group == &quot;treat&quot; &amp; gender != &quot;female&quot; ] ### [1] &quot;frank&quot; # x | y name[ group == &quot;treat&quot; | gender == &quot;female&quot; ] ### [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; Who belongs in these groups? !x &amp; !y x &amp; ! ( x &amp; y ) ( x | y ) &amp; ! ( x &amp; y ) 9.4 Merging Data The Merge Function The merge function joins two datasets. The function requires two datasets as the arguments, and they need to share a unique ID variable. Recall the example from above: merge( customer.info, purchases ) ### CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE ### 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 ### 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 ### 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 ### 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 ### 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 The important thing to keep in mind is that the default merge operation uses the intersection of the two datasets. It will drop all elements that don’t occur in both datasets. We may want to fine-tune this as to not lose valuable data and potentially bias our analysis. As an example, no illegal immigrants will have social security numbers, so if you are merging using the SSN, you will drop this group from the data, which could impact your results. With a little help from the set theory examples above, we can think about which portions of the data we wish to drop and which portions we wish to keep. Argument Usage all=F DEFAULT - new dataset contains intersection of X and Y (B only) all=T New dataset contains union of X and Y (A, B &amp; C) all.x=T New dataset contains A and B, not C all.y=T New dataset contains B and C, not A Here is some demonstrations with examples adapted from the R help file. authors ### surname nationality deceased ### 1 Tukey US yes ### 2 Tierney US no ### 3 Ripley UK no ### 4 McNeil Australia no ### 5 Shakespeare England yes books ### name title ### 1 Tukey Exploratory Data Analysis ### 2 Venables Modern Applied Statistics ### 3 Ripley Spatial Statistics ### 4 Ripley Stochastic Simulation ### 5 McNeil Interactive Data Analysis ### 6 R Core Team An Introduction to R # adding books to the author bios dataset ( set B only ) merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;) ### surname nationality deceased title ### 1 McNeil Australia no Interactive Data Analysis ### 2 Ripley UK no Spatial Statistics ### 3 Ripley UK no Stochastic Simulation ### 4 Tukey US yes Exploratory Data Analysis # adding author bios to the books dataset ( set B only ) merge(books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 Ripley Spatial Statistics UK no ### 3 Ripley Stochastic Simulation UK no ### 4 Tukey Exploratory Data Analysis US yes # keep books without author bios, lose authors without books ( sets A and B ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.x=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; ### 3 Ripley Spatial Statistics UK no ### 4 Ripley Stochastic Simulation UK no ### 5 Tukey Exploratory Data Analysis US yes ### 6 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; # keep authors without book listed, lose books without author bios ( sets B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.y=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 Ripley Spatial Statistics UK no ### 3 Ripley Stochastic Simulation UK no ### 4 Shakespeare &lt;NA&gt; England yes ### 5 Tierney &lt;NA&gt; US no ### 6 Tukey Exploratory Data Analysis US yes # dont&#39; throw out any data ( sets A and B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; ### 3 Ripley Spatial Statistics UK no ### 4 Ripley Stochastic Simulation UK no ### 5 Shakespeare &lt;NA&gt; England yes ### 6 Tierney &lt;NA&gt; US no ### 7 Tukey Exploratory Data Analysis US yes ### 8 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; Also note that the order of your datasets in the argument list will impact the inclusion or exclusion of elements. merge( x, y, all=F ) EQUALS merge( y, x, all=F ) merge( x, y, all.x=T ) DOES NOT EQUAL merge( y, x, all.x=T ) 9.4.1 The by.x and by.y Arguments When you use the default merge() function without specifying the variables to merge upon, the function will check for common variable names across the two datasets. If there are multiple, it will join the shared variables to create a new unique key. This might be problematic if that was not the intent. Take the example of combining fielding and salary data in the Lahman package. If we are not explicit about the merge variable, we may get odd results. Note that they two datasets share four ID variables. library( Lahman ) data( Fielding ) data( Salaries ) intersect( names(Fielding), names(Salaries) ) ### [1] &quot;playerID&quot; &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; # merge id int &lt;- intersect( names(Fielding), names(Salaries) ) paste( int[1],int[2],int[3],int[4], sep=&quot;.&quot; ) ### [1] &quot;playerID.yearID.teamID.lgID&quot; To avoid problems, be explicit using the by.x and by.x arguments to control which variable is used for the merge. head( merge( Salaries, Fielding ) ) ### yearID teamID lgID playerID salary stint POS G GS InnOuts PO A E DP ### 1 1985 ATL NL barkele01 870000 1 P 20 18 221 2 9 1 0 ### 2 1985 ATL NL bedrost01 550000 1 P 37 37 620 13 23 4 3 ### 3 1985 ATL NL benedbr01 545000 1 C 70 67 1698 314 35 4 1 ### 4 1985 ATL NL campri01 633333 1 P 66 2 383 7 13 4 3 ### 5 1985 ATL NL ceronri01 625000 1 C 91 76 2097 384 48 6 4 ### 6 1985 ATL NL chambch01 800000 1 1B 39 27 814 299 25 1 31 ### PB WP SB CS ZR ### 1 NA NA NA NA NA ### 2 NA NA NA NA NA ### 3 1 9 65 24 1 ### 4 NA NA NA NA NA ### 5 6 20 69 29 1 ### 6 NA NA NA NA NA head( merge( Salaries, Fielding, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) ) ### playerID yearID.x teamID.x lgID.x salary yearID.y stint teamID.y ### 1 aardsda01 2010 SEA AL 2750000 2009 1 SEA ### 2 aardsda01 2010 SEA AL 2750000 2015 1 ATL ### 3 aardsda01 2010 SEA AL 2750000 2006 1 CHN ### 4 aardsda01 2010 SEA AL 2750000 2008 1 BOS ### 5 aardsda01 2010 SEA AL 2750000 2013 1 NYN ### 6 aardsda01 2010 SEA AL 2750000 2012 1 NYA ### lgID.y POS G GS InnOuts PO A E DP PB WP SB CS ZR ### 1 AL P 73 0 214 2 5 0 1 NA NA NA NA NA ### 2 NL P 33 0 92 0 1 1 0 NA NA NA NA NA ### 3 NL P 45 0 159 1 5 0 1 NA NA NA NA NA ### 4 AL P 47 0 146 3 6 0 0 NA NA NA NA NA ### 5 NL P 43 0 119 1 5 0 0 NA NA NA NA NA ### 6 AL P 1 0 3 0 0 0 0 NA NA NA NA NA 9.5 Non-Unique Observations in ID Variables In some rare instances, you will need to merge to datasets that have non-singular elements in the unique key ID variables, meaning each observation / individual appears more than one time in the data. Note that in this case, for each occurance of an observation / individual in your X dataset, you will merge once with each occurance of the same observation / individual in the Y dataset. The result will be a multiplicative expansion of the size of your dataset. For example, if John appears on four separate rows of X, and three seperate rows of Y, the new dataset will contain 12 rows of John (4 x 3 = 12). dataset X contains four separate instances of an individual [ X1, X2, X3, X4 ] dataset Y contains three separate instances of an individual [ Y1, Y2, Y3 ] After the merge we have one row for each pair: X1-Y1 X1-Y2 X1-Y3 X2-Y1 X2-Y2 X2-Y3 X3-Y1 X3-Y2 X3-Y3 X4-Y1 X4-Y2 X4-Y3 For example, perhaps a sales company has a database that keeps track of biographical data, and sales performance. Perhaps we want to see if there is peak age for sales performance. We need to merge these datasets. bio &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), age=c(43,44,45) ) performance &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), sales=c(&quot;15k&quot;,&quot;20k&quot;,&quot;17k&quot;) ) # correct merge merge( bio, performance, by.x=c(&quot;name&quot;,&quot;year&quot;), by.y=c(&quot;name&quot;,&quot;year&quot;) ) ### name year age sales ### 1 John 2000 43 15k ### 2 John 2001 44 20k ### 3 John 2002 45 17k # incorrect merge merge( bio, performance, by.x=c(&quot;name&quot;), by.y=c(&quot;name&quot;) ) ### name year.x age year.y sales ### 1 John 2000 43 2000 15k ### 2 John 2000 43 2001 20k ### 3 John 2000 43 2002 17k ### 4 John 2001 44 2000 15k ### 5 John 2001 44 2001 20k ### 6 John 2001 44 2002 17k ### 7 John 2002 45 2000 15k ### 8 John 2002 45 2001 20k ### 9 John 2002 45 2002 17k It is good practice to check the size (number of rows) of your dataset before and after a merge. If it has expanded, chances are you either used the wrong unique IDs, or your dataset contains duplicates. 9.5.1 Example of Incorrect Merge Here is a tangible example using the Lahman baseball dataset. Perhaps we want to examine the relationship between fielding position and salary. The Fielding dataset contains fielding position information, and the Salaries dataset contains salary information. We can merge these two datasets using the playerID field. If we are not thoughtful about this, however, we will end up causing problems. Let’s look at an example using Kirby Pucket. kirby.fielding &lt;- Fielding[ Fielding$playerID == &quot;puckeki01&quot; , ] head( kirby.fielding ) ### playerID yearID stint teamID lgID POS G GS InnOuts PO A E DP ### 83848 puckeki01 1984 1 MIN AL OF 128 128 3377 438 16 3 4 ### 85157 puckeki01 1985 1 MIN AL OF 161 160 4213 465 19 8 5 ### 86489 puckeki01 1986 1 MIN AL OF 160 157 4155 429 8 6 3 ### 87896 puckeki01 1987 1 MIN AL OF 147 147 3820 341 8 5 2 ### 89264 puckeki01 1988 1 MIN AL OF 158 157 4049 450 12 3 4 ### 90685 puckeki01 1989 1 MIN AL OF 157 154 3985 438 13 4 3 ### PB WP SB CS ZR ### 83848 NA NA NA NA NA ### 85157 NA NA NA NA NA ### 86489 NA NA NA NA NA ### 87896 NA NA NA NA NA ### 89264 NA NA NA NA NA ### 90685 NA NA NA NA NA nrow( kirby.fielding ) ### [1] 21 kirby.salary &lt;- Salaries[ Salaries$playerID == &quot;puckeki01&quot; , ] head( kirby.salary ) ### yearID teamID lgID playerID salary ### 280 1985 MIN AL puckeki01 130000 ### 917 1986 MIN AL puckeki01 255000 ### 1610 1987 MIN AL puckeki01 465000 ### 2244 1988 MIN AL puckeki01 1090000 ### 2922 1989 MIN AL puckeki01 2000000 ### 3717 1990 MIN AL puckeki01 2816667 nrow( kirby.salary ) ### [1] 13 kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) head( select( kirby.field.salary, yearID.x, yearID.y, POS, G, GS, salary ) ) ### yearID.x yearID.y POS G GS salary ### 1 1984 1985 OF 128 128 130000 ### 2 1984 1986 OF 128 128 255000 ### 3 1984 1987 OF 128 128 465000 ### 4 1984 1988 OF 128 128 1090000 ### 5 1984 1989 OF 128 128 2000000 ### 6 1984 1990 OF 128 128 2816667 nrow( kirby.field.salary ) ### [1] 273 21*13 ### [1] 273 What we have done here is taken each year of fielding data, and matched it to every year of salary data. We can see that we have 21 fielding observations and 13 years of salary data, so our resulting dataset is 273 observation pairs. This merge also makes it difficult to answer the question of the relationship between fielding position and salary if players change positions over time. The correct merge in this case would be a merge on a playerID-yearID pair. We can create a unique key by combining playerID and yearID using paste(): head( paste( kirby.fielding$playerID, kirby.fielding$yearID, sep=&quot;.&quot;) ) ### [1] &quot;puckeki01.1984&quot; &quot;puckeki01.1985&quot; &quot;puckeki01.1986&quot; &quot;puckeki01.1987&quot; ### [5] &quot;puckeki01.1988&quot; &quot;puckeki01.1989&quot; But there is a simple solution as the merge function also allows for multiple variables to be used for a merge() command. kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=c(&quot;playerID&quot;,&quot;yearID&quot;), by.y=c(&quot;playerID&quot;,&quot;yearID&quot;) ) nrow( kirby.field.salary ) ### [1] 20 9.6 The %in% function Since we are talking about intersections and matches, I want to briefly introduce the %in% function. It is a combination of the two. The intersect() function returns a list of unique matches between two vectors. data(Salaries) data(Fielding) intersect( names(Salaries), names(Fielding) ) ### [1] &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; &quot;playerID&quot; The match() function returns the position of matched elements. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) match( x, y ) ### [1] 3 1 NA 1 The %in% function returns a logical vector, where TRUE signifies that the element in y also occurs in x. In other words, does a specific element in y belong to the intersection of x,y. This is very useful for creating subsets of data that belong to both sets. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;B&quot;,&quot;F&quot;,&quot;B&quot;) y %in% x # does each element of y occur anywhere in x? ### [1] TRUE FALSE TRUE TRUE FALSE TRUE y[ y %in% x] # keep only data that occurs in both ### [1] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; 9.7 The Match Function Often times we do not need to merge data, we may just need sort data in one dataset so that it matches the order of another dataset. This is accomplished using the match() function. Note that we can rearrange the order of a dataset by referencing the desired position. x &lt;- c(&quot;Second&quot;,&quot;Third&quot;,&quot;First&quot;) x ### [1] &quot;Second&quot; &quot;Third&quot; &quot;First&quot; x[ c(3,1,2) ] ### [1] &quot;First&quot; &quot;Second&quot; &quot;Third&quot; The match() function returns the positions of matches of its first vector to the second vector listed in the arguments. Or in other words, the order that vector 2 would need to follow to match vector 1. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;) cbind( x, y ) ### x y ### [1,] &quot;A&quot; &quot;B&quot; ### [2,] &quot;B&quot; &quot;D&quot; ### [3,] &quot;C&quot; &quot;A&quot; match( x, y ) ### [1] 3 1 NA match( y, x) # not a symmetric operation! ### [1] 2 NA 1 # In the y vector: # # [3]=A # [1]=B # [NA]=D (no match) order.y &lt;- match( x, y ) y[ order.y ] ### [1] &quot;A&quot; &quot;B&quot; NA We can see that match() returns the correct order to put y in so that it matches the order of x. In the re-ordered vector, the first element is the original third element A, the second element is the original first element B, and there is no third element because D did not match anything in x. Note the order of arguments in the function: match( data I want to match to , data I need to re-order ) We can use this position information to re-order y as follows: x &lt;- sample( LETTERS[1:15], size=10 ) y &lt;- sample( LETTERS[1:15], size=10 ) cbind( x, y ) ### x y ### [1,] &quot;B&quot; &quot;C&quot; ### [2,] &quot;D&quot; &quot;G&quot; ### [3,] &quot;I&quot; &quot;L&quot; ### [4,] &quot;N&quot; &quot;N&quot; ### [5,] &quot;E&quot; &quot;J&quot; ### [6,] &quot;C&quot; &quot;A&quot; ### [7,] &quot;A&quot; &quot;F&quot; ### [8,] &quot;J&quot; &quot;B&quot; ### [9,] &quot;O&quot; &quot;K&quot; ### [10,] &quot;G&quot; &quot;E&quot; order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;B&quot; &quot;B&quot; ### [2,] &quot;D&quot; NA ### [3,] &quot;I&quot; NA ### [4,] &quot;N&quot; &quot;N&quot; ### [5,] &quot;E&quot; &quot;E&quot; ### [6,] &quot;C&quot; &quot;C&quot; ### [7,] &quot;A&quot; &quot;A&quot; ### [8,] &quot;J&quot; &quot;J&quot; ### [9,] &quot;O&quot; NA ### [10,] &quot;G&quot; &quot;G&quot; # Note the result if you confuse the order or arguments order.y &lt;- match( y, x ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;B&quot; &quot;A&quot; ### [2,] &quot;D&quot; &quot;E&quot; ### [3,] &quot;I&quot; NA ### [4,] &quot;N&quot; &quot;N&quot; ### [5,] &quot;E&quot; &quot;B&quot; ### [6,] &quot;C&quot; &quot;F&quot; ### [7,] &quot;A&quot; NA ### [8,] &quot;J&quot; &quot;C&quot; ### [9,] &quot;O&quot; NA ### [10,] &quot;G&quot; &quot;J&quot; This comes in handy when we are matching information between two tables. For example, in GIS the map regions follow a specific order but your data does not. Create a color scheme for levels of your data, and then re-order the colors so they match the correct region on the map. In this example, we will look at unemployment levels by county. library( maps ) data( county.fips ) data( unemp ) map( database=&quot;county&quot; ) # assign a color to each level of unemployment, red = high, gray = medium, blue = low color.function &lt;- colorRampPalette( c(&quot;steelblue&quot;, &quot;gray70&quot;, &quot;firebrick&quot;) ) color.vector &lt;- cut( rank(unemp$unemp), breaks=7, labels=color.function( 7 ) ) color.vector &lt;- as.character( color.vector ) head( color.vector ) ### [1] &quot;#B28282&quot; &quot;#B28282&quot; &quot;#B22222&quot; &quot;#B25252&quot; &quot;#B28282&quot; &quot;#B22222&quot; # doesn&#39;t look quite right map( database=&quot;county&quot;, col=color.vector, fill=T, lty=0 ) # what went wrong here? # our unemployment data (and thus the color vector) follows a different order cbind( map.id=county.fips$fips, data.id=unemp$fips, color.vector )[ 2500:2510 , ] ### map.id data.id color.vector ### [1,] &quot;48011&quot; &quot;47149&quot; &quot;#B28282&quot; ### [2,] &quot;48013&quot; &quot;47151&quot; &quot;#B22222&quot; ### [3,] &quot;48015&quot; &quot;47153&quot; &quot;#B22222&quot; ### [4,] &quot;48017&quot; &quot;47155&quot; &quot;#B28282&quot; ### [5,] &quot;48019&quot; &quot;47157&quot; &quot;#B28282&quot; ### [6,] &quot;48021&quot; &quot;47159&quot; &quot;#B22222&quot; ### [7,] &quot;48023&quot; &quot;47161&quot; &quot;#B25252&quot; ### [8,] &quot;48025&quot; &quot;47163&quot; &quot;#B3B3B3&quot; ### [9,] &quot;48027&quot; &quot;47165&quot; &quot;#B28282&quot; ### [10,] &quot;48029&quot; &quot;47167&quot; &quot;#B25252&quot; ### [11,] &quot;48031&quot; &quot;47169&quot; &quot;#B25252&quot; # place the color vector in the correct order this.order &lt;- match( county.fips$fips, unemp$fips ) color.vec.ordered &lt;- color.vector[ this.order ] # colors now match their correct counties map( database=&quot;county&quot;, col=color.vec.ordered, fill=T, lty=0 ) title( main=&quot;Unemployment Levels by County in 2009&quot;) Note that elements can be recycled from your y vector: x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) cbind( x, y ) ### x y ### [1,] &quot;A&quot; &quot;B&quot; ### [2,] &quot;B&quot; &quot;D&quot; ### [3,] &quot;C&quot; &quot;A&quot; ### [4,] &quot;B&quot; &quot;F&quot; match( x, y ) ### [1] 3 1 NA 1 order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;A&quot; &quot;A&quot; ### [2,] &quot;B&quot; &quot;B&quot; ### [3,] &quot;C&quot; NA ### [4,] &quot;B&quot; &quot;B&quot; "]
]
