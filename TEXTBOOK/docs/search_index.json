[
["index.html", "Intro to Data Science for the Social Sector Welcome", " Intro to Data Science for the Social Sector Updated September 19, 2018 Welcome Welcome to the course text for Data Science for the Social Sector. This course introduces students to the field of data science and its applications in the public sector. Modern performance management and evaluation processes require strong data literacy and the ability to combine and analyze data from a variety of sources to inform managerial processes. We offer a practical, tools-based approach that is designed to build strong foundations for people that want to work as policy analysts or data-driven managers. We will cover data programming fundamentals, visualization, text analysis, automated reporting, and dynamic reporting using dashboards. The course is analytically rigorous, but no prior programming experience is assumed. The Data Science Process (Wickham &amp; Grolemund, 2016): A data scientist is a person who should be able to leverage existing data sources, and create new ones as needed in order to extract meaningful information and actionable insights. These insights can be used to drive business decisions and changes intended to achieve business goals… ‘The Perfect Data Scientist’ is the individual who is equally strong in business, programming, statistics, and communication. From: “What Is Data Science, and What Does a Data Scientist Do?” "],
["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 Navigation 1.2 Commenting Code 1.3 Help 1.4 Install Programs (packages) 1.5 Accessing Built-In Datasets in R", " Chapter 1 Introduction to R This lecture introduces you to basic operations when you first start using R such as navigation, the object-oriented framework, loading a package, and creating some data vectors. 1.1 Navigation You need to know a few operations to help you maneuver the R work environment, such as listing objects (datasets and functions) that are active, changing your working directory, listing available files, and finding help. 1.1.1 Setting Your Working Directory When you are ready to load data, R needs to know where to look for your files. You can check what is avaiable in the current directory (i.e. folder) by asking to list all of the current files using dir(). dir() If the file that you need is located in a different folder, you can change directories easily in R Studio by Session -&gt; Set working director -&gt; Choose directory (or Ctrl + Shift + H). If you are writing a script, you want to keep track of this step so that it can be reproduced. Use the function get.wd() to check your current working directory, and set.wd() to change. You need to specify your path as an argument to this function, such as. setwd( &quot;C:/user/projects/financial model&quot; ) NOTE! R uses unix style notation with forward slashes, so if you copy and paste from Windows it will look like this, with back slashes: setwd( &quot;C:\\user\\projects\\financial model&quot; ) You will need to change them around for it to work. It is best to save all of your steps in your scripts so that the analysis can be reproduced by yourself or others. In some cases you are doing exploratory or summary work, and you may want to find a file a quickly. You can use the file.choose() function to open a GUI to select your file directly. This function is used as an argument inside of a load data function. my.dat &lt;- read.csv( file.choose() ) 1.2 Commenting Code Most computer languages have a special character that is used to “comment out” lines so that it is not run by the program. It is used for two important purposes. First, we can add text to document our functions and it will not interfere with the program. And two, we can use it to run a program while ignoring some of the code, often for debugging purposes. The # hash tag is used for comments in R. ##============================================== ## ## Here is some documentation for this script ## ##============================================== x &lt;- 1:10 sum( x ) ### [1] 55 # y &lt;- 1:25 # not run # sum( y ) # not run 1.3 Help You will use the help functions frequently to figure out what arguments and values are needed for specific functions. Because R is very customizable, you will find that many functions have several or dozens of arguments, and it is difficult to remember the correct syntax and values. But don’t worry, to look them up all you need is the function name and a call for help: help( dotchart ) # opens an external helpfile If you just need to remind yourself which arguments are defined in a function, you can use the args() command: args( dotchart ) ### function (x, labels = NULL, groups = NULL, gdata = NULL, cex = par(&quot;cex&quot;), ### pt.cex = cex, pch = 21, gpch = 21, bg = par(&quot;bg&quot;), color = par(&quot;fg&quot;), ### gcolor = par(&quot;fg&quot;), lcolor = &quot;gray&quot;, xlim = range(x[is.finite(x)]), ### main = NULL, xlab = NULL, ylab = NULL, ...) ### NULL If you can’t recall a function name, you can list all of the functions from a specific package as follows: help( package=“stats” ) # lists all functions in stats package 1.4 Install Programs (packages) When you open R by default it will launch a core set of programs, called “packages” in R speak, that are use for most data operations. To see which packages are currently active use the search() function. search() ### [1] &quot;.GlobalEnv&quot; &quot;package:Lahman&quot; &quot;package:stargazer&quot; ### [4] &quot;package:pander&quot; &quot;package:dplyr&quot; &quot;package:bookdown&quot; ### [7] &quot;package:rmarkdown&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ### [10] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ### [13] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; These programs manage the basic data operations, run the core graphics engine, and give you basic statistical methods. The real magic for R comes from the over 7,000 contributed packages available on the CRAN: https://cran.r-project.org/web/views/ A package consists of custom functions and datasets that are generated by users. They are packaged together so that they can be shared with others. A package also includes documentation that describes each function, defines all of the arguments, and documents any datasets that are included. If you know a package name, it is easy to install. In R Studio you can select Tools -&gt; Install Packages and a list of available packages will be generated. But it is easier to use the install.packages() command. We will use the Lahman Package in this course, so let’s install that now. Description This package provides the tables from Sean Lahman’s Baseball Database as a set of R data.frames. It uses the data on pitching, hitting and fielding performance and other tables from 1871 through 2013, as recorded in the 2014 version of the database. See the documentation here: https://cran.r-project.org/web/packages/Lahman/Lahman.pdf install.packages( &quot;Lahman&quot; ) You will be asked to select a “mirror”. In R speak this just means the server from which you will download the package (choose anything nearby). R is a community of developers and universities that create code and maintain the infrastructure. A couple of dozen universities around the world host servers that contain copies of the R packages so that they can be easily accessed everywhere. If the package is successfully installed you will get a message similar to this: package ‘Lahman’ successfully unpacked and MD5 sums checked Once a new program is installed you can now open (“load” in R speak) the package using the library() command: library( &quot;Lahman&quot; ) If you now type search() you can see that Lahman has been added to the list of active programs. We can now access all of the functions and data that are available in the Lahman package. 1.5 Accessing Built-In Datasets in R One nice feature of R is that is comes with a bunch of built-in datasets that have been contributed by users are are loaded automatically. You can see the list of available datasets by typing: data() This will list all of the default datasets in core R packages. If you want to see all of the datasets available in installed packages as well use: data( package = .packages(all.available = TRUE) ) 1.5.1 Basic Data Operations Let’s ignore the underlying data structure right now and look at some ways that we might interact with data. We will use the USArrests dataset available in the core files. To access the data we need to load it into working memory. Anything that is active in R will be listed in the environment, which you can check using the ls() command. We will load the dataset using the data() command. remove( list=ls() ) ls() # nothing currently available ### character(0) data( &quot;USArrests&quot; ) ls() # data is now avaible for use ### [1] &quot;USArrests&quot; Now that we have loaded a dataset, we can start to access the variables and analyze relationships. Let’s get to know our dataset. names( USArrests ) # what variables are in the dataset? ### [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; nrow( USArrests ) # how many observations are there? ### [1] 50 dim( USArrests ) # a quick way to see rows and columns - the dimensions of the dataset ### [1] 50 4 row.names( head( USArrests ) ) # what are the obsevations (rows) in our data ### [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; ### [6] &quot;Colorado&quot; summary( USArrests ) # summary statistics of variables ### Murder Assault UrbanPop Rape ### Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 ### 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 ### Median : 7.250 Median :159.0 Median :66.00 Median :20.10 ### Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 ### 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 ### Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 We can see that the dataset consists of four variables: Murder, Assault, UrbanPop, and Rape. We also see that our unit of analysis is the state. But where does the data come from, and how are these variables measured? To see the documentation for a specific dataset you will need to use the help() function: help( &quot;USArrests&quot; ) We get valuable information about the source and metrics: Description This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas. Format A data frame with 50 observations on 4 variables. Murder: numeric Murder arrests (per 100,000) Assault: numeric Assault arrests (per 100,000) UrbanPop: numeric Percent urban population Rape: numeric Rape arrests (per 100,000) To access a specific variable inside of a dataset, you will use the $ operator between the dataset name and the variable name: summary( USArrests$Murder ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 0.800 4.075 7.250 7.788 11.250 17.400 summary( USArrests$Assault ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 45.0 109.0 159.0 170.8 249.0 337.0 # Is there a relationship between urban density and crime? plot( USArrests$UrbanPop, USArrests$Murder, frame.plot=F, pch=19, col=&quot;gray&quot; ) abline( lm( USArrests$Murder ~ USArrests$UrbanPop ), col=&quot;red&quot; ) 1.5.2 Using the Lahman Data Let’s take a look at some of the data available in the Lahman package. data( package = &quot;Lahman&quot; ) # All datasets in package &quot;Lahman&quot;: TABLE NAME DEFITION AllstarFull AllstarFull table Appearances Appearances table AwardsManagers AwardsManagers table AwardsPlayers AwardsPlayers table AwardsShareManagers AwardsShareManagers table AwardsSharePlayers AwardsSharePlayers table Batting Batting table BattingPost BattingPost table CollegePlaying CollegePlaying table Fielding Fielding table FieldingOF FieldingOF table FieldingPost FieldingPost data HallOfFame Hall of Fame Voting Data LahmanData Lahman Datasets Managers Managers table ManagersHalf ManagersHalf table Master Master table Pitching Pitching table PitchingPost PitchingPost table Salaries Salaries table Schools Schools table SeriesPost SeriesPost table Teams Teams table TeamsFranchises TeamFranchises table TeamsHalf TeamsHalf table battingLabels Variable Labels fieldingLabels Variable Labels pitchingLabels Variable Labels We see that we have lots of datasets to choose from here. I will use the Master dataset, which is a list of all of the Major League Baseball players over the past century, and their personal information. library( Lahman ) # loads Lahman package data( Master ) head( Master ) Here are some common functions for exploring datasets: names( Master ) # variable names nrow( Master ) # 18,354 players included summary( Master ) # descriptive statistics of variables We can use help(Master) to get information about the dataset, including a data dictionary. help( Master ) MASTER TABLE Description Master table - Player names, DOB, and biographical info. This file is to be used to get details about players listed in the Batting, Pitching, and other files where players are identified only by playerID. Usage data(Master) Format A data frame with 19105 observations on the following 26 variables. playerID: A unique code asssigned to each player. The playerID links the data in this file with records on players in the other files. birthYear: Year player was born birthMonth: Month player was born birthDay: Day player was born birthCountry: Country where player was born birthState: State where player was born birthCity: City where player was born deathYear: Year player died deathMonth: Month player died deathDay: Day player died deathCountry: Country where player died deathState: State where player died deathCity: City where player died nameFirst: Player’s first name nameLast: Player’s last name nameGiven: Player’s given name (typically first and middle) weight: Player’s weight in pounds height: Player’s height in inches bats: a factor: Player’s batting hand (left (L), right (R), or both (B)) throws: a factor: Player’s throwing hand (left(L) or right(R)) debut: Date that player made first major league appearance finalGame: Date that player made first major league appearance (blank if still active) retroID: ID used by retrosheet, http://www.retrosheet.org/ bbrefID: ID used by Baseball Reference website, http://www.baseball-reference.com/ birthDate: Player’s birthdate, in as.Date format deathDate: Player’s deathdate, in as.Date format Details debut, finalGame were converted from character strings with as.Date. Source Lahman, S. (2016) Lahman’s Baseball Database, 1871-2015, 2015 version, http://www.seanlahman.com/baseball-archive/statistics/ tab &lt;- prop.table( table( Master$birthMonth ) ) names(tab) &lt;- c(&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;,&quot;Dec&quot;) dotchart( tab, pch=19, xlab = &quot;Proportion of MLB Players&quot;, ylab = &quot;Birth Month&quot; ) "],
["functions.html", "Chapter 2 Functions 2.1 Key Concepts 2.2 Computer Programs as Recipes 2.3 Example Function 2.4 Default Argument Values 2.5 Assignment", " Chapter 2 Functions 2.1 Key Concepts Figure 2.1: Anatomy of a function Figure 2.2: Assignment of output values After reading this chapter you should be able to define the following: function argument object assignment 2.2 Computer Programs as Recipes Computer programs are powerful because they allow us to codify recipes for complex tasks, save them, share them, and build upon them. In the simplest form, a computer program is like a recipe. We have inputs, steps, and outputs. Ingredients: 1/3 cup butter 1/2 cup sugar 1/4 cup brown sugar 2 teaspoons vanilla extract 1 large egg 2 cups all-purpose flour 1/2 teaspoon baking soda 1/2 teaspoon kosher salt 1 cup chocolate chips Instructions: Preheat the oven to 375 degrees F. In a large bowl, mix butter with the sugars until well-combined. Stir in vanilla and egg until incorporated. Addflour, baking soda, and salt. Stir in chocolate chips. Bake for 10 minutes. In R, the recipe would look something like this: function( butter=0.33, sugar=0.5, eggs=1, flour=2, temp=375 ) { dry.goods &lt;- combine( flour, sugar ) batter &lt;- mix( dry.goods, butter, eggs ) cookies &lt;- bake( batter, temp, time=10 ) return( cookies ) } Note that this function to make cookies relies on other functions for each step, combine(), mix(), and bake(). Each of these functions would have to be defined as well, or more likely someone else in the open source community has already written a package called “baking” that contains simple functions for cooking so that you can use them for more complicated recipes. You will find that R allows you to conduct powerful analysis primarily because you can build on top of and extend a lot of existing functionality. 2.3 Example Function As you get started in R you will be working with existing functions, not writing your own. It is, however, constructive to see how one is created. This example demonstrates the use of a mortgage calculator that will take a loan size, term, and interest rate and return a monthly payment. calcMortgage &lt;- function( principal, years, APR ) { months &lt;- years * 12 int.rate &lt;- APR / 12 # amortization formula monthly.payment &lt;- ( principal * int.rate ) / (1 - (1 + int.rate)^(-months) ) monthly.payment &lt;- round( monthly.payment, 2 ) return( monthly.payment ) } Let’s then see what the payments will be for a: $100,000 loan 30-year mortgage 5% annual interest rate calcMortgage( principal=100000, years=30, APR=0.05 ) ### [1] 536.82 2.4 Default Argument Values Note that the loan function needs all three of the input values in order to calculate the loan size. If we were to omit one required value, we would get an error. calcMortgage( principal=100000 ) # Error in calcMortgage(APR = 0.05, principal = 1e+05): # argument &quot;years&quot; is missing, with no default When creating functions, we might have a good idea of typical use cases. If true, we can try to guess at reasonable user parameters. For example, perhaps we are working at a bank where most of the customers take out 30-year mortgages, and interest rates have been stable at 5 percent. We can set these as default values when we create the function. calcMortgage &lt;- function( principal, years=30, APR=0.05 ) ... We can now run the function while omitting arguments, as long as they have defaults assigned. calcMortgage( principal=100000 ) ### [1] 536.82 2.5 Assignment When we call a function in R, the default behavior of the function is typically to print the results on the screen: calcMortgage( principal=100000 ) ### [1] 536.82 If we are creating a script, however, we often need to save the function outputs at each step. We can do this by assigning output to a new variable. payments.15.year &lt;- calcMortgage( years=15, principal=100000 ) payments.30.year &lt;- calcMortgage( years=30, principal=100000 ) These values are then stored, and can be used later or printed by typing the object name: payments.15.year ### [1] 790.79 payments.30.year ### [1] 536.82 Note that variable names can include periods or underscores. They can also include numbers, but they cannot start with a number. Like everything in R, they will be case sensitive. "],
["vectors.html", "VECTORS", " VECTORS Vectors are the building blocks of data programming in R, so they are extremely important concepts. This section will cover basic principles of working with vectors in the R language, including the different types of vectors (data types or classes), and common functions used on vectors. "],
["data-types.html", "Chapter 3 Data Types 3.1 Key Concepts 3.2 Vectors 3.3 Common Vectors Functions 3.4 The Combine Function 3.5 Casting 3.6 Numeric Vectors 3.7 Character Vectors 3.8 Factors 3.9 Logical Vectors 3.10 Generating Vectors 3.11 Variable Transformations 3.12 Missing Values: NA’s 3.13 Datasets", " Chapter 3 Data Types 3.1 Key Concepts Figure 3.1: Components of a Vector Figure 3.2: Basic data types in R 3.2 Vectors Generally speaking a vector is a set of numbers, words, or other values stored sequentially: [ 1, 2, 3] [ apple, orange, pear ] [ TRUE, FALSE, FALSE ] In social sciences, a vector usually represents a variable in a dataset, often as a column in a spreadsheet. There are four primary vector types (“classes”) in R: Class Description numeric Typical variable of only numbers character A vector of letters or words, always enclosed with quotes factor Categories which represent groups, like treatment and control logical A vector of TRUE and FALSE to designate which observations fit a criteria Each vector or dataset has a “class” that tells R the data type. These different vectors can be combined into three different types of datasets (data frames, matrices, and lists), which will be discussed below. x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) x4 &lt;- c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) x5 &lt;- x4 == &quot;treatment&quot; dat &lt;- data.frame( name=x2, sex=x3, treat=x4, is.treat=x5, strength=x1 ) name sex treat is.treat strength adam male treatment TRUE 167 jamal male control FALSE 185 linda female treatment TRUE 119 sriti female control FALSE 142 R keeps track of the data type of each object, which can be ascertained using the class() function. class( x ) ### $name ### [1] &quot;character&quot; ### ### $sex ### [1] &quot;factor&quot; ### ### $treat ### [1] &quot;character&quot; ### ### $is.treat ### [1] &quot;logical&quot; ### ### $strength ### [1] &quot;numeric&quot; class( dat ) ### [1] &quot;data.frame&quot; 3.3 Common Vectors Functions You will spend a lot of time creating data vectors, transforming variables, generating subsets, cleaning data, and adding new observations. These are all accomplished through functions() that act on vectors. We often need to know how many elements belong to a vector, which we find with the length() function. x1 ### [1] 167 185 119 142 length( x1 ) ### [1] 4 3.4 The Combine Function We often need to combine several elements into a single vector, or combine two vectors to form one. This is done using the c() function. c(1,2,3) # create a numeric vector ### [1] 1 2 3 c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) # create a character vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Combining two vectors: x &lt;- 1:5 y &lt;- 10:15 z &lt;- c(x,y) z ### [1] 1 2 3 4 5 10 11 12 13 14 15 Combining two vectors of different data types: x &lt;- c(1,2,3) y &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) z &lt;- c(x,y) z ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; 3.5 Casting You can easily move from one data type to another by casting a specific type as another type: x &lt;- 1:5 x ### [1] 1 2 3 4 5 as.character(x) ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; y &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) y ### [1] TRUE FALSE TRUE TRUE FALSE as.numeric( y ) ### [1] 1 0 1 1 0 as.character( y ) ### [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; &quot;TRUE&quot; &quot;FALSE&quot; But in some cases it might not make sense to cast one variable type as another. z &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) z ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; as.numeric( z ) ### [1] NA NA NA Casting will often be induced automatically when you try to combine different types of data. For example, when you add a character element to a numeric vector, the whole vector will be cast as a character vector. x1 &lt;- 1:5 x1 ### [1] 1 2 3 4 5 x1 &lt;- c( x1, &quot;a&quot; ) # a vector can only have one data type x1 # all numbers silently recast as characters ### [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;a&quot; If you consider the example above, when a numeric and character vector are combined all elements are re-cast as strings because numbers can be represented as characters but not vice-versa. R tries to select a reasonable default type, but sometimes casting will create some strange and unexpected behaviors. Consider some of these examples. What do you think each will produce? x1 &lt;- c(1,2,3) # numeric x2 &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) # character x3 &lt;- c(TRUE,FALSE,TRUE) # logical x4 &lt;- factor( c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) ) # factor case1 &lt;- c( x1, x3 ) case2 &lt;- c( x2, x3 ) case3 &lt;- c( x1, x4 ) case4 &lt;- c( x2, x4 ) The answers to case1 and case2 are somewhat intuitive. case1 # combine a numeric and logical vector ### [1] 1 2 3 1 0 1 case2 # combine a character and logical vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; Recall that TRUE and FALSE are often represented as 1 and 0 in datasets, so they can be recast as numeric elements. The numbers 2 and 3 have no meaning in a logical vector, so we can’t cast a numeric vector as a logical vector. case3 and case4 are a little more nuanced. See the section on factors below to make sense of them. case3 # combine a numeric and factor vector ### [1] 1 2 3 1 2 3 case4 # combine a character and factor vector ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; TIP: When you read data in from outside sources, the input functions often will cast character or numeric vectors as factors if they contain a low number of elements. See the section on factors below for special instructions on moving from factors to numeric vectors. 3.6 Numeric Vectors There are some specific things to note about each vector type. Math operators will only work on numeric vectors. summary( x1 ) ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 1.0 1.5 2.0 2.0 2.5 3.0 Note that if we try to run this mathematical function we get an error: sum( x2 ) # Error in sum(x2) : invalid &#39;type&#39; (character) of argument Many functions in R are sensitive to the data type of vectors. Mathematical functions, for example, do not make sense when applied to text (character vectors). In many cases R will give an error. In some cases R will silently re-cast the variable, then perform the operation. Be watchful for when silent re-casting occurs because it might have unwanted side effects, such as deleting data or re-coding group levels in the wrong way. 3.6.1 Integers Are Simple Numeric Vectors The integer vector is a special type of numeric vector. It is used to save memory since integers require less space than numbers that contain decimals points (you need to allocate space for the numbers to the left and the numbers to the right of the decimal). Google “computer memory allocation” if you are interested in the specifics. If you are doing advanced programming you will be more sensitive to memory allocation and the speed of your code, but in the intro class we will not differentiate between the two types of number vectors. In most cases they result in the same results, unless you are doing advanced numerical analysis where rounding errors matter. n &lt;- 1:5 n ### [1] 1 2 3 4 5 class( n ) ### [1] &quot;integer&quot; n[ 2 ] &lt;- 2.01 n # all elements converted to decimals ### [1] 1.00 2.01 3.00 4.00 5.00 class( n ) ### [1] &quot;numeric&quot; 3.7 Character Vectors The most important rule to remember with this data type: when creating character vectors, all text must be enclosed by quotation marks. This one works: c( &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ) # this works ### [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; This one will not: c( a, b, c ) # Error: object &#39;a&#39; not found When you type characters surrounded by quotes then R knows you are creating new text (“strings” in programming speak). When you type characters that are not surrounded by quotes, R thinks that you are looking for an object in the environment, like the variables we have already created. It gets confused when it doesn’t find the object that you typed. In generate, you will use quotes when you are creating character vectors, and for arguments in functions. You do not use quotes when you are referencing an active object. An active object is typically a dataset or vector that you have imported or created. You can print a list of all active objects with the ls() function. 3.7.1 Quotes in Arguments When you first start using R it can be confusing about when quotes are needed around arguments. Take the following example of the color argument (col=) in the plot() function. strength &lt;- c(167,185,119,142) name &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) group &lt;- factor( c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) ) plot( strength, col=&quot;blue&quot;, ... ) plot( strength, col=group, ... ) In the first example we are using a text string as an argument to specify a color (col=&quot;blue&quot;), so it must be enclosed by quotes because it is text. In the second example R selects the color based upon group membership specified by the factor called ‘group’ (treatment or control). Since the argument is now referencing an object (col=group), we do not use quotes. The exception here is when your argument requires a number. Numbers are not passed with quotes, or they would be cast as text. 3.8 Factors When there are categorical variables within our data, or groups, then we use a special vector to keep track of these groups. We could just use numbers (1=female, 0=male) or characters (“male”,“female”), but factors are useful for two reasons. First, it saves memory. Text is very “expensive” in terms of memory allocation and processing speed, so using simpler data structure makes R faster. Second, when a variable is set as a factor, R recognizes that it represents a group and it can deploy object-oriented functionality. When you use a factor in analysis, R knows that you want to split the analysis up by groups. height &lt;- c( 70, 68, 62, 64, 72, 69, 58, 63 ) strength &lt;- c(167,185,119,142,175,204,124,117) sex &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot; ) ) plot( height, strength ) # simple scatter plot plot( sex, strength ) # box and whisker plot Factors are more memory efficient than character vectors because they store the underlying data as a numeric vector instead of a categorical (text) vector. Each group in the data is assigned a number, and when printing items the program only has to remember which group corresponds to which number: sex ### [1] male male female female male male female female ### Levels: female male as.numeric( sex ) ### [1] 2 2 1 1 2 2 1 1 # male = 2 # female = 1 If you print a factor, the computer just replaces each category designation with its name (2 would be replaced with “male” in this example). These replacements can be done in real time without clogging the memory of your computer as they don’t need to be saved. In some instances a categorical variable might be represented by numbers. For example, grades 9-12 for high school kids. The very important rule to remember with factors is you can’t move directly from the factor to numeric using the as.numeric() casting function. This will give you the underlying data structure, but will not give you the category names. To get these, you need the as.character casting function. grades &lt;- sample( x=9:12, size=10, replace=T ) grades ### [1] 12 10 12 9 11 10 12 11 12 12 grades &lt;- as.factor( grades ) grades ### [1] 12 10 12 9 11 10 12 11 12 12 ### Levels: 9 10 11 12 as.numeric( grades ) ### [1] 4 2 4 1 3 2 4 3 4 4 as.character( grades ) ### [1] &quot;12&quot; &quot;10&quot; &quot;12&quot; &quot;9&quot; &quot;11&quot; &quot;10&quot; &quot;12&quot; &quot;11&quot; &quot;12&quot; &quot;12&quot; # to get back to the original numeric vector as.numeric( as.character( grades )) ### [1] 12 10 12 9 11 10 12 11 12 12 Note that when sub-setting a factor, it will retain all of the original levels, even when they are not in use. In this example, there are 37 teams in the Lahman dataset (some of them defunct) and 16 teams in the National League in 2002. But after applying the year and league subsets you will still have 37 levels. # there are only 16 teams in the NL in 2002 sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; After applying a subset, in order to remove the unused factor levels you need to apply either droplevels(), or else recast your factor as a new factor. For example: sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; # fix in one of two equivalent ways: # # nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) # nl.sals$teamID &lt;- factor( nl.sals$teamID ) levels( nl.sals$teamID ) ### [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; ### [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; ### [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; ### [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; ### [45] &quot;WAS&quot; &quot;WSN&quot; nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) levels( nl.sals$teamID ) ### [1] &quot;ARI&quot; &quot;ATL&quot; &quot;CHN&quot; &quot;CIN&quot; &quot;COL&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;LAN&quot; &quot;MIL&quot; &quot;MON&quot; &quot;NYN&quot; ### [12] &quot;PHI&quot; &quot;PIT&quot; &quot;SDN&quot; &quot;SFN&quot; &quot;SLN&quot; TIP: When reading data from Excel spreadsheets (usually saved in the comma separated value or CSV format), remember to include the following argument to prevent the creation of factors, which can produce some annoying behaviors. dat &lt;- read.csv( &quot;filename.csv&quot;, stringsAsFactors=F ) 3.9 Logical Vectors Logical vectors are collections of a set of TRUE and FALSE statements. Logical statements allow us to define groups based upon criteria, then decide whether observations belong to the group. A logical statement is one that contains a logical operator, and returns only TRUE, FALSE, or NA values. Logical vectors are important because organizing data into these sets is what drives all of the advanced data analytics (set theory is at the basis of mathematics and computer science). name sex treat strength adam male treatment 167 jamal male control 185 linda female treatment 119 sriti female control 142 dat$name == &quot;sriti&quot; ### [1] FALSE FALSE FALSE TRUE dat$sex == &quot;male&quot; ### [1] TRUE TRUE FALSE FALSE dat$strength &gt; 180 ### [1] FALSE TRUE FALSE FALSE When defining logical vectors, you can use the abbreviated versions of T for TRUE and F for FALSE. z1 &lt;- c(T,T,F,T,F,F) z1 ### [1] TRUE TRUE FALSE TRUE FALSE FALSE Typically logical vectors are used in combination with subset operators to identify specific groups in the data. # isolate data on all of the females in the dataset dat[ dat$sex == &quot;female&quot; , ] ### name sex treat strength ### 3 linda female treatment 119 ### 4 sriti female control 142 See the next chapter for more details on subsets. 3.10 Generating Vectors You will often need to generate vectors for data transformations or simulations. Here are the most common functions that will be helpful. # repeat a number, or series of numbers rep( x=9, times=5 ) ### [1] 9 9 9 9 9 rep( x=c(5,7), times=5 ) ### [1] 5 7 5 7 5 7 5 7 5 7 rep( x=c(5,7), each=5 ) ### [1] 5 5 5 5 5 7 7 7 7 7 rep( x=c(&quot;treatment&quot;,&quot;control&quot;), each=5 ) # also works to create categories ### [1] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; ### [6] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; # create a sequence of numbers seq( from=1, to=15, by=1 ) ### [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 seq( from=1, to=15, by=3 ) ### [1] 1 4 7 10 13 1:15 # shorthand if by=1 ### [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # create a random sample hat &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;f&quot;) sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;c&quot; &quot;f&quot; &quot;b&quot; sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;f&quot; &quot;a&quot; &quot;c&quot; sample( x=hat, size=3, replace=FALSE ) ### [1] &quot;b&quot; &quot;a&quot; &quot;b&quot; # for multiple samples use replacement sample( x=hat, size=10, replace=TRUE ) ### [1] &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;f&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;b&quot; &quot;f&quot; # create data that follows a normal curve iq &lt;- rnorm( n=1000, mean=100, sd=15 ) hist( iq, col=&quot;gray&quot; ) 3.11 Variable Transformations When we create a new variable from existing variables, it is called a ‘transformation’. This is very common in data science. Crime is measures by the number of assaults per 100,000 people, for example (crime / pop). A batting average is the number of hits divided by the number of at bats. In R, mathematical operations are vectorized, which means that operations are performed on the entire vector all at once. This makes transformations fast and easy. x &lt;- 1:10 x + 5 ### [1] 6 7 8 9 10 11 12 13 14 15 x * 5 ### [1] 5 10 15 20 25 30 35 40 45 50 R uses a convention called “recycling”, which means that it will re-use elements of a vector if necessary. In the example below the x vector has 10 elements, but the y vector only has 5 elements. When we run out of y, we just start over from the beginning. This is powerful in some instances, but can be dangerous in others if you don’t realize that that it is happening. x &lt;- 1:10 y &lt;- 1:5 x + y ### [1] 2 4 6 8 10 7 9 11 13 15 x * y ### [1] 1 4 9 16 25 6 14 24 36 50 # the colors are recycled plot( 1:5, 1:5, col=c(&quot;red&quot;,&quot;blue&quot;), pch=19, cex=3 ) Here is an example of recycling gone wrong: x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x4 &lt;- c( &quot;treatment&quot;,&quot;contro&quot;,&quot;treatment&quot;,&quot;control&quot; ) dat &lt;- data.frame( name=x2, sex=x3, treat=x4, strength=x1 ) # create a subset of data of all female study participants dat$sex == &quot;female&quot; ### [1] FALSE FALSE TRUE TRUE these &lt;- dat$sex == &quot;female&quot; dat[ these, ] # correct subset ### name sex treat strength ### 3 linda female treatment 119 ### 4 sriti female control 142 # same thing, but i mess is up - the female element is recycled in the overwrite dat$sex = &quot;female&quot; # whoops just over-wrote my data! should be double equal these &lt;- dat$sex == &quot;female&quot; dat[ these , ] ### name sex treat strength ### 1 adam female treatment 167 ### 2 jamal female contro 185 ### 3 linda female treatment 119 ### 4 sriti female control 142 3.12 Missing Values: NA’s Missing values are coded differently in each data analysis program. SPSS uses a period, for example. In R, missing values are coded as “NA”. The important thing to note is that R wants to make sure you know there are missing values if you are conducting analysis. As a result, it will give you the answer of “NA” when you try to do math with a vector that includes a missing value. You have to ask it explicitly to ignore the missing value. x5 &lt;- c( 1, 2, 3, 4 ) x5 ### [1] 1 2 3 4 sum( x5 ) ### [1] 10 mean( x5 ) ### [1] 2.5 x5 &lt;- c( 1, 2, NA, 4 ) x5 ### [1] 1 2 NA 4 # should missing values be treated as zeros or dropped? sum( x5 ) ### [1] NA mean( x5 ) ### [1] NA sum( x5, na.rm=T ) # na.rm=T argument drops missing values ### [1] 7 mean( x5, na.rm=T ) # na.rm=T argument drops missing values ### [1] 2.333333 You cannot use the == operator to identify missing values in a dataset. There is a special is.na() function to locate all of the missing values in a vector. x5 ### [1] 1 2 NA 4 x5 == NA # this does not do what you want ### [1] NA NA NA NA is.na( x5 ) # much better ### [1] FALSE FALSE TRUE FALSE ! is.na( x5 ) # if you want to create a selector vector to drop missing values ### [1] TRUE TRUE FALSE TRUE x5[ ! is.na(x5) ] ### [1] 1 2 4 x5[ is.na(x5) ] &lt;- 0 # replace missing values with zero 3.13 Datasets When multiple vectors represent data from a single sample, they are typically combined into a dataset. There are three main types of datasets that we will use in this class. Class Description data frame A typical data set comprised of several variables matrix A data set comprised of only numbers, used for matrix math list The grab bag of data structures - several vectors held together 3.13.1 Data Frames The most familiar spreadsheet-type data structure is called a data frame in R. It consists of rows, which represent observations, and columns, which represent variables. data( USArrests ) dim( USArrests ) # number of rows by number of columns ### [1] 50 4 names( USArrests ) # variable names or column names ### [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; row.names( USArrests ) ### [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ### [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; ### [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; ### [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ### [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; ### [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; ### [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; ### [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; ### [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ### [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; ### [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; ### [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; ### [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; head( USArrests ) # print first six rows of the data ### Murder Assault UrbanPop Rape ### Alabama 13.2 236 58 21.2 ### Alaska 10.0 263 48 44.5 ### Arizona 8.1 294 80 31.0 ### Arkansas 8.8 190 50 19.5 ### California 9.0 276 91 40.6 ### Colorado 7.9 204 78 38.7 3.13.2 Matrices A matrix is also a rectangular data object that consists of collections of vectors, but it is special in the sense that it only has numeric vectors and no variable names. mat &lt;- matrix( 1:20, nrow=5 ) mat ### [,1] [,2] [,3] [,4] ### [1,] 1 6 11 16 ### [2,] 2 7 12 17 ### [3,] 3 8 13 18 ### [4,] 4 9 14 19 ### [5,] 5 10 15 20 names( mat ) ### NULL dim( mat ) ### [1] 5 4 as.data.frame( mat ) # creates variable names ### V1 V2 V3 V4 ### 1 1 6 11 16 ### 2 2 7 12 17 ### 3 3 8 13 18 ### 4 4 9 14 19 ### 5 5 10 15 20 These are used almost exclusively for matrix algebra operations, which are fundamental to mathematical statistics. We will not use matrices in this course. 3.13.3 Lists The list is the most flexible data structure. It is created by sticking a bunch of unrelated vectors or datasets together. For example, when you run a regression you generate a bunch of interesting information. This information is saved as a list. x &lt;- 1:100 y &lt;- 2*x + rnorm( 100, 0, 10) m.01 &lt;- lm( y ~ x ) names( m.01 ) ### [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ### [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ### [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; m.01$coefficients ### (Intercept) x ### -2.802059 2.028025 m.01$residuals ### 1 2 3 4 5 ### 3.28672324 -9.40279512 -1.84804310 0.81599414 -5.85309744 ### 6 7 8 9 10 ### 3.66257442 -6.52474920 -6.68880912 11.59620665 -2.15426698 ### 11 12 13 14 15 ### 4.94802081 12.02075213 -14.82711970 -7.63986729 3.35081107 ### 16 17 18 19 20 ### -1.25575702 6.61006643 -3.07613066 -17.89888147 14.58827544 ### 21 22 23 24 25 ### 10.98893254 -5.01164704 -5.65158153 3.36361748 22.17009392 ### 26 27 28 29 30 ### 28.97317178 -8.90165052 -4.13638871 -8.84752790 -2.56869222 ### 31 32 33 34 35 ### 11.12977416 -0.12801566 2.22441778 1.82085749 -28.46651852 ### 36 37 38 39 40 ### -5.99482055 -14.65210081 -1.36946614 -1.95487881 1.55776732 ### 41 42 43 44 45 ### 0.75204879 -15.16122598 0.06860676 16.83947461 -2.25461550 ### 46 47 48 49 50 ### 13.13863625 -9.33211599 11.19292530 -4.67346587 12.36853192 ### 51 52 53 54 55 ### -17.00217155 -1.23553541 -4.48849874 13.61813012 8.96199819 ### 56 57 58 59 60 ### 5.48170467 -0.69011300 7.63638220 9.88140337 -4.41554747 ### 61 62 63 64 65 ### -4.26295791 1.68863461 10.52619139 10.50943520 -4.45523701 ### 66 67 68 69 70 ### 0.73508610 -4.45122248 -3.69334468 7.12089415 -4.08352113 ### 71 72 73 74 75 ### -6.49667839 0.72539144 -7.55254297 -12.64800630 -12.03107788 ### 76 77 78 79 80 ### -7.10010593 9.47019543 -10.95716915 -4.29757544 10.32652964 ### 81 82 83 84 85 ### 1.29587192 7.52642447 -1.91174788 -5.31639417 -20.27013724 ### 86 87 88 89 90 ### 2.64283854 13.28543415 15.44876937 3.49751355 2.76711475 ### 91 92 93 94 95 ### 10.15113559 -9.55406103 -5.38805320 -0.13030861 13.37283304 ### 96 97 98 99 100 ### 5.29326182 1.62048968 10.14706466 -14.48316759 -18.00960451 m.01$call ### lm(formula = y ~ x) These output are all related to the model we have run, so they are kept organized by the list so they can be used for various further steps like comparing models or checking for model fit. A data frame is a bit more rigid that a list in that you cannot combine elements that do not have the same dimensions. # new.dataframe &lt;- data.frame( m.01$coefficients, m.01$residuals, m.01$call ) # # these will fail because the vectors have different lengths "],
["logical-statements.html", "Chapter 4 Logical Statements 4.1 Key Concepts 4.2 Operators 4.3 Selector Vectors 4.4 Usefulness of Selector Vectors 4.5 Compound Logical Statements 4.6 NAs in Logical Statements 4.7 Subsets", " Chapter 4 Logical Statements th { text-align: left; } td { text-align: left; } 4.1 Key Concepts Figure 4.1: Logical statements define group membership Logical statements are used to translate regular language into computer code. Many of our data analysis problems start by defining our sub-groups of interest. What percentage of men over 30 are bald? these &lt;- gender == &quot;male&quot; &amp; age &gt; 30 &amp; hair == FALSE mean( these ) Do bearded men earn more than men without beards? We need to make sure statements are correct. For example, the complement of “men with beards” is not men without beards, it is men without beards OR women (with or without beards). Figure 4.2: Compound statements can be tricky bearded.men &lt;- gender == &quot;male&quot; &amp; beard == TRUE proper.gentlemen &lt;- gender == &quot;male&quot; &amp; beard == FALSE mean( salary[ bearded.men ] ) mean( salary[ ! bearded.men ] ) # this is incorrect !!! mean( salary[ proper.gentlemen ] ) 4.2 Operators Logical operators are the most basic type of data programming and the core of many types of data analysis. Most of the time we are not conducting fancy statistics, we just want to identify members of a group (print all of the females from the study), or describe things that belong to a subset of the data (compare the average price of houses with garages to houses without garages). In order to accomplish these simple tasks we need to use logic statements. A logic statement answers the question, does an observation belong to a group. Many times groups are simple. Show me all of the professions that make over $100k a year, for example. Sometimes groups are complex. Identify the African American children from a specific zip code in Chicago that live in households with single mothers. You will use nine basic logical operators: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to x | y x OR y x &amp; y x AND y ! opposite of [ ] subset Logical operators create logical vectors, a vector that contains only TRUE or FALSE. The TRUE means that the observation belongs to the group, FALSE means it does not. x1 &lt;- c( 7, 9, 1, 2 ) x1 &gt; 7 ### [1] FALSE TRUE FALSE FALSE x1 &gt;= 7 ### [1] TRUE TRUE FALSE FALSE x1 == 9 | x1 == 1 ### [1] FALSE TRUE TRUE FALSE gender &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) gender == &quot;female&quot; ### [1] FALSE FALSE TRUE TRUE Note that the logical statement for “equals” is written with two equal signs. This is important to remember, because using a single equal sign can introduce subtle errors into your analysis. x1 &lt;- c( 7, 9, 1, 2 ) x1 == 9 ### [1] FALSE TRUE FALSE FALSE x1 = 9 # don&#39;t use a single equals operator! it overwrites your variable x1 ### [1] 9 We can write compound logical statements using the AND and OR operators: gender &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) study.group &lt;- c(&quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) gender == &quot;female&quot; &amp; study.group == &quot;treatment&quot; ### [1] FALSE FALSE TRUE FALSE gender == &quot;female&quot; | study.group == &quot;treatment&quot; ### [1] TRUE FALSE TRUE TRUE 4.3 Selector Vectors Note that we use operators to create logical vectors where TRUE designates observation that belong to the defined group, and FALSE designates observations outside the group. The term “selector vector” is a useful way to remember this purpose. After you have defined a group by composing a logical statement, then the vector can be used to count group members and take subsets of other variables to calculate group statistics. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 these.female &lt;- dat$gender == &quot;female&quot; sum( these.female ) # number of women in the study ### [1] 2 mean( these.female ) # proportion of the study that is women ### [1] 0.5 dat[ these.female , ] # all data belonging to women ### name gender group strength ### 2 wanda female treat 43 ### 4 nancy female control 58 mean( dat$strength[ these.female ] ) # average outcome for women in the study ### [1] 50.5 I will consistently name my logical vectors “these.GROUP” throughout the chapters, where GROUP represents the group label. For example, I selected women above, so the selector vector is called “these.female”. 4.4 Usefulness of Selector Vectors Selector vectors, i.e. logical vectors that were created by defining a group, have three main uses in our analysis. ONE: Logical vectors give us an easy way to count things within defined groups. We can apply a sum() function to a logical vector, and the result will be a tally of all of the TRUE cases. The mean() function will give us the proportion of the sample that belongs to our defined group. # how many females do we have in our study? sum( gender == &quot;female&quot; ) ### [1] 2 # how many females do we have in our treatment group? sum( gender == &quot;female&quot; &amp; study.group == &quot;treatment&quot; ) ### [1] 0 # what proportion of our study are men? mean( gender == &quot;male&quot; ) ### [1] 0.5 TWO: We can create a selector variable that is used for subsets. A selector vector used in a subset operator will drop all observations that are FALSE, isolating data belonging to the group: these.female &lt;- gender == &quot;female&quot; name[ these.female ] ### [1] &quot;wanda&quot; &quot;nancy&quot; strength[ these.female ] ### [1] 43 58 Or we can create a subset of the full dataset: dat[ these.female , ] name gender group strength 2 wanda female treat 43 4 nancy female control 58 THREE: We use selector variables to replace observations with new values using the assignment operator. This is similar to a find and replace operation. animals &lt;- c( &quot;mole&quot;, &quot;mouse&quot;, &quot;shrew&quot;, &quot;mouse&quot;, &quot;rat&quot;, &quot;shrew&quot; ) # the lab assistant incorrectly identified the shrews animals ### [1] &quot;mole&quot; &quot;mouse&quot; &quot;shrew&quot; &quot;mouse&quot; &quot;rat&quot; &quot;shrew&quot; animals[ animals == &quot;shrew&quot; ] &lt;- &quot;possum&quot; animals ### [1] &quot;mole&quot; &quot;mouse&quot; &quot;possum&quot; &quot;mouse&quot; &quot;rat&quot; &quot;possum&quot; We don’t know if linda received the treatment: name study.group adam treatment jamal control linda treatment sriti control study.group[ name == &quot;linda&quot; ] &lt;- NA study.group ### [1] &quot;treatment&quot; &quot;control&quot; NA &quot;control&quot; The ! operator is a special case, where it is not used to define a new logical vector, but rather it swaps the values of an existing logical vector. x1 &lt;- c(7,9,1,2) these &lt;- x1 &gt; 5 these ### [1] TRUE TRUE FALSE FALSE ! these ### [1] FALSE FALSE TRUE TRUE ! TRUE ### [1] FALSE ! FALSE ### [1] TRUE 4.5 Compound Logical Statements We can combine multiple logical statements using the AND, OR, and NOT operators ( &amp;, |, ! ). This functionality gives us an incredible ability to specify very granular groups within our analysis. This will be important as we begin to construct analysis in a way that we search for apples to apples comparisons within our data in order to make inferences about program effectiveness. These statements require some precision, however. Use care when applying that AND, OR, and NOT operators as to not include unintended data in your sample. In the example above, the statement “NOT bearded men” does not mean men without beards. It means all people outside of the category of men without beards (the “complement”), which includes women with or without beards as well. ! ( gender == &quot;male&quot; &amp; beard == TRUE ) Also note that parentheses matter. Compare this statement to the statement above: ! gender == &quot;male&quot; &amp; beard == TRUE Because we excluded the parentheses this statement now defines the group “NOT men AND with beards”, or bearded women. Figure 4.3: Examples of group construction with compound statements 4.6 NAs in Logical Statements Recall that missing values are an extremely important concept in statistics. If one-third of our survey sample reports that they never smoked pot, one-third reports they have smoked pot, and one-third did not answer the question, then what do we report for the proportion of the population that has smoked pot? We might prefer to be cautious and count only the people that have confirmed they have smoked pot, resulting in an estimate of 33.3%. If we throw out the missing data, then 50% of respondents have smoked pot. If we assume those that refuse to answer have likely smoked pot, our estimate might be 66.6% of the sample. These different results are a function of how we treat the missing data in our survey, so it is important that we can keep track of missing values, especially during subset operations. Note how NAs effect compound logical statements: TRUE &amp; TRUE ### [1] TRUE TRUE &amp; FALSE ### [1] FALSE TRUE &amp; NA ### [1] NA FALSE &amp; NA ### [1] FALSE To make sense of these rules consider the following: If one condition is already FALSE, the missing value does not matter because under the &amp; condition BOTH must be TRUE for the observation to belong to our defined group. After we know that one of the conditions is FALSE the missing value is irrelevant. For example, if we want to select all women in the treatment group, and we have a man with an unclear treatment group status, he is still excluded from the group because he is a man. On the other hand, if one condition is TRUE, and another is NA, R does not want to throw out the data because the state of the missing value is unclear. As a result, it will preserve the observation, but it will replace all of the data with missing values to signal the lack of certainty associated with that observation. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 keep.these &lt;- c(T,F,NA,F) dat[ keep.these , ] ### name gender group strength ### 1 frank male treat 27 ### NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA To remove these rows with missing values in your selector vector, replace all NAs with FALSE: keep.these[ is.na(keep.these) ] &lt;- FALSE dat[ keep.these , ] ### name gender group strength ### 1 frank male treat 27 4.7 Subsets The subset operators [ ] are one of the most common you will use in R. The primary rule of subsets is to use a data operator to create a logical selector vector, and use that to generate subsets. Any observation that corresponds to TRUE will be retained, any observation that corresponds to FALSE will be dropped. For vectors, you need to specify a single dimension. name gender group strength frank male treat 27 wanda female treat 43 sanjay male control 19 nancy female control 58 these.treat &lt;- dat$group == &quot;treat&quot; name[ these.treat ] ### [1] &quot;frank&quot; &quot;wanda&quot; strength[ these.treat ] ### [1] 27 43 For data frames, you need two dimensions (rows and columns). The two dimensions are seperated by a comma, and if you leave one blank you will not drop anything. dat[ row position , column position ] these.control &lt;- dat$group == &quot;control&quot; dat[ these.control , ] # all data in the control group ### name gender group strength ### 3 sanjay male control 19 ### 4 nancy female control 58 dat[ , c(&quot;name&quot;,&quot;gender&quot;) ] # select two columns of data ### name gender ### 1 frank male ### 2 wanda female ### 3 sanjay male ### 4 nancy female # to keep a subset as a separate dataset dat.women &lt;- dat[ dat$gender == &quot;female&quot; , ] dat.women ### name gender group strength ### 2 wanda female treat 43 ### 4 nancy female control 58 Note the rules listed above about subsetting factors. After applying a subset, they will retain all of the original levels, even when they are not longer useful. You need to drop the unused levels if you would like them to be omitted from functions that use the factor levels for analysis. df &lt;- data.frame( letters=LETTERS[1:5], numbers=seq(1:5) ) levels( df$letters ) ### [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; sub.df &lt;- df[ 1:3, ] sub.df$letters ### [1] A B C ### Levels: A B C D E levels( sub.df$letters ) ### [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; droplevels( sub.df$letters ) ### [1] A B C ### Levels: A B C sub.df$letters &lt;- droplevels( sub.df$letters ) "],
["data-verbs.html", "DATA VERBS", " DATA VERBS In this section we are moving from a focus on vector operations to a focus on dataset operations. Some of the most popular packages for building and manipulating datasets come from what is affectionately referred to as the “tidyverse”, a set of packages developed by Hadley Wickam and the R Studio team. They are designed to make basic dataset operations more intuitive and robust. These packages were created with a common framework for data design called “tidy data”. When datasets are built as or wrangled into the tidy format they are much easier to analyze, and they will play nicely with a large set of packages the follow the same philosophy of data design. The core R packages contain all of the functions necessary to wrangle datasets, but they are sometimes counter-intuitive or clumbsy in the sense that they might start with a data frame and return a table or a list, making them hard to use in data recipes. All of the “data verbs” that we will cover in this section start with a data frame and return a data frame, making them easier to use as data steps within a larger project. We will begin with some extremely useful verbs contained within the dplyr package. Figure 4.4: A data verb requires a dataset as input, and returns a transformed dataset. "],
["the-dplyr-package.html", "Chapter 5 The dplyr Package 5.1 Packages Used in This Chapter 5.2 Key Concepts 5.3 The dplyr Package 5.4 Use filter() to Subset Rows 5.5 select() Columns 5.6 arrange() Sorts Data 5.7 Variable Transforms with mutate() 5.8 rename() Variables 5.9 summarize() Variables", " Chapter 5 The dplyr Package th { text-align: left; } td { text-align: left; } 5.1 Packages Used in This Chapter library( dplyr ) library( pander ) 5.2 Key Concepts Figure 5.1: Data verbs are functions that require a dataframe as the primary argument, perform some transformation on the data, then return a new dataframe. For a nice overview of all of the dataset verbs in dplyr check out The dplyr Cheatsheet. 5.3 The dplyr Package This chapter will demonstrate a few basic dataset functions contained within the dplyr package. There are a few things to note as you get started: dplyr functions are all data verbs that accept a dataset as the argument, transform the data, and return a new dataset. The first argument is always the dataset name, and variables (columns) can be referenced directly by name without quotation marks. dplyr functions will return data as a “tibble” (tbl_df class), which is a regular data frame wrapped in a nice print method that includes metadata in the printout. For example, here is the regular data frame preview: ### weight group ### 1 4.17 ctrl ### 2 5.58 ctrl ### 3 5.18 ctrl ### 4 6.11 ctrl ### 5 4.50 ctrl ### 6 4.61 ctrl ### 7 5.17 ctrl ### 8 4.53 ctrl ### 9 5.33 ctrl ### 10 5.14 ctrl The tibble will print the first few rows and columns of a dataset, and includes dataset dimensions and vector classes: ### # A tibble: 30 x 2 ### weight group ### &lt;dbl&gt; &lt;fct&gt; ### 1 4.17 ctrl ### 2 5.58 ctrl ### 3 5.18 ctrl ### 4 6.11 ctrl ### 5 4.50 ctrl ### 6 4.61 ctrl ### # ... with 24 more rows We will cover the following dplyr functions in this section of the textbook: DATA VERB ACTION filter() Select rows select() Select columns arrage() Sort the dataset by one or more columns mutate() Create a new variable by transforming an existing variable or variables summarize() Create summary statistics for specified variables group_by() Split the dataset (implicitly) into a separate dataset for each group 5.4 Use filter() to Subset Rows In the last chapter we learned how to use operators to translate from plain English questions to data queries. As an example, a city manager might want to know the average amount owed on a delinquent property tax. tax.id amount.owed 1 $0 2 $5,549 3 $0 4 $1,709 5 $0 6 $634 7 $0 8 $0 9 $0 10 $9,353 We could write the query as follows: Define the group. Select the data that belongs to the group. Analyze the group subset. these.late &lt;- taxdat$amount.owed &gt; 0 # 1. Define group overdue.amounts &lt;- taxdat$amount.owed[ these.late ] # 2. Select data overdue.amounts ### [1] 5549 1709 634 9353 1366 mean( overdue.amounts ) # 3. Analyze data ### [1] 3722.2 The filter() function in the dplyr package is a slightly more elegant verb for selecting the group and subsetting the data by rows. filter( dataset name , logical expression ) filter( taxdat, amount.owed &gt; 0 ) ### tax.id amount.owed ### 1 2 5549 ### 2 4 1709 ### 3 6 634 ### 4 10 9353 ### 5 12 1366 Note that we do not need to reference the dat$ references inside dplyr functions. 5.5 select() Columns In the core R operators, we select colums from a dataset using the subset function: Murder Assault UrbanPop Rape Alabama 13.2 236 58 21.2 Alaska 10 263 48 44.5 Arizona 8.1 294 80 31 Arkansas 8.8 190 50 19.5 California 9 276 91 40.6 Colorado 7.9 204 78 38.7 USArrests[ , c(&quot;Murder&quot;,&quot;Assault&quot;) ] Murder Assault Alabama 13.2 236 Alaska 10 263 Arizona 8.1 294 Arkansas 8.8 190 California 9 276 Colorado 7.9 204 The select() function converts this oeration into a data verb: select( USArrests, Murder, Assault ) Murder Assault Alabama 13.2 236 Alaska 10 263 Arizona 8.1 294 Arkansas 8.8 190 California 9 276 Colorado 7.9 204 The select() function adds a lot of additional arguments that make it easy to quickly identify and keep only the necessary variables. We can demonstrate a few using the built-in iris dataset in R. Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Exclude a column with the negative sign: select( iris, -Species ) Sepal.Length Sepal.Width Petal.Length Petal.Width 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 Select by variable names: select( iris, ends_with( &quot;Length&quot; ) ) Sepal.Length Petal.Length 5.1 1.4 4.9 1.4 4.7 1.3 4.6 1.5 5 1.4 5.4 1.7 select( iris, starts_with( &quot;Petal&quot; ) ) Petal.Length Petal.Width 1.4 0.2 1.4 0.2 1.3 0.2 1.5 0.2 1.4 0.2 1.7 0.4 select( iris, matches(&quot;pal&quot;) ) Sepal.Length Sepal.Width 5.1 3.5 4.9 3 4.7 3.2 4.6 3.1 5 3.6 5.4 3.9 Or we can select by a range of variables by placing a colon between the first and last: select( iris, Sepal.Length:Petal.Width ) Petal.Length Petal.Width Species 1.4 0.2 setosa 1.4 0.2 setosa 1.3 0.2 setosa 1.5 0.2 setosa 1.4 0.2 setosa 1.7 0.4 setosa 5.6 arrange() Sorts Data The arrange() function sorts a dataset by one or more columns. By default, it sorts from smallest to largest. arrange( PlantGrowth, weight ) weight group 3.59 trt1 3.83 trt1 4.17 ctrl 4.17 trt1 4.32 trt1 If we prefer the dataset be sorted from largest to smallest, we can applyr the descending function desc() to the sort variable. arrange( PlantGrowth, desc(weight) ) weight group 6.31 trt2 6.15 trt2 6.11 ctrl 6.03 trt1 5.87 trt1 Or alternatively we can use the shortcut syntax of adding a negative sign in front of the variable: arrange( PlantGrowth, -weight ) We can also sort by multiple columns at once: arrange( PlantGrowth, group, weight ) group weight ctrl 5.17 ctrl 5.18 ctrl 5.33 ctrl 5.58 ctrl 6.11 trt1 4.69 trt1 4.81 trt1 4.89 trt1 5.87 trt1 6.03 NOTE, the equivalent core R functions would use subset[] and order() functions together. You might see examples on Stack Overflow written like this: PlantGrowth[ order(PlantGrowth$weight, decreasing=TRUE) , ] weight group 21 6.31 trt2 28 6.15 trt2 4 6.11 ctrl 17 6.03 trt1 15 5.87 trt1 29 5.8 trt2 As you can see, the dplyr versions are typically more intuitive and concise! 5.7 Variable Transforms with mutate() One of the most common operations in data analysis is to create a new variable from one or more existing variables, a “variable transformation”. Some examples include: x_squared &lt;- x * x celsius &lt;- ( fereinheit - 32 ) * ( 5/9 ) body.mass.index &lt;- kg / meters^2 per.capita.income &lt;- income / population The mutate() function creates a new transformed variable from the forumala you specify and adds it to the original dataset. As an example, perhaps we have data on the number of nonprofits located in each US city. If we look at the raw count of nonprofits, it makes it look as though the large cities have the most vibrant nonprofit sectors: city nonprofits NEW YORK 26503 LOS ANGELES 17417 WASHINGTON 15701 SAN FRANCISCO 12149 BOSTON 10536 CHICAGO 10247 PHILADELPHIA 8538 DALLAS 6008 SEATTLE 5830 ATLANTA 5438 But these numbers may be misleading. Once we account for the population size through a new nonprofit density metric (nonprofits per 1,000 residents), we can see that some smaller cities have higher densities per capita. dat.npos &lt;- mutate( dat.npos, density = nonprofits / (pop/1000) ) city density PORTLAND 2.65 MADISON 2.415 ANCHORAGE 2.156 SANTA BARBARA 1.928 LINCOLN 1.886 WASHINGTON 1.866 ASHEVILLE 1.805 TALLAHASSEE 1.785 BOSTON 1.692 ALBANY 1.685 5.8 rename() Variables More often than not you will read in a dataset that has strange or meaningless variable names: x1 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x2 &lt;- c(&quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) dat &lt;- data.frame( x1, x2 ) x1 x2 male treatment male control female treatment female control The core R functions make it a little awkward to rename these variables. names( dat ) &lt;- c(&quot;gender&quot;,&quot;study.group&quot;) gender study.group male treatment male control female treatment female control The rename() function provides a more intuitive syntax: rename( dat, gender=x1, study.group=x2 ) 5.9 summarize() Variables The next chapter will cover descriptive statistics in more depth, including some useful packages and functions for generating statistics for a variety of variable types and and reporting nice tables. Most descriptive functions, however, are not data verbs in the sense that they accept a data frame as the input and return a transformed data frame or tibble. The dplyr function summarize() is the primary function that will be used in data recipes (see the next chapter). Like other data verbs, the first argument will be the input dataset. In this case, there is no pre-determined set of descriptive statistics. The user needs to specify the desired metrics. group gender strength control male 108 treatment female 95 treatment male 86 control male 90 control female 114 treatment male 87 summarize( dat, n=n(), min=min(strength), mean=mean(strength), max=max(strength) ) ### n min mean max ### 1 100 53 96.53 145 Similarly, the native table() function is useful, but returns a table object. The dplyr count() function will function almost identically, but it will return a data frame. dplyr::count( dat, group, gender ) ### # A tibble: 4 x 3 ### group gender n ### &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ### 1 control female 30 ### 2 control male 28 ### 3 treatment female 17 ### 4 treatment male 25 The real power of this function is the ability to use it with the group_by() function to analyze outcomes for many data subsets at once. grouped.dat &lt;- group_by( dat, group, gender ) dplyr::summarize( grouped.dat, n=n(), min=min(strength), mean=round( mean(strength), 1 ), max=max(strength) ) ### # A tibble: 4 x 6 ### # Groups: group [?] ### group gender n min mean max ### &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ### 1 control female 30 79.0 110 145 ### 2 control male 28 53.0 78.3 108 ### 3 treatment female 17 94.0 114 143 ### 4 treatment male 25 60.0 88.9 115 We will discuss this functionality in-depth two chapters from now. "],
["descriptive-statistics.html", "Chapter 6 Descriptive Statistics 6.1 Useful Packages", " Chapter 6 Descriptive Statistics 6.1 Useful Packages library( stargazer ) # publication quality tables library( skimr ) # quick and comprehensive descriptive stats Descriptive statistics are hugely important for any analysis, but they can be challenging to produce because different classes of variable require different tables or statistics to be meaningful. The most general core R summary() function prints some basic descriptives about variables in a dataset, reporting statistics based upon data type: name group gender height weight strength adam treatment male 73 180 167 jamal control male 67 190 185 linda treatment female 62 130 119 sriti control female 65 140 142 summary( dat ) ### name group gender height ### Length:4 control :2 female:2 Min. :62.00 ### Class :character treatment:2 male :2 1st Qu.:64.25 ### Mode :character Median :66.00 ### Mean :66.75 ### 3rd Qu.:68.50 ### Max. :73.00 ### weight strength ### Min. :130.0 Min. :119.0 ### 1st Qu.:137.5 1st Qu.:136.2 ### Median :160.0 Median :154.5 ### Mean :160.0 Mean :153.2 ### 3rd Qu.:182.5 3rd Qu.:171.5 ### Max. :190.0 Max. :185.0 These are not pretty enough to include in a report. Fortunately there are some functions that produce nice tables for R Markdown reports. We will use the stargazer package extensively for regression results and descriptive statistics. library( stargazer ) dat.numeric &lt;- select_if( dat, is.numeric ) stargazer( dat.numeric, type=&quot;html&quot;, digits=2, summary.stat = c(&quot;n&quot;,&quot;min&quot;,&quot;median&quot;,&quot;mean&quot;,&quot;max&quot;,&quot;sd&quot;) ) Statistic N Min Median Mean Max St. Dev. height 4 62 66 66.75 73 4.65 weight 4 130 160 160.00 190 29.44 strength 4 119 154.5 153.25 185 28.85 In many instances we will be working with a large dataset with many variables that are non-numeric. For example, the Lahman package contains a Master data frame with the demographic information of all Major League baseball players in the League’s 100-year history. Variables contained in the Master data frame in the Lahman package: VARIABLE CLASS DESCRIPTION playerID factor A unique code asssigned to each player. The playerID links the data in this file with records on players in the other files. birthYear numeric Year player was born birthMonth numeric Month player was born birthDay numeric Day player was born birthCountry character Country where player was born birthState character State where player was born birthCity character City where player was born deathYear numeric Year player died deathMonth numeric Month player died deathDay numeric Day player died deathCountry character Country where player died deathState character State where player died deathCity character City where player died nameFirst character Player’s first name nameLast character Player’s last name nameGiven character Player’s given name (typically first and middle) weight numeric Player’s weight in pounds height numeric Player’s height in inches bats factor a factor: Player’s batting hand (left (L), right (R), or both (B)) throws factor a factor: Player’s throwing hand (left(L) or right(R)) debut character Date that player made first major league appearance finalGame character Date that player made first major league appearance (blank if still active) retroID character ID used by retrosheet, http://www.retrosheet.org/ bbrefID character ID used by Baseball Reference website, http://www.baseball-reference.com/ birthDate date Player’s birthdate, in as.Date format deathDate date Player’s deathdate, in as.Date format In these cases, many of the summary functions will be of limited use. The skimr package was developed for large datasets like these. It will automatically create a set of summary tables for a variety of data types, and the default statistics are reasonable and informative: library( skimr ) skim( Master ) ### Skim summary statistics ### n obs: 19105 ### n variables: 26 ### ### -- Variable type:character ------------------------------------------------ ### variable missing complete n min max empty n_unique ### bbrefID 2 19103 19105 5 9 0 19103 ### birthCity 180 18925 19105 3 26 0 4745 ### birthCountry 69 19036 19105 3 14 0 53 ### birthState 571 18534 19105 2 22 0 260 ### deathCity 9674 9431 19105 2 28 0 2578 ### deathCountry 9669 9436 19105 3 14 0 25 ### deathState 9715 9390 19105 2 20 0 101 ### debut 195 18910 19105 10 10 0 10155 ### finalGame 195 18910 19105 10 10 0 9138 ### nameFirst 37 19068 19105 2 12 0 2353 ### nameGiven 37 19068 19105 2 43 0 12616 ### nameLast 0 19105 19105 2 14 0 9831 ### playerID 0 19105 19105 5 9 0 19105 ### retroID 56 19049 19105 8 8 0 19049 ### ### -- Variable type:Date ----------------------------------------------------- ### variable missing complete n min max median ### birthDate 449 18656 19105 1820-04-17 1996-08-12 1939-01-01 ### deathDate 9666 9439 19105 1872-03-17 2017-02-19 1967-03-06 ### n_unique ### 15404 ### 8458 ### ### -- Variable type:factor --------------------------------------------------- ### variable missing complete n n_unique ### bats 1185 17920 19105 3 ### throws 979 18126 19105 3 ### top_counts ordered ### R: 11788, L: 4957, NA: 1185, B: 1175 FALSE ### R: 14472, L: 3653, NA: 979, S: 1 FALSE ### ### -- Variable type:integer -------------------------------------------------- ### variable missing complete n mean sd p0 p25 p50 p75 p100 ### birthDay 449 18656 19105 15.61 8.75 1 8 16 23 31 ### birthMonth 302 18803 19105 6.63 3.47 1 4 7 10 12 ### birthYear 132 18973 19105 1931.44 41.56 1820 1895 1937 1969 1996 ### deathDay 9666 9439 19105 15.57 8.78 1 8 15 23 31 ### deathMonth 9665 9440 19105 6.48 3.53 1 3 6 10 12 ### deathYear 9664 9441 19105 1964.29 31.81 1872 1942 1967 1990 2017 ### height 785 18320 19105 72.27 2.6 43 71 72 74 83 ### weight 854 18251 19105 186.38 21.52 65 170 185 200 320 ### hist ### &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt; ### &lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2586&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2587&gt; ### &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt; ### &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt; ### &lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt; ### &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2586&gt; ### &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt; ### &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; For more functionality see: vignette( &quot;Using_skimr&quot;, package = &quot;skimr&quot; ) There are many additional packages and tricks for producing descriptive statistics. Note, though, that most produce a print-out of summary statistics but do not return a useful “tidy” dataset that can be used in subsequent steps. For most data recipes, we will rely on the summarize() function in the dplyr package. It’s utility will become obvious in the next two chapters. "],
["data-recipes.html", "Chapter 7 Data Recipes 7.1 Packages Used in This Chapter 7.2 Key Concepts 7.3 The Pipe Operator %&gt;% 7.4 Building Data Recipes 7.5 Conclusion", " Chapter 7 Data Recipes th { text-align: left; } td { text-align: left; } 7.1 Packages Used in This Chapter library( dplyr ) library( pander ) library( ggvis ) 7.2 Key Concepts Figure 7.1: Data recipes are written using a series of data steps. We can simplify this process using pipes Figure 7.2: Recall, data verbs use data frames as the primary input and the output value. Figure 7.3: The pipe operator passes a data frame forward through a chain of data vergs. We only reference the dataset name once, and all other times it’s implicitly called through through piping. 7.3 The Pipe Operator %&gt;% The idea of functions() was first introduced using a metaphor of a cookie recipe that has ingredients (data and arguments) and requires that each step of the process building on the results of the previous step. The pipe operator allows us to follow this same model to build “data recipes”, a stylized way of writing a program as a series of data verbs chained together to wrangle and analyze the data. The pipe operator passes the data from one verb to the next without having to name it directly. Figure 7.4: The pipe operator allows us to pass a transformed dataset forward in the recipe. 7.4 Building Data Recipes Data recipes are simple scripts that follow a series of steps, just like a recipe. This chapter demonstrates how data verbs and pipe operators can be used to write recipes to generate interesting insights. To demonstrate the idea, we will use a dataset of US Baby Names released by the Social Security Administration. This version was downloaded by Ryan Burge and posted on Kaggle. I’ve re-posted it on GitHub so it can be read directly into R easily: URL &lt;- &quot;https://github.com/DS4PS/Data-Science-Class/blob/master/DATA/BabyNames.rds?raw=true&quot; names &lt;- readRDS( gzcon( url( URL ))) names %&gt;% head() %&gt;% pander() Id Name Year Gender Count 1 Mary 1880 F 7065 2 Anna 1880 F 2604 3 Emma 1880 F 2003 4 Elizabeth 1880 F 1939 5 Minnie 1880 F 1746 6 Margaret 1880 F 1578 Let’s start by building a recipe to identify the top 10 male names for Baby Boomers. Create a subset of data for men born between 1946 and 1964. Sort by the annual count of each name in the subset. Keep only the most popular year for each name. Identify the top 10 most popular during this period. Print the results in a nice table that includes name and peak year data. The recipe will look something like this: names %&gt;% filter( Gender ==&quot;M&quot; &amp; Year &gt;= 1946 &amp; Year &lt;= 1964 ) %&gt;% arrange( desc( Count ) ) %&gt;% distinct( Name, .keep_all=T ) %&gt;% top_n( 10, Count ) %&gt;% select( Name, Year, Count ) %&gt;% pander() Name Year Count James 1947 94755 Michael 1957 92709 Robert 1947 91642 John 1947 88318 David 1955 86191 William 1947 66969 Richard 1946 58859 Mark 1960 58735 Thomas 1952 48617 Charles 1947 40773 There are many ways to construct a data recipe. We could have alternatively taken this approach: Create a subset of data for men born between 1946 and 1964. Count the total numer of men given each name during the period. Find the top 10 most popular names. names %&gt;% filter( Gender ==&quot;M&quot; &amp; Year &gt;= 1946 &amp; Year &lt;= 1964 ) %&gt;% group_by( Name ) %&gt;% dplyr::summarize( total=sum(Count) ) %&gt;% dplyr::arrange( desc(total) ) %&gt;% slice( 1:10 ) %&gt;% pander() Name total James 1570607 Robert 1530527 John 1524619 Michael 1463911 David 1395499 William 1072303 Richard 959321 Thomas 810160 Mark 684159 Charles 657780 We can see that these two approaches to answering our question give us slightly different results, but are pretty close. Let’s try to identify when specific female names have peaked. Create a subset of data for women. Group the data by “Name” so we can analyze each name separately. Find the year with the highest count for each name. Store this data as “peak.years”. Each name will occur once in this dataset in the year that it experienced it’s peak popularity. peak.years &lt;- names %&gt;% filter( Gender == &quot;F&quot; ) %&gt;% group_by( Name ) %&gt;% top_n( 1, Count ) %&gt;% ungroup() peak.years %&gt;% head( 5 ) %&gt;% pander() Id Name Year Gender Count 568 Manerva 1880 F 10 720 Neppie 1880 F 7 2621 Zilpah 1881 F 9 4625 Crete 1882 F 8 4750 Alwina 1882 F 6 We can then filter by years to see which names peaked in a given period. filter( peak.years, Year == 1950 ) %&gt;% arrange( desc( Count ) ) %&gt;% slice( 1:5 ) %&gt;% pander() Id Name Year Gender Count 462006 Constance 1950 F 4442 462008 Glenda 1950 F 4213 462103 Bonita 1950 F 1527 462301 Ilene 1950 F 453 462305 Marta 1950 F 445 # library( ggvis ) names %&gt;% filter( Name == &quot;Constance&quot; &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.1920 &lt;- filter( peak.years, Year == 1920 ) %&gt;% top_n( 5, Count ) top.five.1920 ### # A tibble: 5 x 5 ### Id Name Year Gender Count ### &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ### 1 169464 Ruth 1920 F 26100 ### 2 169465 Mildred 1920 F 18058 ### 3 169472 Marie 1920 F 12745 ### 4 169477 Lillian 1920 F 10050 ### 5 169481 Gladys 1920 F 8819 names %&gt;% filter( Name %in% top.five.1920$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.1975 &lt;- filter( peak.years, Year == 1975 ) %&gt;% top_n( 5, Count ) names %&gt;% filter( Name %in% top.five.1975$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download top.five.2000 &lt;- filter( peak.years, Year == 2000 ) %&gt;% top_n( 5, Count ) names %&gt;% filter( Name %in% top.five.2000$Name &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download Ryan Burge posted a fun project on Kaggle about how to find hipster names using this historical data. He defines hipster names as those meeting the following criteria: They were popular when your grandmother was young. They were unpopular when your parent were young. They have recently become popular again. Let’s stick with women’s names. df1 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year &gt;= 1915 &amp; Year &lt;= 1935 &amp; Count &gt; 3000 ) df2 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year == 1980 &amp; Count &lt;= 1000 ) df3 &lt;- filter( names, Gender == &quot;F&quot; &amp; Year &gt;= 2010 &amp; Count &gt; 2000) hipster.names &lt;- names %&gt;% filter( Name %in% df1$Name &amp; Name %in% df2$Name &amp; Name %in% df3$Name ) %&gt;% group_by( Name ) %&gt;% dplyr::summarize( total=sum(Count), peak=max(Count) ) %&gt;% arrange( desc( peak ) ) Here are the top 6 female hipster names: top.hipster.names &lt;- c(&quot;Emma&quot;,&quot;Evelyn&quot;,&quot;Alice&quot;,&quot;Grace&quot;,&quot;Lillian&quot;,&quot;Charlotte&quot;) names %&gt;% filter( Name %in% top.hipster.names &amp; Gender ==&quot;F&quot; ) %&gt;% select (Name, Year, Count) %&gt;% ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;% layer_lines() Renderer: SVG | Canvas Download And the full list: hipster.names %&gt;% pander() Name total peak Emma 595546 22701 Evelyn 534502 14279 Grace 469034 12770 Alice 551034 11956 Lillian 421392 10050 Charlotte 312022 10048 Ella 273663 9868 Josephine 297064 8683 Eleanor 268153 8499 Ruby 340143 8407 Hazel 244069 7615 Clara 268980 5779 Eva 252465 4564 Lucy 185064 4257 Stella 155080 4165 Violet 122984 4156 Vivian 198012 4128 7.5 Conclusion The pipe operator is a little confusing when you first encounter it, but you will find that using data verbs contained in the dplyr package and the pipe operator will speed up your analysis and make your code more readable. In the next chapter we focus more on the use of groups in data science, and the applications of the group_by() function to make your job easier. "],
["analysis-with-groups.html", "Chapter 8 Analysis with Groups 8.1 Packages Used in this Chapter 8.2 Hypothetical Experimental Data 8.3 Group Structure 8.4 Analysis by Group", " Chapter 8 Analysis with Groups 8.1 Packages Used in this Chapter library( pander ) library( dplyr ) library( tidyr ) library( reshape2 ) library( scales ) library( ggplot2 ) library( Lahman ) 8.2 Hypothetical Experimental Data We will demonstrate some functions using this hypothetical dataset: head( d ) %&gt;% pander id race blood.type gender age study.group speed 1 white B male 25 treatment 367 2 white B male 28 treatment 570.1 3 black B female 23 treatment 420.9 4 white A female 39 treatment 664.6 5 black B female 34 treatment 600.8 6 white B male 49 treatment 557.3 8.3 Group Structure The two most important skills as you first learn a data programming language are: Translating English phrases into computer code using logical statements Organizing your data into groups This lecture focuses on efficienty splitting your data into groups, and then analyzing your data by group. 8.3.1 What Are Groups? A group represents a set of elements with identical characteristics - mice all belong to one group and elephants belong to another. Easy enough, right? In data analysis, it is a little more complicated because a group is defined by a set of features. Each group still represents a set of elements with identical characteristics, but when we have multiple features there is a unique group for each combination of features. The simple way to think about this is that the cross-tab of features generates a grid (table), and each cell represents a unique group: We might be interested in simple groups (treatment cases versus control cases), or complex groups (does the treatment effect women and men differently?). In previous lectures you have learned to identify a group with a logical statement, and analyze that group discretely. mean( speed[ study.group == &quot;treatment&quot; &amp; gender==&quot;female&quot; ] ) ### [1] 613.3943 In this lecture you will learn to define a group structure, then analyze all of your data using that structure. tapply( speed, INDEX = list( study.group, gender ), FUN = mean ) female male control 469 357 treatment 613.4 511.6 8.3.2 Main Take-Away R has been designed to do efficient data analysis by defining a group structure, then quickly applying a function to all unique members. The base R packages do this with a set of functions in the apply() family. The tapply() function allows you to specify an outcome to analyze and a group, then ask for results from a function. tapply( X=speed, INDEX=list( study.group, gender ), FUN=mean ) female male control 469 357 treatment 613.4 511.6 The dplyr package makes this process easier using some simple verbs and the “pipe” operator. dat %&gt;% group_by( study.group, gender ) %&gt;% summarize( ave.speed = mean(speed) ) study.group gender ave.speed control male 357 control female 469 treatment male 511.6 treatment female 613.4 8.3.3 Example Let’s think about a study looking at reading speed. The treatment is a workshop that teaches some speed-reading techniques. In this study we have data on: gender (male,female) race (black,white,asian) blood.type (A,B) age (from 18 to 93) Examining descriptive statistics we can see that reading speed varies by gender and the treatment group, but not by race or blood type: The question is, how many unique groups can we create with these four factors? Each individual factor contains a small number of levels (only 2 or 3 in this case), which makes the group structure look deceptively simple at first glance. When we start to examine combinations of factors we see that group structure can get complicated pretty quickly. If we look at gender alone, we have two levels: male and female. So we have two groups. If we look at our study groups alone we have two groups: treatment and control. If we look at gender and the study groups together, we now have a 2 x 2 grid, or four unique groups. If the race factor has three levels, how many unique groups will we have considering the study design, gender, and race together? We can calculate the size of the grid by multiplying number of levels for each factor. We see here we have 12 unique groups: nlevels( gender ) * nlevels( study.group ) * nlevels( race ) ### [1] 12 If we add blood type, a factor with two levels (A and B), we now have 24 unique groups: p + facet_grid( race + study.group ~ gender + blood.type) What about age? It is a continuous variable, so it’s a little more tricky. We can certainly analyze the relationship between age and speed using correlation tools. plot( age, speed, bty=&quot;n&quot;, main=&quot;Age&quot; ) But we can also incorporate this independent variable into a group structure. We can treat each distinct age as a separate group. The ages in this study range from 18 to 93, so we have 65 distinct ages represented. plot( factor(age), speed, las=2, frame.plot=F, outline=F, main=&quot;Age&quot;, xaxt=&quot;n&quot; ) If we think about the overall group structure, then, we have unique groups defined by gender, race, blood type, and study design, and another 65 age groups. So in total we now have 24 x 65 = 1,560 groups! That is getting complicated. This group design is problematic for two reasons. From a pragmatic standpoint, we can’t report results from 1,500 groups in a table. From a more substantive perspective, we although we have 1,500 distinct cells in our grid, many may not include observations that represent the unique combination of all factors. So this group design is not very practical. A similar problem arises if our data includes time. If our data includes the time of events recorded by hours, days of the week, months, and years, we can have generate complicated group structures if we try to analyze every unique combination. We can simplify our analysis by thinking about age ranges instead of ages, or in other words by binning our continuous data. If we split it into five-year ranges, for example, we have gone from 65 distinct ages to 12 distinct age groups. age.group &lt;- cut( age, breaks=seq(from=20,to=80,by=5), labels=paste( seq(from=20,to=75,by=5), &quot;to&quot;, seq(from=25,to=80,by=5) ) ) group.structure &lt;- formula( speed ~ age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group&quot; ) We have now simplified our analysis from 1,560 to 288 possible groups. Combinations of groups will also be easier: group.structure &lt;- formula( speed ~ gender * age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group by Gender&quot;, col=c(&quot;firebrick&quot;,&quot;steelblue&quot;), xaxt=&quot;n&quot; ) 8.4 Analysis by Group Let’s demonstrate some analysis of groups using the Lahman package and some dplyr verbs. Let’s do some analysis of player salaries (Salaries dataset), and start with a simple group structure - teams in the National League and time. Which team has the highest average player salary? Which team has the most players paid over $5 million a season? Which team has raised it’s pay the most over the past decade? Let’s start by thinking about group structure. We have teams, and we have seasons. Teams is stored as a factor, and seasons as a numeric value, so we can consider group for each by counting levels and unique values: nlevels( Salaries$teamID ) ### [1] 46 length( unique( Salaries$yearID ) ) ### [1] 32 So we can potentially calculate 32 x 46 = 1,472 average player salaries. 8.4.1 Highest Ave Player Salary For our first question, we will select only teams from the National League. Let’s use the most recent year of data to calculate average pay. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID) %&gt;% summarize( Ave_Salary = mean(salary) ) ### # A tibble: 15 x 2 ### teamID Ave_Salary ### &lt;fct&gt; &lt;dbl&gt; ### 1 ARI 3363041 ### 2 ATL 2362010 ### 3 CHC 5312678 ### 4 CIN 3066899 ### 5 COL 3413487 ### 6 LAD 6322525 ### # ... with 9 more rows Since the salaries are large, they are a little hard to read. Let’s clean up the table a bit. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID ) %&gt;% summarize( Ave_Salary=dollar( mean(salary,na.rm=T) ) ) %&gt;% arrange( desc(Ave_Salary) ) %&gt;% pander() teamID Ave_Salary SFG $6,890,151 LAD $6,322,525 WSN $5,448,179 CHC $5,312,678 NYM $4,958,857 STL $4,614,629 SDP $3,756,475 PIT $3,706,387 COL $3,413,487 ARI $3,363,041 CIN $3,066,899 MIA $2,761,222 ATL $2,362,010 MIL $2,292,508 PHI $2,033,793 8.4.2 Most Players Paid Over $5 Million This question requires you to utilize a logical statement in order to translate from the question to code. We need to inspect each salary, determine whether it is over the $5m threshold, then count all of the cases. The operation will look something like this: sum( Salaries$salary &gt; 5000000 ) ### [1] 3175 It gets a little trickier when we want to do the operation simultaneously across groups. Our team group structure is already defined, so let’s define our logical vector and count cases that match: dat.NL &lt;- filter( Salaries, yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() gt.5m &lt;- dat.NL$salary &gt; 5000000 table( dat.NL$teamID, gt.5m ) ### gt.5m ### FALSE TRUE ### ARI 23 3 ### ATL 21 6 ### CHN 19 8 ### CIN 21 5 ### COL 23 6 ### FLO 23 4 ### HOU 24 4 ### LAN 20 7 ### MIL 25 4 ### NYN 19 9 ### PHI 18 10 ### PIT 27 0 ### SDN 25 1 ### SFN 21 7 ### SLN 19 6 ### WAS 26 4 This solution works, but the table provides too much information. We can use dply to simplify and format the table nicely for our report: Salaries %&gt;% filter( yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID ) %&gt;% summarise( gt_five_million = sum( salary &gt; 5000000 ) ) %&gt;% arrange( desc(gt_five_million) ) %&gt;% pander teamID gt_five_million PHI 10 NYN 9 CHN 8 LAN 7 SFN 7 ATL 6 COL 6 SLN 6 CIN 5 FLO 4 HOU 4 MIL 4 WAS 4 ARI 3 SDN 1 PIT 0 8.4.3 Fielding Positions Which fielding position is the highest paid? merge( Salaries, Fielding ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( POS ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander POS Mean_Salary 1B $5,570,032 2B $3,162,075 3B $3,579,088 C $2,521,903 OF $3,546,115 P $3,401,676 SS $2,510,833 8.4.4 Country of Birth Which country has produced the highest paid baseball players? merge( Salaries, Master ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( birthCountry ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander birthCountry Mean_Salary Aruba $650,000 Australia $523,400 Brazil $1,548,792 CAN $7,854,167 Colombia $3,125,289 Cuba $5,532,484 Curacao $5,724,167 D.R. $5,102,318 Germany $511,500 Japan $8,247,012 Mexico $4,617,038 Netherlands $2,425,000 Nicaragua $2,375,000 P.R. $3,241,378 Panama $2,946,550 Saudi Arabia $522,500 South Korea $5,326,190 Taiwan $6,750,000 USA $4,189,640 V.I. $507,500 Venezuela $4,521,051 8.4.5 Pay Raises To examine pay raises, we will now use more than one year of data. Since the question asks about pay raises over the past decade, we will filter the last ten years of data. And how since we are looking at patterns over teams and over time, we need to define a group structure with two variables: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean= dollar(mean(salary)) ) %&gt;% head( 20 ) %&gt;% pander teamID yearID mean ARI 2007 $1,859,555 ARI 2008 $2,364,383 ARI 2009 $2,812,141 ARI 2010 $2,335,314 ARI 2011 $1,986,660 ARI 2012 $2,733,512 ARI 2013 $3,004,400 ARI 2014 $3,763,904 ARI 2015 $2,034,250 ARI 2016 $3,363,041 ATL 2007 $3,117,530 ATL 2008 $3,412,189 ATL 2009 $3,335,385 ATL 2010 $3,126,802 ATL 2011 $3,346,257 ATL 2012 $2,856,205 ATL 2013 $3,254,501 ATL 2014 $4,067,042 ATL 2015 $2,990,885 ATL 2016 $2,362,010 This might seem like an odd format. We might expect something that looks more like our grid structure: dat.NL &lt;- filter( Salaries, yearID &gt; 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() tapply( dat.NL$salary, INDEX=list(dat.NL$teamID, dat.NL$yearID), FUN=mean, na.rm=T ) %&gt;% pander() 2011 2012 2013 2014 2015 2016 ARI 1986660 2733512 3004400 3763904 2034250 3363041 ATL 3346257 2856205 3254501 4067042 2990885 2362010 CHC NA NA NA NA NA 5312678 CHN 5001893 3392194 3867989 2426759 4138547 NA CIN 2531571 2935843 4256178 3864911 4187862 3066899 CLE NA NA NA 4500000 NA NA COL 3390310 2692054 2976363 3180117 3827544 3413487 FLO 2190154 NA NA NA NA NA HOU 2437724 2332731 NA NA NA NA LAD NA NA NA NA NA 6322525 LAN 3472967 3171453 6980069 6781706 7441103 NA MIA NA 4373259 1400079 1549515 2835688 2761222 MIL 2849911 3755921 3077881 3748778 3477586 2292508 NYM NA NA NA NA NA 4958857 NYN 4401752 3457555 1648278 3168777 3870667 NA PHI 5765879 5817965 6533200 5654530 4295885 2033793 PIT 1553345 2248286 2752214 2756357 3065259 3706387 SDN 1479650 1973025 2342339 2703061 4555435 NA SDP NA NA NA NA NA 3756475 SFG NA NA NA NA NA 6890151 SFN 4377716 3920689 5006441 5839649 6100056 NA SLN 3904947 3939317 3295004 4310464 4586212 NA STL NA NA NA NA NA 4614629 WAS 2201963 2695171 4548131 4399456 5365085 NA WSN NA NA NA NA NA 5448179 Later on we will look at the benefits of “tidy data”, but the basic idea is that you can “facet” your analysis easily when your groups are represented as factors instead of arranged as a table. For example, here is a time series graph that is faceted by teams: Salaries %&gt;% filter( yearID &gt; 2000 &amp; lgID == &quot;AL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( Mean_Player_Salary=mean(salary) ) -&gt; t1 qplot( data=t1, x=yearID, y=Mean_Player_Salary, geom=c(&quot;point&quot;, &quot;smooth&quot;) ) + facet_wrap( ~ teamID, ncol=5 ) Now you can quickly see that Detroit is the team that has raised salaries most aggressively. If we need to, we can easily convert a tidy dataset into something that looks like a table using the spread() function: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=yearID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% na.omit() %&gt;% pander teamID yearID_2007 yearID_2008 yearID_2009 yearID_2010 yearID_2011 ARI $1,859,555 $2,364,383 $2,812,141 $2,335,314 $1,986,660 ATL $3,117,530 $3,412,189 $3,335,385 $3,126,802 $3,346,257 CHN $3,691,494 $4,383,179 $5,392,360 $5,429,963 $5,001,893 CIN $2,210,483 $2,647,061 $3,198,196 $2,760,059 $2,531,571 COL $2,078,500 $2,640,596 $2,785,222 $2,904,379 $3,390,310 FLO $984,097 $660,955 $1,315,500 $2,112,212 $2,190,154 HOU $3,250,333 $3,293,719 $3,814,682 $3,298,411 $2,437,724 LAN $3,739,811 $4,089,260 $4,016,584 $3,531,778 $3,472,967 MIL $2,629,130 $2,790,948 $3,083,942 $2,796,837 $2,849,911 NYN $3,841,055 $4,593,113 $5,334,785 $4,800,819 $4,401,752 PHI $2,980,940 $3,495,710 $4,185,335 $5,068,871 $5,765,879 PIT $1,427,327 $1,872,684 $1,872,808 $1,294,185 $1,553,345 SDN $2,235,022 $2,376,697 $1,604,952 $1,453,819 $1,479,650 SFN $3,469,964 $2,641,190 $2,965,230 $3,522,905 $4,377,716 SLN $3,224,529 $3,018,923 $3,278,830 $3,741,630 $3,904,947 WAS $1,319,554 $1,895,207 $2,140,286 $2,046,667 $2,201,963 Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=teamID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% pander yearID teamID_ARI teamID_ATL teamID_CHC teamID_CHN teamID_CIN 2007 $1,859,555 $3,117,530 NA $3,691,494 $2,210,483 2008 $2,364,383 $3,412,189 NA $4,383,179 $2,647,061 2009 $2,812,141 $3,335,385 NA $5,392,360 $3,198,196 2010 $2,335,314 $3,126,802 NA $5,429,963 $2,760,059 2011 $1,986,660 $3,346,257 NA $5,001,893 $2,531,571 2012 $2,733,512 $2,856,205 NA $3,392,194 $2,935,843 2013 $3,004,400 $3,254,501 NA $3,867,989 $4,256,178 2014 $3,763,904 $4,067,042 NA $2,426,759 $3,864,911 2015 $2,034,250 $2,990,885 NA $4,138,547 $4,187,862 2016 $3,363,041 $2,362,010 $5,312,678 NA $3,066,899 "],
["data-viz.html", "DATA VIZ", " DATA VIZ Visualization can be a powerful way to generate insights from data. Creating impactful graphics is not a trivial undertaking, however. There is a science to how the brain consumes visual information. And there is an art to combining elements of graphics in ways that make the data both aesthetically pleasing and informative. It takes practice to develop these skills. This section of the textbook is not about the design of a specific visualization or graphic, but rather the implementation. Once you have an idea for your graphic in your mind, you need a few basic R functions to create the visualization. We will cover these nuts and bolts of building custom graphics, some popular R packages for visualization, and some tricks to take your data viz game to the next level using dynamic graphics and animations. Figure 8.1: The core graphics package allows you to control every element of a visualization. If you would like to start building your data viz muscles, we recommend the following resources as good jumping-off points. Popular blogs that demonstrate the step-by-step process of making a mediocre graphic into a compelling graphic: Makeover Mondays Flowing Data Junk Charts NYT Graphics Blog Help Me Viz Useful introductory textbooks on data visualization: Schwabish, J. A. (2014). An economist’s guide to visualizing data. Journal of Economic Perspectives, 28(1), 209-34. T. Chiasson, D. Gregory, &amp; Contributors (2013). Data + Design: A simple introduction to preparing and visualizing information. Tableau: Which Chart Goes with What Data Inspiration and help with R graphics: R Graph Gallery R Graph Catalog R Graph Compendium ggplot2 Geoms Gallery "],
["the-plot-function.html", "Chapter 9 The plot() Function 9.1 Key Concepts 9.2 plot() Arguments 9.3 Titles 9.4 Type of Plot 9.5 Shape of Points 9.6 Size of Points 9.7 Colors 9.8 Looking Ahead", " Chapter 9 The plot() Function Figure 9.1: The core graphics package allows you to control every element of a visualization. 9.1 Key Concepts Mastering a few arguments in the plot() function allows for a lot of customization. plot( x=fertilizer, # data, if x is omitted then uses 1:length(y) y=corn.height, xlim=c(0,100), # min and max value of axes ylim=c(200,350), frame.plot=FALSE, # draw a box around the data? col=&quot;steel blue&quot;, # color of the points type=&quot;b&quot;, # points=&quot;p&quot;, lines=&quot;l&quot;, both=&quot;b&quot;, or none=&quot;n&quot; pch=19, # shape of points to plot cex=2, # size of points main=&quot;Plot Title&quot;, # title of your plot xlab=&quot;Label for X&quot;, # axes labels ylab=&quot;Label for Y&quot;, cex.lab=1.5 # aspect ratio for axes labels ) 9.2 plot() Arguments This lecture is a brief introduction to the plot() function in R, the work horse of the graphics package. We will introduce the flexibility of the fully-customizable graphics engine in R through the demonstration of some useful arguments. To demonstrate these arguments we will use a simple dataset from a hypothetical farming experiment that examines the relationship between levels of new fertilizer under development and the height of the corn. To identify the optimal dosage of fertilizer to use, the experiment applies different levels to separate fields of corn, then measures the average final corn height at each dosage. The fields are scattered across three farms, and “moisture” represents the average Volumetric Water Content of the soil in each field. You can load it as follows: source( &quot;https://raw.githubusercontent.com/DS4PS/Data-Science-Class/master/DATA/corn_stalks.R&quot; ) fertilizer corn.height moisture farm 1 252 0.89 C 2 264 0.66 B 3 255 0.37 A 4 240 0.77 A 5 238 0.36 A 6 255 0.3 B The default plot() function requires an x-variable and y-variable and will create a scatterplot, adding axes and a title: plot( x=fertilizer, y=corn.height ) Ok, so let’s improve upon this a bit. You can use the following arguments to customize the plot: 9.3 Titles We can add better labels and a title with xlab=, ylab=, and main=. plot( x=fertilizer, y=corn.height, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot; ) We can also change their size with cex.lab= to control the size of the axes labels, and cex.main= to control the size of the title. Note that all of the cex arguments are aspect ratios, meaning that the default value of 1 represents 100% and all other argument values are in relation to this default. A value of 2 means to increase the title to 200% of the size, an argument of 0.5 shrinks the title to half the original size. plot( x=fertilizer, y=corn.height, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;cex.lab=2&quot;, cex.lab=2, # double the size of the axis labels col.lab=&quot;steelblue&quot; # change color of axis labels ) 9.4 Type of Plot We can plot points, lines, or some combination of lines and points using the type= argument: “l” for lines “p” for points “b” for both points and lines “o” plots lines over points “n” for no lines or points plot( x=fertilizer, y=corn.height, type=&quot;p&quot;, main=&#39;type=&quot;p&quot;&#39;, cex.main=2, xlab=&quot;&quot;, ylab=&quot;&quot;, col.axis=&quot;gray60&quot;, frame.plot=F ) 9.5 Shape of Points The argument pch determines the shape of the plot points. The numeric values 0 to 25 represent different default shapes. We can also use any number, letter, or symbol as a plotting shape. Note that shapes 0 to 14 are hollow, 15 to 20 are solid, and 21 to 25 can also plot a background color specified by the bg= argument. plot( x=fertilizer, y=corn.height, frame.plot=FALSE, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;pch=23&quot;, cex.main=1.5, pch=23, col=&quot;red&quot;, bg=&quot;green&quot; ) 9.6 Size of Points We change the size of points using the cex= argument (pronounced “chex”). Similar to the title cex, it is an aspect ratio so cex=2 increases the size of the plotting points to 200% of the original, and cex=0.5 scales the size down to half of the original size. plot( x=fertilizer, y=corn.height, col=&quot;darkgoldenrod2&quot;, pch=19, cex=2, # scale points to 200% normal size xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, main=&quot;cex=2&quot;, cex.main=2, frame.plot=FALSE ) The cex= argument is also useful for incorporating a third numeric variable into the analysis. For example, perhaps we want to include the average moisture levels of the soil for each field. When we use a numeric vector like this with the cex= argument, instead of a single constant, the plot will adjust the size of observation based upon its measured moisture level. Since moisture values are between 0 and 1, I have scaled them by 3 to ensure the points are large enough to see. plot( x=fertilizer, y=corn.height, col=&quot;darkgoldenrod2&quot;, cex=3*moisture, pch=19, frame.plot=F, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot; ) 9.7 Colors The argument col= determines the color of plot points. To see a list of preset options check out: List of default named colors in R plot( x=fertilizer, y=corn.height, col=&quot;darkgoldenrod2&quot;, pch=19, cex=2, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot;, frame.plot=FALSE ) In the example above we specified a single color for all of our corn heights. If we want to incorporate a third categorical variable in our analysis, we can use a factor in our dataset as the value we pass to the col= argument. For example, perhaps we want to indicate which farm each field belongs to in the graph. plot( x=fertilizer, y=corn.height, pch=19, cex=2, col=farm, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot;, frame.plot=FALSE ) Note that “farms” has to be a factor in order to use it in the col= argument. In this example, the farms have labels of “A” to “C”. levels( farm ) ### [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; You might be curious how R selected the colors for the three farms. The palette() function will print the default values that R uses for categorical variables: palette() ### [1] &quot;black&quot; &quot;red&quot; &quot;green3&quot; &quot;blue&quot; &quot;cyan&quot; &quot;magenta&quot; &quot;yellow&quot; ### [8] &quot;gray&quot; You can see that the first three are the colors used in the graph above. Since there are only 8 default values, if your categorical variable has more than 8 levels it will start to recycle colors. Perhaps you don’t like the default values. You can select your own by passing color names to the palette() function as follows: palette( c(&quot;forestgreen&quot;,&quot;darkorange1&quot;,&quot;darkorchid&quot;) ) plot( x=fertilizer, y=corn.height, pch=18, cex=3, col=farm, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot;, frame.plot=FALSE ) 9.8 Looking Ahead In the next section, we will add some lines, points, and text to the plot. We can add lines to highlight trends (a regression is just the average of Y at each level of X). plot( x=fertilizer, y=corn.height, xlab=&quot;Fertilizer (mg)&quot;, ylab=&quot;Corn Height (cm)&quot;, main=&quot;Relationship Between Fertilizer Intensity and Corn Growth&quot;, pch=19, col=&quot;gray&quot;, cex=2, bty=&quot;n&quot; ) lines( lowess( fertilizer, corn.height ), col=&quot;darkgoldenrod2&quot;, lwd=4 ) In order to add narrative to your graphs, you can add points and text. The points() function operates with basically the same parameters as the plot() function. The text() function uses the same X and Y coordinates, but you also have to add an argument for the text that you want added to the plot. Let’s highlight the tallest corn stalk as an example. tallest.x &lt;- fertilizer[ which.max( corn.height ) ] tallest.y &lt;- corn.height[ which.max( corn.height ) ] points( x=tallest.x, y=tallest.y, cex=3, lwd=1.5, col=&quot;firebrick4&quot; ) text( x=tallest.x, y=tallest.y, labels=&quot;Tallest Stalk&quot;, pos=3, offset=1, col=&quot;firebrick4&quot; ) "],
["customizing-plots.html", "Chapter 10 Customizing Plots 10.1 Key Concepts 10.2 Packages Used in This Chapter 10.3 Layering Approach 10.4 Highlighting Cases 10.5 Visual Hypothesis Testing 10.6 Additional Parameters", " Chapter 10 Customizing Plots 10.1 Key Concepts We can create almost any customized visualization using a small number of functions from the Core R graphics engine: points( x=x, y=y, # plots points at the x,y positions pch=19, # the type of point to plot cex=2, # aspect ratio of point size col=&quot;red&quot;, # color of points bg=&quot;green&quot; # fill color for open symbols ) text( x=x, y=y, # draws a line by connecting points labels=some.text, # vector of labels to plot on the graph pos=3, # position: 1=below, 2=left, 3=above, 4=right cex=2, # aspect ratio of text size col=&quot;red&quot; # color of text ) lines( x=x, y=y, # draws a line by connecting points lty=&quot;l&quot;, # type of lines, same as above lwd=0.5, # line thickness ) segments( x0=x0, y0=y0, # starting points of the segments (usually a vector) x1=x1, y1=y1, # end points of the segments (usually a vector) ... # other arguments from lines() ) title( main=&quot;Plot Title&quot;, # text for the plot title xlab=&quot;x variable&quot;, # text for the x-axis label ylab=&quot;y variable&quot;, # text for the y-axis label line= -1 # move the title closer / further ) axis( side=1 # 1=below, 2=left, 3=above, 4=right at=c(10,20,30), # position of tick marks labels=c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;) # labels for tick marks ) 10.2 Packages Used in This Chapter We will use some data from the Lahman baseball data package for examples in this chapter. The Master data frame contains information about professional baseball players. We will focus on the relationship between height and weight of the players. library( Lahman ) data( Master ) nameFirst nameLast weight height bats throws birthYear David Aardsma 215 75 R R 1981 Hank Aaron 180 72 R R 1934 Tommie Aaron 190 75 R R 1939 Don Aase 190 75 R R 1954 Andy Abad 184 73 L L 1972 Fernando Abad 220 73 L L 1985 10.3 Layering Approach Graphs in R are created by layering elements on top of each other. The easiest way to understand this is to build a plot from the ground up. The following graph is comprised of six components: points plot box x-axis y-axis x-label y-label Many other elements can be added to the plot, but this is a good start. # remove players with no height or weight measures Master &lt;- select( Master, height, weight ) Master &lt;- na.omit( Master ) # find max and min values for each xmin &lt;- min(Master$height) xmax &lt;- max(Master$height) ymin &lt;- min(Master$weight) ymax &lt;- max(Master$weight) # empty plot plot.new() plot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) ) box( col=&quot;gray&quot; ) text( 64, 200, &quot;(Empty Plot Window)&quot;, cex=2 ) # add points plot.new() plot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) ) box( col=&quot;gray&quot; ) points( Master$height, Master$weight, pch=19 ) # add an x-axis and y-axis plot.new() plot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) ) box( col=&quot;gray&quot; ) points( Master$height, Master$weight, pch=19 ) axis( side=1 ) axis( side=2, las=1 ) # add axis labels plot.new() plot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) ) box( col=&quot;gray&quot; ) points( Master$height, Master$weight, pch=19 ) axis( side=1 ) axis( side=2, las=1 ) title( xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot; ) title( main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) Now let’s see if we can improve the aestetics a bit. This is pretty dense data, so let’s see if we can use some color transparency to get a sense of typical players versus outliers. Let’s adjust the xlim and ylim arguments so we don’t show the empty bottom left quadrant, and let’s use the gray() color function to add some transparency to the over-plotted data points. The first argument is a value between 0 and 1 specifying how dark you want the gray (0 being white, 1 being black, and any value in-between being a shade of gray). The second argument is another value between 0 and 1 specifying the transparency of the points, 0 being invisible and 1 being no transparency. We can also jitter them a bit (a tiny amount of random error to each data point) so they are not all plotted on top of each other since height and weight only take integer values. height.jitter &lt;- Master$height + rnorm( nrow(Master) ) weight.jitter &lt;- Master$weight + rnorm( nrow(Master) ) plot.new() plot.window( xlim=c(60,xmax), ylim=c(110,ymax) ) points( height.jitter, weight.jitter, pch=19, cex=1.2, col=gray(0.5,0.02) ) axis( side=1 ) axis( side=2 ) title( xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot;, main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) This is helpful, but maybe we want to highlight the data core some more. Let’s add another layer with only that data that falls within the 25th to 75th percentiles of height and weight. quantile( Master$height ) %&gt;% pander() 0% 25% 50% 75% 100% 43 71 72 74 83 quantile( Master$weight ) %&gt;% pander() 0% 25% 50% 75% 100% 65 170 185 200 320 height.jitter &lt;- Master$height + rnorm( nrow(Master) ) weight.jitter &lt;- Master$weight + rnorm( nrow(Master) ) height.25th &lt;- quantile( Master$height, 0.25, na.rm=T ) height.75th &lt;- quantile( Master$height, 0.75, na.rm=T ) these.height.25th.to.75th &lt;- Master$height &gt; height.25th &amp; Master$height &lt; height.75th weight.25th &lt;- quantile( Master$weight, 0.25, na.rm=T ) weight.75th &lt;- quantile( Master$weight, 0.75, na.rm=T ) these.weight.25th.to.75th &lt;- Master$weight &gt; weight.25th &amp; Master$weight &lt; weight.75th these.both.25th.to.75th &lt;- these.height.25th.to.75th &amp; these.weight.25th.to.75th plot.new() plot.window( xlim=c(60,xmax), ylim=c(110,ymax) ) axis( side=1 ) axis( side=2 ) title( xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot;, main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) points( height.jitter, weight.jitter, pch=19, cex=1.2, col=gray(0.5,0.02) ) points( height.jitter[ these.both.25th.to.75th ], weight.jitter[ these.both.25th.to.75th ], pch=19, cex=0.3, col=&quot;firebrick3&quot; ) That didn’t quite work out because a player might be “average” height (height between 25th and 75th percentile), but be heavier or lighter than normal. The criteria we applied basically draws a box and finds data within the box. Let’s rethink this a bit, and select players with “average” size using ratios between height and weight, often expressed as Body Mass Index (BMI). This selection criteria will not limit us to a square. bmi &lt;- (Master$weight * 0.45359237) / (Master$height / 39.370)^2 bmi.25th &lt;- quantile( bmi, 0.25, na.rm=T ) bmi.75th &lt;- quantile( bmi, 0.75, na.rm=T ) these.bmi.25th.to.75th &lt;- bmi &gt; bmi.25th &amp; bmi &lt; bmi.75th plot.new() plot.window( xlim=c(60,xmax), ylim=c(110,ymax) ) axis( side=1 ) axis( side=2 ) title( xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot;, main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) points( height.jitter, weight.jitter, pch=19, cex=2, col=gray(0.5,0.02) ) points( height.jitter[ these.bmi.25th.to.75th ], weight.jitter[ these.bmi.25th.to.75th ], pch=19, cex=0.1, col=&quot;firebrick3&quot; ) 10.4 Highlighting Cases Let’s look back at the original plot. Do you see anything strange? The smallest data point in the dataset seems to represent a baseball player that is 40 inches tall and weighs 80 pounds. That must be some sort of data entry error, right? data( Master) Master[ which.min(Master$height) , c(14,15,17:20,2) ] %&gt;% pander() nameFirst nameLast weight height bats throws birthYear 5839 Eddie Gaedel 65 43 R L 1925 It turns out that Eddie Gaedel, at 3 foot 7 inches, was the shortest man to ever play on a professional team. He was hired by the owner of the St. Louis Browns as a publicity stunt. He was so short that his strike zone, which goes form the knee to mid-chest, was too small for most pitchers. Crowds would cheer as he would quickly accumulate walks. Perhaps it would be useful to annotate some of the outliers on our graph. To do this, we can use the text() function. text( x=x, y=y, # draws a line by connecting points labels=some.text, # vector of labels to plot on the graph pos=3, # position: 1=below, 2=left, 3=above, 4=right cex=2, # aspect ratio of text size col=&quot;red&quot; # color of text ) Let’s start by circling some data points. We do this by plotting a slightly larger point around the originals for the tallest, shortest, and heaviest players. plot.new() plot.window( xlim=c(0,1), ylim=c(0,1) ) points( 0.5, 0.5, pch=19, col=&quot;gray&quot;, cex=3 ) points( 0.5, 0.5, pch=1, cex=5, col=&quot;firebrick&quot; ) title( main=&quot;Large Hollow Point Plotted \\nOver Solid Point&quot; ) this.shortest &lt;- which.min(Master$height) this.tallest &lt;- which.max(Master$height) this.heaviest &lt;- which.max(Master$weight) Master[ c(this.shortest,this.tallest,this.heaviest), c(14,15,17:20,2) ] %&gt;% pander() nameFirst nameLast weight height bats throws birthYear 5839 Eddie Gaedel 65 43 R L 1925 14139 Jon Rauch 290 83 R R 1978 19008 Walter Young 320 77 L R 1980 Let’s highlight the three outliers in our dataset. plot( Master$height, Master$weight, pch=19, col=&quot;gray&quot;, frame.plot=FALSE, xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot;, main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) points( Master$height[ this.shortest ], Master$weight[ this.shortest ], col=&quot;firebrick&quot;, cex=2, lwd=2 ) points( Master$height[ this.tallest ], Master$weight[ this.tallest ], col=&quot;firebrick&quot;, cex=2, lwd=2 ) points( Master$height[ this.heaviest ], Master$weight[ this.heaviest], col=&quot;firebrick&quot;, cex=2, lwd=2 ) text( Master$height[ this.shortest ], Master$weight[ this.shortest ], &quot;Some Text Here&quot;, col=&quot;firebrick&quot;, pos=3 ) We add text to the graph through the x,y coordinate for the text, the text itself, and the pos= argument is used to specify where the text should go. pos=1: below pos=2: left pos=3: top pos=4: right no pos: on the point You can see that we have a problem above. Our text above Eddie Gaedel is lopped off because it wanders outside of the plot window. We can add additional real estate by expanding plot window with the xlim= and ylim= arguments. You can add a line break to a string by including the carriage return symbol “”. data( Master) plot( Master$height, Master$weight, pch=19, col=&quot;gray&quot;, cex=0.8, xlim=c(30,100), ylim=c(50,450), frame.plot=FALSE, xlab=&quot;Height (inches)&quot;, ylab=&quot;Weight (lbs)&quot;, main=&quot;Relationship Between Height and Weight of MLB Players&quot; ) d.small &lt;- Master[ c(this.shortest,this.tallest,this.heaviest), c(14,15,17:20,2) ] points( d.small$height, d.small$weight, col=&quot;firebrick&quot;, cex=2, lwd=2 ) text( Master$height[ this.shortest ], Master$weight[ this.shortest ], &quot;Eddie Gaedel \\nHeight: 43 Inches \\nWeight: 65 Lbs&quot;, col=&quot;firebrick&quot;, cex=0.8, pos=3, offset=1 ) text( Master$height[ this.tallest ], Master$weight[ this.tallest ], &quot;Jon Rauch \\nHeight: 83 Inches \\nWeight: 290 Lbs&quot;, col=&quot;firebrick&quot;, cex=0.8, pos=4, offset=1 ) text( Master$height[ this.heaviest ], Master$weight[ this.heaviest ], &quot;Walter Young \\nHeight: 77 Inches \\nWeight: 320 Lbs&quot;, col=&quot;firebrick&quot;, cex=0.8, pos=3, offset=1 ) 10.5 Visual Hypothesis Testing As you can see, the text() function can be used to add narrative to a graphic. It can also be used in place of plotting points. Let’s think about testing the hypothesis that players have gotten larger over time. When we look at the basic relationship between height and weight we see the distribution of player sizes, but this plot has no information about the time-periods in which they played so we can’t tell if they are growing larger over time. What if we replace the plotting points with birth years? plot.new() plot.window( xlim=c(60,xmax), ylim=c(110,ymax) ) text( x=Master$height, y=Master$weight, labels=Master$birthYear ) This is too dense to be meaningful. Maybe we can try to make the text smaller? plot.new() plot.window( xlim=c(60,xmax), ylim=c(110,ymax) ) text( x=Master$height, y=Master$weight, labels=Master$birthYear, cex=0.5 ) Still not very insightful. We have over 19,000 players in the database, which appears to be too many for this graphic. Let’s thin the data out by taking a random sample of the full dataset. par( mar=c(0,0,0,0) ) m.sample &lt;- sample_n( Master, 100 ) plot.new() plot.window( xlim=c(65,78), ylim=c(130,280) ) text( x=m.sample$height, y=m.sample$weight, labels=m.sample$birthYear, cex=0.8 ) axis( side=1, line=-2 ) axis( side=2, line=-2 ) This is a big improvement. We can eyeball the data and start to pull out some trends. The people born in the 1800’s tend to be near the bottom of each pile of years, for example. Perhaps we can improve our visual hypothesis testing if we add some color coding. Let’s identify all of the individuals born since 1980 and color their birth years red to highlight the youngest cohort in the data. If people have gotten larger over time, we would expect this group to cluster near the top of the distribution. red.gray &lt;- ifelse( m.sample$birthYear &gt;= 1980, &quot;firebrick&quot;, &quot;gray&quot; ) text( x=m.sample$height, y=m.sample$weight, labels=m.sample$birthYear, col=red.gray ) And we in fact see the pattern emerge. The youngest cohort, highlighted in red, seems to cluster near the top right, which suggests they have grown both taller and heavier over time. This hypothesis can be test more rigorously in other ways, but the demonstration at least gives an idea about how we might explore the data efficiently using flexible R graphing functions. 10.6 Additional Parameters The small number of functions introduced here in these chapters on data visualization can be used to make highly-customizable graphics that help you discover important patterns in the data and share the story with others. We are only scratching the surface on the options available within plotting functions. To see the default list of parameter settings in the R plot() function, type: par() help( par ) These chapters show how core R graphics can be used to create highly-customized graphics. You can see that you have fine-tuned access to all aspects of the figures you create. You don’t have to become familiar with all of the options to develop solid visualization skills. Rather, you will likely focus on a few specific functions. A little time invested in learning the arguments for plot(), points(), lines(), text(), title(), and axis() goes a long way. "],
["custom-plot-example.html", "Chapter 11 Custom Plot Example 11.1 Basic Rank Plot 11.2 Creating a Submay Map Rank Plot 11.3 Lines 11.4 Connected Points 11.5 Separate Points and Connecting Lines 11.6 Add Labels 11.7 Add Axes and Title 11.8 Generalizing Code 11.9 Improved Aesthetics", " Chapter 11 Custom Plot Example This example demonstrates the creation of custom visualizations in R. For reference, it took approximately two hours to develop the custom graphic. This example was inspired by a blog about the popularity of computer languages on Stack Overflow. Joshua Kunst commented on the deficiencies of rank plots, and demonstrated a superior plot for representing changes in rank over time that looks like a NYC subway map. To demonstrate how a similar graphic can be made in R, this chapter uses some hypothetical school data representing five years of rankings for ten schools within a specific district. The rankings represent some performance metric like graduation rates or average test scores. SCHOOL RANK.2011 RANK.2012 RANK.2013 RANK.2014 RANK.2015 A 1 1 1 1 2 B 5 5 6 7 8 C 2 3 4 3 1 D 4 7 7 8 7 E 7 9 9 9 9 F 9 10 8 6 6 G 3 2 2 2 3 H 10 8 10 10 10 I 6 4 3 4 4 J 8 6 5 5 5 11.1 Basic Rank Plot The original design for the graphic was very simple. It showed the starting rank and end rank of the school over the five-year period to give a sense of how much each school had changed. It is a little boring and also loses a lot of information regarding how consistent performance has been over time. 11.2 Creating a Submay Map Rank Plot Let’s see how we put together a similar graphic using our school ranking data. Let’s start by looking at the variable names and the type of data we are working with: head( df, 3 ) %&gt;% pander id r1 r2 r3 r4 r5 A 10 10 10 10 9 B 6 6 5 4 3 C 9 8 7 8 10 Note that the first row of data belongs to School A, so we can reference it as df[ 1 , ]. The second row of data belongs to School B, which we can reference as df[ 2 , ]. Etc. We will open a blank plotting window, then we can add the path of each school’s rankings from 2011 to 2015 using the points() function. We can play with the plot type to get the aesthetics correct. Values for type= argument. Argument Result “p” points “l” lines “b” both “c” only connecting lines “n” nothing 11.3 Lines Let’s try one with basic lines. Note that referencing a row of the data frame will return the ranking data for a single school: df[ 1, ] ### id r1 r2 r3 r4 r5 ### 1 A 10 10 10 10 9 Now let’s plot the rankings over time: id &lt;- df$id # save ID as separate variable df &lt;- df[ , -1 ] # drop ID from the dataset so it&#39;s all numeric # save some useful dimension information ymin &lt;- min(df) # what is the smallest value in the data frame? ymax &lt;- max(df) # what is the largest value in the data frame? num.x &lt;- 5 # we have five years of data # create an empty plot with the correct dimensions for our plot plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;l&quot; ) # school A points( 1:5, df[2,], type=&quot;l&quot; ) # school B points( 1:5, df[3,], type=&quot;l&quot; ) points( 1:5, df[4,], type=&quot;l&quot; ) points( 1:5, df[5,], type=&quot;l&quot; ) points( 1:5, df[6,], type=&quot;l&quot; ) points( 1:5, df[7,], type=&quot;l&quot; ) points( 1:5, df[8,], type=&quot;l&quot; ) points( 1:5, df[9,], type=&quot;l&quot; ) points( 1:5, df[10,], type=&quot;l&quot; ) text( x=0.5, y=10, labels=&quot;A&quot; ) # rank in 2011 text( x=5.5, y=9, labels=&quot;A&quot; ) # rank in 2015 11.4 Connected Points Not bad but I think that we can improve upon the aestetics. Let’s change the lines to points plus connecting lines. plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;b&quot; ) # school A points( 1:5, df[2,], type=&quot;b&quot; ) # school B points( 1:5, df[3,], type=&quot;b&quot; ) # etc... points( 1:5, df[4,], type=&quot;b&quot; ) points( 1:5, df[5,], type=&quot;b&quot; ) points( 1:5, df[6,], type=&quot;b&quot; ) points( 1:5, df[7,], type=&quot;b&quot; ) points( 1:5, df[8,], type=&quot;b&quot; ) points( 1:5, df[9,], type=&quot;b&quot; ) points( 1:5, df[10,], type=&quot;b&quot; ) text( x=0.5, y=10, labels=&quot;A&quot; ) # rank in 2011 text( x=5.5, y=9, labels=&quot;A&quot; ) # rank in 2015 That looks better, but if we want to match the subway map aestetic we need thicker lines. Let’s try the line width parameter, lwd. plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[2,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[3,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[4,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[5,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[6,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[7,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[8,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[9,], type=&quot;b&quot;, lwd=5, cex=2 ) points( 1:5, df[10,], type=&quot;b&quot;, lwd=5, cex=2 ) text( x=0.5, y=10, labels=&quot;A&quot; ) # rank in 2011 text( x=5.5, y=9, labels=&quot;A&quot; ) # rank in 2015 11.5 Separate Points and Connecting Lines It’s not quite working because when we increase the line width, it makes our points that serve as the elbows thicker as well. Let’s try to separate these components: plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[2,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[3,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[4,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[5,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[6,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[7,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[8,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[9,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[10,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[1,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[2,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[3,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[4,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[5,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[6,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[7,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[8,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[9,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[10,], type=&quot;c&quot;, lwd=5 ) text( x=0.5, y=10, labels=&quot;A&quot; ) # rank in 2011 text( x=5.5, y=9, labels=&quot;A&quot; ) # rank in 2015 That’s starting to look like what we want! 11.6 Add Labels How about we add some more labels. plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[2,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[3,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[4,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[5,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[6,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[7,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[8,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[9,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[10,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[1,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[2,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[3,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[4,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[5,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[6,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[7,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[8,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[9,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[10,], type=&quot;c&quot;, lwd=5 ) text( 0.7, 1:10, id[order(df[,1])] ) text( 5.3, 1:10, id[order(df[,5])] ) text( 0, 1:10, paste(10:1,&quot;---&quot;) ) text( 6, 1:10, paste(&quot;---&quot;,10:1) ) 11.7 Add Axes and Title And finally: plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) points( 1:5, df[1,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[2,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[3,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[4,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[5,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[6,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[7,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[8,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[9,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[10,], type=&quot;p&quot;, cex=2 ) points( 1:5, df[1,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[2,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[3,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[4,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[5,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[6,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[7,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[8,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[9,], type=&quot;c&quot;, lwd=5 ) points( 1:5, df[10,], type=&quot;c&quot;, lwd=5 ) text( 0.7, 1:10, id[order(df[,1])] ) text( 5.3, 1:10, id[order(df[,5])] ) text( 0, 1:10, paste(10:1,&quot;---&quot;) ) text( 6, 1:10, paste(&quot;---&quot;,10:1) ) title( ylab=&quot;Rank&quot;, line=1 ) axis( side=1, tick=F, at=1:5, labels=2011:2015 ) title( main=&quot;School Rank 2011-2015&quot; ) We are in business. 11.8 Generalizing Code Because we are plotting data row by row (each row of data represents rankings for one school over time), we can simplify our code by writing the plot() command once inside a loop (for loops are covered in subsequent chapters): r1 r2 r3 r4 r5 10 10 10 10 9 6 6 5 4 3 9 8 7 8 10 plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) for( i in 1:nrow(df) ) { points( 1:num.x, df[i,], type=&quot;p&quot;, cex=1.5, col=gray(0.5,0.5), pch=21, bg=&quot;white&quot; ) points( 1:num.x, df[i,], type=&quot;c&quot;, lwd=6, col=gray(0.5,0.5) ) } 11.9 Improved Aesthetics Finally, let’s play with the colors and sizes of elements so that we can highlight specific schools for our report. plot.new() plot.window( xlim=c(0,6), ylim=c(ymin,ymax) ) # add each row to a loop instead of repeating lines of code for( i in 1:nrow(df) ) { points( 1:num.x, df[i,], type=&quot;p&quot;, cex=1.5, col=gray(0.5,0.5), pch=21, bg=&quot;white&quot; ) points( 1:num.x, df[i,], type=&quot;c&quot;, lwd=6, col=gray(0.5,0.5) ) } text( 0.7, 1:10, id[order(df[,1])], col=gray(0.5,0.5) ) text( 5.3, 1:10, id[order(df[,5])], col=gray(0.5,0.5) ) text( 0, 1:10, paste(10:1,&quot;---&quot;), col=&quot;gray60&quot;, cex=0.7 ) text( 6, 1:10, paste(&quot;---&quot;,10:1), col=&quot;gray60&quot;, cex=0.7 ) title( ylab=&quot;Rank&quot;, col.lab=&quot;gray80&quot;, cex.lab=1.5, line=1 ) axis( side=1, tick=F, at=1:5, labels=2011:2015, col.axis=&quot;gray&quot;, cex.axis=1 ) title( main=&quot;School Rank 2011-2015&quot;, col.main=&quot;gray50&quot; ) # highlight some schools points( 1:5, df[2,], type=&quot;c&quot;, lwd=6, col=&quot;firebrick4&quot; ) text( 0.7, df[2,1], &quot;B&quot;, col=&quot;firebrick4&quot; ) text( 5.3, df[2,5], &quot;B&quot;, col=&quot;firebrick4&quot; ) points( 1:5, df[6,], type=&quot;c&quot;, lwd=6, col=&quot;steelblue&quot; ) text( 0.7, df[6,1], &quot;F&quot;, col=&quot;steelblue&quot; ) text( 5.3, df[6,5], &quot;F&quot;, col=&quot;steelblue&quot; ) points( 1:5, df[3,], type=&quot;c&quot;, lwd=6, col=&quot;goldenrod&quot; ) text( 0.7, df[3,1], id[3], col=&quot;goldenrod&quot; ) text( 5.3, df[3,5], id[3], col=&quot;goldenrod&quot; ) "],
["data-wrangling.html", "DATA WRANGLING", " DATA WRANGLING "],
["merging-data.html", "Chapter 12 Merging Data 12.1 Packages Used in This Chapter 12.2 Relational Databases 12.3 Set Theory 12.4 Merging Data 12.5 Non-Unique Observations in ID Variables 12.6 The %in% function 12.7 The Match Function", " Chapter 12 Merging Data 12.1 Packages Used in This Chapter library( pander ) library( dplyr ) library( maps ) 12.2 Relational Databases Modern databases are huge - think about the amount of information stored at Amazon in the history of each transation, the database where Google logs every single search from every person around the world, or Twitter’s database of all of the tweets (millions each day). When databases become large, flat spreadsheet style formats are not useful because they create a lot of redundant information, are large to store, and are not efficient to search. Large datasets are instead stored in relational databases - sets of tables that contain unique IDs that allow them to be joined when necessary. For example, consider a simple customer database. We don’t want to store customer info with our transactions because we would be repeating their name and street address every time they make a new purchase. As a result, we store customer information and transaction information separately. Customer Database CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE 178 Alvaro Jaurez 123 Park Ave 57701 934 Janette Johnson 456 Candy Ln 57701 269 Latisha Shane 1600 Penn Ave 20500 Transactions Database CUSTOMER.ID PRODUCT PRICE 178 video 5.38 178 shovel 12 269 book 3.99 269 purse 8 934 mirror 7.64 If we want to make the information actionable then we need to combine these datasets. For example, perhaps we want to know the average purchase amount from an individual in the 57701 zip code. We cannot answer that question with either dataset since the zip code is in one dataset, and the price is in another. We need to merge the data. merge( customer.info, purchases ) ### CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE ### 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 ### 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 ### 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 ### 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 ### 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 full.dat &lt;- merge( customer.info, purchases ) full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] ### [1] 5.38 12.00 7.64 mean( full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] ) ### [1] 8.34 In reality, each purchase would have a purchase ID that is linked to shipping addresses, customer complaints, seller ratings, etc. Each seller would have their own data table with info. Each purchase would be tied to a payment type, which has its own data table. The system gets quite complex, which is why it is important to pay attention to the details of putting the data back together again. Figure 12.1: Example of a relational database schema We will cover a few details of data merges that will help you avoid common and very subtle mistakes that can lead to incorrect inferences. 12.3 Set Theory In order to merge data correctly you need to understand some very basic principles of set theory. 12.3.1 Set Theory Functions Let’s assume we have two sets: set1=[A,B], set2=[B,C]. Each element in this set represents a group of observations that occurs in the dataset. So B represents people that occur in both datasets, A represents people that occur only in the first dataset, and C represents people that only occur in the second dataset. We can then describe membership through three operations: Figure 12.2: Membership defined by two sets Operation Description union: X OR Y The universe of all elements across all both sets: [A,B,C] intersection: X &amp; Y The elements shared by both sets: [B] difference: X &amp; ! Y The elements in my first set, not in my second [A] or [C] Let’s see how this might work in practice with an example of members of a study: name group gender frank treat male wanda treat female sanjay control male nancy control female For this example let’s define set 1 as the treatment group, and set 2 as all women in the study. Note that set membership is always defined as binary (you are in the set or out), but it can include multiple criteria (the set of animals can contains cats, dogs, and mice). treated &lt;- name[ group == &quot;treat&quot; ] treated ### [1] &quot;frank&quot; &quot;wanda&quot; females &lt;- name[ gender == &quot;female&quot; ] females ### [1] &quot;wanda&quot; &quot;nancy&quot; Now we can specify group belonging using some convenient set theory functions: union(), setdiff(), and intersect(). union( treated, females ) ### [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; intersect( treated, females ) ### [1] &quot;wanda&quot; setdiff( treated, females ) ### [1] &quot;frank&quot; setdiff( females, treated ) ### [1] &quot;nancy&quot; It is very important to note that union() and intersect() are symmetric functions, meaning intersect(x,y) will give you the same result as intersect(y,x). The setdiff() function is not symmetric, however. 12.3.2 Set Theory Using Logical Operators Typically you will define your groups using logical operators, which perform the exact same funciton as set theory functions but are a little more expressive and flexible. Let’s use the same example above where x=“treatment” and y=“female”, then consider these cases: Who belongs in each group? name group gender frank treat male wanda treat female sanjay control male nancy control female # x name[ group == &quot;treat&quot; ] ### [1] &quot;frank&quot; &quot;wanda&quot; # x &amp; y name[ group == &quot;treat&quot; &amp; gender == &quot;female&quot; ] ### [1] &quot;wanda&quot; # x &amp; ! y name[ group == &quot;treat&quot; &amp; gender != &quot;female&quot; ] ### [1] &quot;frank&quot; # x | y name[ group == &quot;treat&quot; | gender == &quot;female&quot; ] ### [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; Who belongs in these groups? !x &amp; !y x &amp; ! ( x &amp; y ) ( x | y ) &amp; ! ( x &amp; y ) 12.4 Merging Data The Merge Function The merge function joins two datasets. The function requires two datasets as the arguments, and they need to share a unique ID variable. Recall the example from above: merge( customer.info, purchases ) ### CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE ### 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 ### 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 ### 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 ### 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 ### 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 The important thing to keep in mind is that the default merge operation uses the intersection of the two datasets. It will drop all elements that don’t occur in both datasets. We may want to fine-tune this as to not lose valuable data and potentially bias our analysis. As an example, no illegal immigrants will have social security numbers, so if you are merging using the SSN, you will drop this group from the data, which could impact your results. With a little help from the set theory examples above, we can think about which portions of the data we wish to drop and which portions we wish to keep. Argument Usage all=F DEFAULT - new dataset contains intersection of X and Y (B only) all=T New dataset contains union of X and Y (A, B &amp; C) all.x=T New dataset contains A and B, not C all.y=T New dataset contains B and C, not A Here is some demonstrations with examples adapted from the R help file. authors ### surname nationality deceased ### 1 Tukey US yes ### 2 Tierney US no ### 3 Ripley UK no ### 4 McNeil Australia no ### 5 Shakespeare England yes books ### name title ### 1 Tukey Exploratory Data Analysis ### 2 Venables Modern Applied Statistics ### 3 Ripley Spatial Statistics ### 4 Ripley Stochastic Simulation ### 5 McNeil Interactive Data Analysis ### 6 R Core Team An Introduction to R # adding books to the author bios dataset ( set B only ) merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;) ### surname nationality deceased title ### 1 McNeil Australia no Interactive Data Analysis ### 2 Ripley UK no Spatial Statistics ### 3 Ripley UK no Stochastic Simulation ### 4 Tukey US yes Exploratory Data Analysis # adding author bios to the books dataset ( set B only ) merge(books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 Ripley Spatial Statistics UK no ### 3 Ripley Stochastic Simulation UK no ### 4 Tukey Exploratory Data Analysis US yes # keep books without author bios, lose authors without books ( sets A and B ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.x=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; ### 3 Ripley Spatial Statistics UK no ### 4 Ripley Stochastic Simulation UK no ### 5 Tukey Exploratory Data Analysis US yes ### 6 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; # keep authors without book listed, lose books without author bios ( sets B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.y=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 Ripley Spatial Statistics UK no ### 3 Ripley Stochastic Simulation UK no ### 4 Shakespeare &lt;NA&gt; England yes ### 5 Tierney &lt;NA&gt; US no ### 6 Tukey Exploratory Data Analysis US yes # dont&#39; throw out any data ( sets A and B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all=T ) ### name title nationality deceased ### 1 McNeil Interactive Data Analysis Australia no ### 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; ### 3 Ripley Spatial Statistics UK no ### 4 Ripley Stochastic Simulation UK no ### 5 Shakespeare &lt;NA&gt; England yes ### 6 Tierney &lt;NA&gt; US no ### 7 Tukey Exploratory Data Analysis US yes ### 8 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; Also note that the order of your datasets in the argument list will impact the inclusion or exclusion of elements. merge( x, y, all=F ) EQUALS merge( y, x, all=F ) merge( x, y, all.x=T ) DOES NOT EQUAL merge( y, x, all.x=T ) 12.4.1 The by.x and by.y Arguments When you use the default merge() function without specifying the variables to merge upon, the function will check for common variable names across the two datasets. If there are multiple, it will join the shared variables to create a new unique key. This might be problematic if that was not the intent. Take the example of combining fielding and salary data in the Lahman package. If we are not explicit about the merge variable, we may get odd results. Note that they two datasets share four ID variables. library( Lahman ) data( Fielding ) data( Salaries ) intersect( names(Fielding), names(Salaries) ) ### [1] &quot;playerID&quot; &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; # merge id int &lt;- intersect( names(Fielding), names(Salaries) ) paste( int[1],int[2],int[3],int[4], sep=&quot;.&quot; ) ### [1] &quot;playerID.yearID.teamID.lgID&quot; To avoid problems, be explicit using the by.x and by.x arguments to control which variable is used for the merge. head( merge( Salaries, Fielding ) ) ### yearID teamID lgID playerID salary stint POS G GS InnOuts PO A E DP ### 1 1985 ATL NL barkele01 870000 1 P 20 18 221 2 9 1 0 ### 2 1985 ATL NL bedrost01 550000 1 P 37 37 620 13 23 4 3 ### 3 1985 ATL NL benedbr01 545000 1 C 70 67 1698 314 35 4 1 ### 4 1985 ATL NL campri01 633333 1 P 66 2 383 7 13 4 3 ### 5 1985 ATL NL ceronri01 625000 1 C 91 76 2097 384 48 6 4 ### 6 1985 ATL NL chambch01 800000 1 1B 39 27 814 299 25 1 31 ### PB WP SB CS ZR ### 1 NA NA NA NA NA ### 2 NA NA NA NA NA ### 3 1 9 65 24 1 ### 4 NA NA NA NA NA ### 5 6 20 69 29 1 ### 6 NA NA NA NA NA head( merge( Salaries, Fielding, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) ) ### playerID yearID.x teamID.x lgID.x salary yearID.y stint teamID.y ### 1 aardsda01 2010 SEA AL 2750000 2009 1 SEA ### 2 aardsda01 2010 SEA AL 2750000 2015 1 ATL ### 3 aardsda01 2010 SEA AL 2750000 2006 1 CHN ### 4 aardsda01 2010 SEA AL 2750000 2008 1 BOS ### 5 aardsda01 2010 SEA AL 2750000 2013 1 NYN ### 6 aardsda01 2010 SEA AL 2750000 2012 1 NYA ### lgID.y POS G GS InnOuts PO A E DP PB WP SB CS ZR ### 1 AL P 73 0 214 2 5 0 1 NA NA NA NA NA ### 2 NL P 33 0 92 0 1 1 0 NA NA NA NA NA ### 3 NL P 45 0 159 1 5 0 1 NA NA NA NA NA ### 4 AL P 47 0 146 3 6 0 0 NA NA NA NA NA ### 5 NL P 43 0 119 1 5 0 0 NA NA NA NA NA ### 6 AL P 1 0 3 0 0 0 0 NA NA NA NA NA 12.5 Non-Unique Observations in ID Variables In some rare instances, you will need to merge to datasets that have non-singular elements in the unique key ID variables, meaning each observation / individual appears more than one time in the data. Note that in this case, for each occurance of an observation / individual in your X dataset, you will merge once with each occurance of the same observation / individual in the Y dataset. The result will be a multiplicative expansion of the size of your dataset. For example, if John appears on four separate rows of X, and three seperate rows of Y, the new dataset will contain 12 rows of John (4 x 3 = 12). dataset X contains four separate instances of an individual [ X1, X2, X3, X4 ] dataset Y contains three separate instances of an individual [ Y1, Y2, Y3 ] After the merge we have one row for each pair: X1-Y1 X1-Y2 X1-Y3 X2-Y1 X2-Y2 X2-Y3 X3-Y1 X3-Y2 X3-Y3 X4-Y1 X4-Y2 X4-Y3 For example, perhaps a sales company has a database that keeps track of biographical data, and sales performance. Perhaps we want to see if there is peak age for sales performance. We need to merge these datasets. bio &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), age=c(43,44,45) ) performance &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), sales=c(&quot;15k&quot;,&quot;20k&quot;,&quot;17k&quot;) ) # correct merge merge( bio, performance, by.x=c(&quot;name&quot;,&quot;year&quot;), by.y=c(&quot;name&quot;,&quot;year&quot;) ) ### name year age sales ### 1 John 2000 43 15k ### 2 John 2001 44 20k ### 3 John 2002 45 17k # incorrect merge merge( bio, performance, by.x=c(&quot;name&quot;), by.y=c(&quot;name&quot;) ) ### name year.x age year.y sales ### 1 John 2000 43 2000 15k ### 2 John 2000 43 2001 20k ### 3 John 2000 43 2002 17k ### 4 John 2001 44 2000 15k ### 5 John 2001 44 2001 20k ### 6 John 2001 44 2002 17k ### 7 John 2002 45 2000 15k ### 8 John 2002 45 2001 20k ### 9 John 2002 45 2002 17k It is good practice to check the size (number of rows) of your dataset before and after a merge. If it has expanded, chances are you either used the wrong unique IDs, or your dataset contains duplicates. 12.5.1 Example of Incorrect Merge Here is a tangible example using the Lahman baseball dataset. Perhaps we want to examine the relationship between fielding position and salary. The Fielding dataset contains fielding position information, and the Salaries dataset contains salary information. We can merge these two datasets using the playerID field. If we are not thoughtful about this, however, we will end up causing problems. Let’s look at an example using Kirby Pucket. kirby.fielding &lt;- Fielding[ Fielding$playerID == &quot;puckeki01&quot; , ] head( kirby.fielding ) ### playerID yearID stint teamID lgID POS G GS InnOuts PO A E DP ### 83848 puckeki01 1984 1 MIN AL OF 128 128 3377 438 16 3 4 ### 85157 puckeki01 1985 1 MIN AL OF 161 160 4213 465 19 8 5 ### 86489 puckeki01 1986 1 MIN AL OF 160 157 4155 429 8 6 3 ### 87896 puckeki01 1987 1 MIN AL OF 147 147 3820 341 8 5 2 ### 89264 puckeki01 1988 1 MIN AL OF 158 157 4049 450 12 3 4 ### 90685 puckeki01 1989 1 MIN AL OF 157 154 3985 438 13 4 3 ### PB WP SB CS ZR ### 83848 NA NA NA NA NA ### 85157 NA NA NA NA NA ### 86489 NA NA NA NA NA ### 87896 NA NA NA NA NA ### 89264 NA NA NA NA NA ### 90685 NA NA NA NA NA nrow( kirby.fielding ) ### [1] 21 kirby.salary &lt;- Salaries[ Salaries$playerID == &quot;puckeki01&quot; , ] head( kirby.salary ) ### yearID teamID lgID playerID salary ### 280 1985 MIN AL puckeki01 130000 ### 917 1986 MIN AL puckeki01 255000 ### 1610 1987 MIN AL puckeki01 465000 ### 2244 1988 MIN AL puckeki01 1090000 ### 2922 1989 MIN AL puckeki01 2000000 ### 3717 1990 MIN AL puckeki01 2816667 nrow( kirby.salary ) ### [1] 13 kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) head( select( kirby.field.salary, yearID.x, yearID.y, POS, G, GS, salary ) ) ### yearID.x yearID.y POS G GS salary ### 1 1984 1985 OF 128 128 130000 ### 2 1984 1986 OF 128 128 255000 ### 3 1984 1987 OF 128 128 465000 ### 4 1984 1988 OF 128 128 1090000 ### 5 1984 1989 OF 128 128 2000000 ### 6 1984 1990 OF 128 128 2816667 nrow( kirby.field.salary ) ### [1] 273 21*13 ### [1] 273 What we have done here is taken each year of fielding data, and matched it to every year of salary data. We can see that we have 21 fielding observations and 13 years of salary data, so our resulting dataset is 273 observation pairs. This merge also makes it difficult to answer the question of the relationship between fielding position and salary if players change positions over time. The correct merge in this case would be a merge on a playerID-yearID pair. We can create a unique key by combining playerID and yearID using paste(): head( paste( kirby.fielding$playerID, kirby.fielding$yearID, sep=&quot;.&quot;) ) ### [1] &quot;puckeki01.1984&quot; &quot;puckeki01.1985&quot; &quot;puckeki01.1986&quot; &quot;puckeki01.1987&quot; ### [5] &quot;puckeki01.1988&quot; &quot;puckeki01.1989&quot; But there is a simple solution as the merge function also allows for multiple variables to be used for a merge() command. kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=c(&quot;playerID&quot;,&quot;yearID&quot;), by.y=c(&quot;playerID&quot;,&quot;yearID&quot;) ) nrow( kirby.field.salary ) ### [1] 20 12.6 The %in% function Since we are talking about intersections and matches, I want to briefly introduce the %in% function. It is a combination of the two. The intersect() function returns a list of unique matches between two vectors. data(Salaries) data(Fielding) intersect( names(Salaries), names(Fielding) ) ### [1] &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; &quot;playerID&quot; The match() function returns the position of matched elements. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) match( x, y ) ### [1] 3 1 NA 1 The %in% function returns a logical vector, where TRUE signifies that the element in y also occurs in x. In other words, does a specific element in y belong to the intersection of x,y. This is very useful for creating subsets of data that belong to both sets. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;B&quot;,&quot;F&quot;,&quot;B&quot;) y %in% x # does each element of y occur anywhere in x? ### [1] TRUE FALSE TRUE TRUE FALSE TRUE y[ y %in% x] # keep only data that occurs in both ### [1] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; 12.7 The Match Function Often times we do not need to merge data, we may just need sort data in one dataset so that it matches the order of another dataset. This is accomplished using the match() function. Note that we can rearrange the order of a dataset by referencing the desired position. x &lt;- c(&quot;Second&quot;,&quot;Third&quot;,&quot;First&quot;) x ### [1] &quot;Second&quot; &quot;Third&quot; &quot;First&quot; x[ c(3,1,2) ] ### [1] &quot;First&quot; &quot;Second&quot; &quot;Third&quot; The match() function returns the positions of matches of its first vector to the second vector listed in the arguments. Or in other words, the order that vector 2 would need to follow to match vector 1. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;) cbind( x, y ) ### x y ### [1,] &quot;A&quot; &quot;B&quot; ### [2,] &quot;B&quot; &quot;D&quot; ### [3,] &quot;C&quot; &quot;A&quot; match( x, y ) ### [1] 3 1 NA match( y, x) # not a symmetric operation! ### [1] 2 NA 1 # In the y vector: # # [3]=A # [1]=B # [NA]=D (no match) order.y &lt;- match( x, y ) y[ order.y ] ### [1] &quot;A&quot; &quot;B&quot; NA We can see that match() returns the correct order to put y in so that it matches the order of x. In the re-ordered vector, the first element is the original third element A, the second element is the original first element B, and there is no third element because D did not match anything in x. Note the order of arguments in the function: match( data I want to match to , data I need to re-order ) We can use this position information to re-order y as follows: x &lt;- sample( LETTERS[1:15], size=10 ) y &lt;- sample( LETTERS[1:15], size=10 ) cbind( x, y ) ### x y ### [1,] &quot;M&quot; &quot;K&quot; ### [2,] &quot;G&quot; &quot;L&quot; ### [3,] &quot;E&quot; &quot;F&quot; ### [4,] &quot;O&quot; &quot;B&quot; ### [5,] &quot;C&quot; &quot;D&quot; ### [6,] &quot;B&quot; &quot;I&quot; ### [7,] &quot;N&quot; &quot;M&quot; ### [8,] &quot;I&quot; &quot;E&quot; ### [9,] &quot;K&quot; &quot;O&quot; ### [10,] &quot;F&quot; &quot;N&quot; order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;M&quot; &quot;M&quot; ### [2,] &quot;G&quot; NA ### [3,] &quot;E&quot; &quot;E&quot; ### [4,] &quot;O&quot; &quot;O&quot; ### [5,] &quot;C&quot; NA ### [6,] &quot;B&quot; &quot;B&quot; ### [7,] &quot;N&quot; &quot;N&quot; ### [8,] &quot;I&quot; &quot;I&quot; ### [9,] &quot;K&quot; &quot;K&quot; ### [10,] &quot;F&quot; &quot;F&quot; # Note the result if you confuse the order or arguments order.y &lt;- match( y, x ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;M&quot; &quot;O&quot; ### [2,] &quot;G&quot; NA ### [3,] &quot;E&quot; &quot;N&quot; ### [4,] &quot;O&quot; &quot;I&quot; ### [5,] &quot;C&quot; NA ### [6,] &quot;B&quot; &quot;E&quot; ### [7,] &quot;N&quot; &quot;K&quot; ### [8,] &quot;I&quot; &quot;F&quot; ### [9,] &quot;K&quot; &quot;B&quot; ### [10,] &quot;F&quot; &quot;M&quot; This comes in handy when we are matching information between two tables. For example, in GIS the map regions follow a specific order but your data does not. Create a color scheme for levels of your data, and then re-order the colors so they match the correct region on the map. In this example, we will look at unemployment levels by county. library( maps ) data( county.fips ) data( unemp ) map( database=&quot;county&quot; ) # assign a color to each level of unemployment, red = high, gray = medium, blue = low color.function &lt;- colorRampPalette( c(&quot;steelblue&quot;, &quot;gray70&quot;, &quot;firebrick&quot;) ) color.vector &lt;- cut( rank(unemp$unemp), breaks=7, labels=color.function( 7 ) ) color.vector &lt;- as.character( color.vector ) head( color.vector ) ### [1] &quot;#B28282&quot; &quot;#B28282&quot; &quot;#B22222&quot; &quot;#B25252&quot; &quot;#B28282&quot; &quot;#B22222&quot; # doesn&#39;t look quite right map( database=&quot;county&quot;, col=color.vector, fill=T, lty=0 ) # what went wrong here? # our unemployment data (and thus the color vector) follows a different order cbind( map.id=county.fips$fips, data.id=unemp$fips, color.vector )[ 2500:2510 , ] ### map.id data.id color.vector ### [1,] &quot;48011&quot; &quot;47149&quot; &quot;#B28282&quot; ### [2,] &quot;48013&quot; &quot;47151&quot; &quot;#B22222&quot; ### [3,] &quot;48015&quot; &quot;47153&quot; &quot;#B22222&quot; ### [4,] &quot;48017&quot; &quot;47155&quot; &quot;#B28282&quot; ### [5,] &quot;48019&quot; &quot;47157&quot; &quot;#B28282&quot; ### [6,] &quot;48021&quot; &quot;47159&quot; &quot;#B22222&quot; ### [7,] &quot;48023&quot; &quot;47161&quot; &quot;#B25252&quot; ### [8,] &quot;48025&quot; &quot;47163&quot; &quot;#B3B3B3&quot; ### [9,] &quot;48027&quot; &quot;47165&quot; &quot;#B28282&quot; ### [10,] &quot;48029&quot; &quot;47167&quot; &quot;#B25252&quot; ### [11,] &quot;48031&quot; &quot;47169&quot; &quot;#B25252&quot; # place the color vector in the correct order this.order &lt;- match( county.fips$fips, unemp$fips ) color.vec.ordered &lt;- color.vector[ this.order ] # colors now match their correct counties map( database=&quot;county&quot;, col=color.vec.ordered, fill=T, lty=0 ) title( main=&quot;Unemployment Levels by County in 2009&quot;) Note that elements can be recycled from your y vector: x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) cbind( x, y ) ### x y ### [1,] &quot;A&quot; &quot;B&quot; ### [2,] &quot;B&quot; &quot;D&quot; ### [3,] &quot;C&quot; &quot;A&quot; ### [4,] &quot;B&quot; &quot;F&quot; match( x, y ) ### [1] 3 1 NA 1 order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) ### x y.new ### [1,] &quot;A&quot; &quot;A&quot; ### [2,] &quot;B&quot; &quot;B&quot; ### [3,] &quot;C&quot; NA ### [4,] &quot;B&quot; &quot;B&quot; "]
]
