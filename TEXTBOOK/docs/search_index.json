[
["index.html", "Intro to Data Science for the Social Sector Welcome", " Intro to Data Science for the Social Sector Jesse Lecy Latest build on August 16, 2018 Welcome Welcome to the course text for Data Science for the Social Sector. This text covers topics in R programming and project management "],
["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 Navigation 1.2 Commenting Code 1.3 Help 1.4 Install Programs (packages) 1.5 Accessing Built-In Datasets in R 1.6 MASTER TABLE", " Chapter 1 Introduction to R This lecture introduces you to basic operations when you first start using R such as navigation, the object-oriented framework, loading a package, and creating some data vectors. 1.1 Navigation You need to know a few operations to help you maneuver the R work environment, such as listing objects (datasets and functions) that are active, changing your working directory, listing available files, and finding help. 1.1.1 Setting Your Working Directory When you are ready to load data, R needs to know where to look for your files. You can check what is avaiable in the current directory (i.e. folder) by asking to list all of the current files using dir(). dir() If the file that you need is located in a different folder, you can change directories easily in R Studio by Session -&gt; Set working director -&gt; Choose directory (or Ctrl + Shift + H). If you are writing a script, you want to keep track of this step so that it can be reproduced. Use the function get.wd() to check your current working directory, and set.wd() to change. You need to specify your path as an argument to this function, such as. setwd( &quot;C:/user/projects/financial model&quot; ) NOTE! R uses unix style notation with forward slashes, so if you copy and paste from Windows it will look like this, with back slashes: setwd( &quot;C:\\user\\projects\\financial model&quot; ) You will need to change them around for it to work. It is best to save all of your steps in your scripts so that the analysis can be reproduced by yourself or others. In some cases you are doing exploratory or summary work, and you may want to find a file a quickly. You can use the file.choose() function to open a GUI to select your file directly. This function is used as an argument inside of a load data function. my.dat &lt;- read.csv( file.choose() ) 1.2 Commenting Code Most computer languages have a special character that is used to “comment out” lines so that it is not run by the program. It is used for two important purposes. First, we can add text to document our functions and it will not interfere with the program. And two, we can use it to run a program while ignoring some of the code, often for debugging purposes. The # hash tag is used for comments in R. ##============================================== ## ## Here is some documentation for this script ## ##============================================== x &lt;- 1:10 sum( x ) #&gt; [1] 55 # y &lt;- 1:25 # not run # sum( y ) # not run 1.3 Help You will use the help functions frequently to figure out what arguments and values are needed for specific functions. Because R is very customizable, you will find that many functions have several or dozens of arguments, and it is difficult to remember the correct syntax and values. But don’t worry, to look them up all you need is the function name and a call for help: help( dotchart ) # opens an external helpfile If you just need to remind yourself which arguments are defined in a function, you can use the args() command: args( dotchart ) #&gt; function (x, labels = NULL, groups = NULL, gdata = NULL, cex = par(&quot;cex&quot;), #&gt; pt.cex = cex, pch = 21, gpch = 21, bg = par(&quot;bg&quot;), color = par(&quot;fg&quot;), #&gt; gcolor = par(&quot;fg&quot;), lcolor = &quot;gray&quot;, xlim = range(x[is.finite(x)]), #&gt; main = NULL, xlab = NULL, ylab = NULL, ...) #&gt; NULL If you can’t recall a function name, you can list all of the functions from a specific package as follows: help( package=“stats” ) # lists all functions in stats package 1.4 Install Programs (packages) When you open R by default it will launch a core set of programs, called “packages” in R speak, that are use for most data operations. To see which packages are currently active use the search() function. search() #&gt; [1] &quot;.GlobalEnv&quot; &quot;package:pander&quot; &quot;package:dplyr&quot; #&gt; [4] &quot;package:bookdown&quot; &quot;package:rmarkdown&quot; &quot;package:stats&quot; #&gt; [7] &quot;package:graphics&quot; &quot;package:grDevices&quot; &quot;package:utils&quot; #&gt; [10] &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; #&gt; [13] &quot;package:base&quot; These programs manage the basic data operations, run the core graphics engine, and give you basic statistical methods. The real magic for R comes from the over 7,000 contributed packages available on the CRAN: https://cran.r-project.org/web/views/ A package consists of custom functions and datasets that are generated by users. They are packaged together so that they can be shared with others. A package also includes documentation that describes each function, defines all of the arguments, and documents any datasets that are included. If you know a package name, it is easy to install. In R Studio you can select Tools -&gt; Install Packages and a list of available packages will be generated. But it is easier to use the install.packages() command. We will use the Lahman Package in this course, so let’s install that now. Description This package provides the tables from Sean Lahman’s Baseball Database as a set of R data.frames. It uses the data on pitching, hitting and fielding performance and other tables from 1871 through 2013, as recorded in the 2014 version of the database. See the documentation here: https://cran.r-project.org/web/packages/Lahman/Lahman.pdf install.packages( &quot;Lahman&quot; ) You will be asked to select a “mirror”. In R speak this just means the server from which you will download the package (choose anything nearby). R is a community of developers and universities that create code and maintain the infrastructure. A couple of dozen universities around the world host servers that contain copies of the R packages so that they can be easily accessed everywhere. If the package is successfully installed you will get a message similar to this: package ‘Lahman’ successfully unpacked and MD5 sums checked Once a new program is installed you can now open (“load” in R speak) the package using the library() command: library( &quot;Lahman&quot; ) If you now type search() you can see that Lahman has been added to the list of active programs. We can now access all of the functions and data that are available in the Lahman package. 1.5 Accessing Built-In Datasets in R One nice feature of R is that is comes with a bunch of built-in datasets that have been contributed by users are are loaded automatically. You can see the list of available datasets by typing: data() This will list all of the default datasets in core R packages. If you want to see all of the datasets available in installed packages as well use: data( package = .packages(all.available = TRUE) ) 1.5.1 Basic Data Operations Let’s ignore the underlying data structure right now and look at some ways that we might interact with data. We will use the USArrests dataset available in the core files. To access the data we need to load it into working memory. Anything that is active in R will be listed in the environment, which you can check using the ls() command. We will load the dataset using the data() command. remove( list=ls() ) ls() # nothing currently available #&gt; character(0) data( &quot;USArrests&quot; ) ls() # data is now avaible for use #&gt; [1] &quot;USArrests&quot; Now that we have loaded a dataset, we can start to access the variables and analyze relationships. Let’s get to know our dataset. names( USArrests ) # what variables do you have? #&gt; [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; row.names( USArrests ) # what are the obsevations (rows) in our data #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; #&gt; [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; #&gt; [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; #&gt; [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; #&gt; [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; #&gt; [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; #&gt; [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; #&gt; [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; #&gt; [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; #&gt; [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; #&gt; [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; #&gt; [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; #&gt; [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; nrow( USArrests ) # how many observations are there? #&gt; [1] 50 dim( USArrests ) # a quick way to see rows and columns - the dimensions of the dataset #&gt; [1] 50 4 summary( USArrests ) # summary statistics of variables #&gt; Murder Assault UrbanPop Rape #&gt; Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 #&gt; 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 #&gt; Median : 7.250 Median :159.0 Median :66.00 Median :20.10 #&gt; Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 #&gt; 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 #&gt; Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 We can see that the dataset consists of four variables: Murder, Assault, UrbanPop, and Rape. We also see that our unit of analysis is the state. But where does the data come from, and how are these variables measured? To see the documentation for a specific dataset you will need to use the help() function: help( &quot;USArrests&quot; ) We get valuable information about the source and metrics: Description This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas. Format A data frame with 50 observations on 4 variables. [,1] Murder numeric Murder arrests (per 100,000) [,2] Assault numeric Assault arrests (per 100,000) [,3] UrbanPop numeric Percent urban population [,4] Rape numeric Rape arrests (per 100,000) To access a specific variable inside of a dataset, you will use the $ operator between the dataset name and the variable name: summary( USArrests$Murder ) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.800 4.075 7.250 7.788 11.250 17.400 summary( USArrests$Assault ) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 45.0 109.0 159.0 170.8 249.0 337.0 # Is there a relationship between urban density and crime? plot( USArrests$UrbanPop, USArrests$Murder ) abline( lm( USArrests$Murder ~ USArrests$UrbanPop ), col=&quot;red&quot; ) 1.5.2 Using the Lahman Data Let’s take a look at some of the data available in the Lahman package. data( package = &quot;Lahman&quot; ) # Data sets in package ‘Lahman’: # # AllstarFull AllstarFull table # Appearances Appearances table # AwardsManagers AwardsManagers table # AwardsPlayers AwardsPlayers table # AwardsShareManagers AwardsShareManagers table # AwardsSharePlayers AwardsSharePlayers table # Batting Batting table # BattingPost BattingPost table # CollegePlaying CollegePlaying table # Fielding Fielding table # FieldingOF FieldingOF table # FieldingPost FieldingPost data # HallOfFame Hall of Fame Voting Data # LahmanData Lahman Datasets # Managers Managers table # ManagersHalf ManagersHalf table # Master Master table # Pitching Pitching table # PitchingPost PitchingPost table # Salaries Salaries table # Schools Schools table # SeriesPost SeriesPost table # Teams Teams table # TeamsFranchises TeamFranchises table # TeamsHalf TeamsHalf table # battingLabels Variable Labels # fieldingLabels Variable Labels # pitchingLabels Variable Labels We see that we have lots of datasets to choose from here. I will use the Master dataset, which is a list of all of the Major League Baseball players over the past century, and their personal information. library( Lahman ) data( Master ) names( Master ) #&gt; [1] &quot;playerID&quot; &quot;birthYear&quot; &quot;birthMonth&quot; &quot;birthDay&quot; #&gt; [5] &quot;birthCountry&quot; &quot;birthState&quot; &quot;birthCity&quot; &quot;deathYear&quot; #&gt; [9] &quot;deathMonth&quot; &quot;deathDay&quot; &quot;deathCountry&quot; &quot;deathState&quot; #&gt; [13] &quot;deathCity&quot; &quot;nameFirst&quot; &quot;nameLast&quot; &quot;nameGiven&quot; #&gt; [17] &quot;weight&quot; &quot;height&quot; &quot;bats&quot; &quot;throws&quot; #&gt; [21] &quot;debut&quot; &quot;finalGame&quot; &quot;retroID&quot; &quot;bbrefID&quot; #&gt; [25] &quot;deathDate&quot; &quot;birthDate&quot; nrow( Master ) # 18,354 players included #&gt; [1] 19105 summary( Master ) #&gt; playerID birthYear birthMonth birthDay #&gt; Length:19105 Min. :1820 Min. : 1.000 Min. : 1.00 #&gt; Class :character 1st Qu.:1895 1st Qu.: 4.000 1st Qu.: 8.00 #&gt; Mode :character Median :1937 Median : 7.000 Median :16.00 #&gt; Mean :1931 Mean : 6.629 Mean :15.61 #&gt; 3rd Qu.:1969 3rd Qu.:10.000 3rd Qu.:23.00 #&gt; Max. :1996 Max. :12.000 Max. :31.00 #&gt; NA&#39;s :132 NA&#39;s :302 NA&#39;s :449 #&gt; birthCountry birthState birthCity deathYear #&gt; Length:19105 Length:19105 Length:19105 Min. :1872 #&gt; Class :character Class :character Class :character 1st Qu.:1942 #&gt; Mode :character Mode :character Mode :character Median :1967 #&gt; Mean :1964 #&gt; 3rd Qu.:1990 #&gt; Max. :2017 #&gt; NA&#39;s :9664 #&gt; deathMonth deathDay deathCountry deathState #&gt; Min. : 1.000 Min. : 1.00 Length:19105 Length:19105 #&gt; 1st Qu.: 3.000 1st Qu.: 8.00 Class :character Class :character #&gt; Median : 6.000 Median :15.00 Mode :character Mode :character #&gt; Mean : 6.484 Mean :15.57 #&gt; 3rd Qu.:10.000 3rd Qu.:23.00 #&gt; Max. :12.000 Max. :31.00 #&gt; NA&#39;s :9665 NA&#39;s :9666 #&gt; deathCity nameFirst nameLast #&gt; Length:19105 Length:19105 Length:19105 #&gt; Class :character Class :character Class :character #&gt; Mode :character Mode :character Mode :character #&gt; #&gt; #&gt; #&gt; #&gt; nameGiven weight height bats #&gt; Length:19105 Min. : 65.0 Min. :43.00 B : 1175 #&gt; Class :character 1st Qu.:170.0 1st Qu.:71.00 L : 4957 #&gt; Mode :character Median :185.0 Median :72.00 R :11788 #&gt; Mean :186.4 Mean :72.27 NA&#39;s: 1185 #&gt; 3rd Qu.:200.0 3rd Qu.:74.00 #&gt; Max. :320.0 Max. :83.00 #&gt; NA&#39;s :854 NA&#39;s :785 #&gt; throws debut finalGame retroID #&gt; L : 3653 Length:19105 Length:19105 Length:19105 #&gt; R :14472 Class :character Class :character Class :character #&gt; S : 1 Mode :character Mode :character Mode :character #&gt; NA&#39;s: 979 #&gt; #&gt; #&gt; #&gt; bbrefID deathDate birthDate #&gt; Length:19105 Min. :1872-03-17 Min. :1820-04-17 #&gt; Class :character 1st Qu.:1942-06-05 1st Qu.:1896-11-21 #&gt; Mode :character Median :1967-03-06 Median :1939-01-01 #&gt; Mean :1964-10-11 Mean :1933-03-08 #&gt; 3rd Qu.:1990-09-22 3rd Qu.:1970-04-02 #&gt; Max. :2017-02-19 Max. :1996-08-12 #&gt; NA&#39;s :9666 NA&#39;s :449 We can use help(Master) to get information about the dataset, including a data dictionary. help( Master ) 1.6 MASTER TABLE Description Master table - Player names, DOB, and biographical info. This file is to be used to get details about players listed in the Batting, Pitching, and other files where players are identified only by playerID. Usage data(Master) Format A data frame with 19105 observations on the following 26 variables. playerID: A unique code asssigned to each player. The playerID links the data in this file with records on players in the other files. birthYear: Year player was born birthMonth: Month player was born birthDay: Day player was born birthCountry: Country where player was born birthState: State where player was born birthCity: City where player was born deathYear: Year player died deathMonth: Month player died deathDay: Day player died deathCountry: Country where player died deathState: State where player died deathCity: City where player died nameFirst: Player’s first name nameLast: Player’s last name nameGiven: Player’s given name (typically first and middle) weight: Player’s weight in pounds height: Player’s height in inches bats: a factor: Player’s batting hand (left (L), right (R), or both (B)) throws: a factor: Player’s throwing hand (left(L) or right(R)) debut: Date that player made first major league appearance finalGame: Date that player made first major league appearance (blank if still active) retroID: ID used by retrosheet, http://www.retrosheet.org/ bbrefID: ID used by Baseball Reference website, http://www.baseball-reference.com/ birthDate: Player’s birthdate, in as.Date format deathDate: Player’s deathdate, in as.Date format Details debut, finalGame were converted from character strings with as.Date. Source Lahman, S. (2016) Lahman’s Baseball Database, 1871-2015, 2015 version, http://www.seanlahman.com/baseball-archive/statistics/ 1.6.1 Example Analysis Perhaps I am curious about some of the data. I see that we have information on the birth month of professional baseball players. If you have read Malcolm Gladwell’s book Outliers you know there is an interesting cumulative advantage phenomenon that can occur with atheletes as they are young. If you are born near the end of the cutoff, you are on average six months older than other players in your league, and therefore slightly larger physically and more coordinated on average. Six months does not sound like much, but the slight size and coordination advantage means more playing time, which also improves skill. Over time, this small difference accumulates so that those lucky enough to be born near the cutoff become the best players. Gladwell looked at studies of hockey. Do we see this in baseball? table( Master$birthMonth ) #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 #&gt; 1631 1439 1528 1450 1449 1366 1503 1831 1661 1759 1620 1566 tab &lt;- prop.table( table( Master$birthMonth ) ) names(tab) &lt;- c(&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;,&quot;Dec&quot;) dotchart( tab, pch=19, xlab = &quot;Percent of Players&quot;, ylab = &quot;Birth Month&quot; ) "],
["data-structures.html", "Chapter 2 Data Structures 2.1 Vectors 2.2 Casting 2.3 The Combine Function 2.4 Numeric Vectors 2.5 Character Vectors 2.6 Logical Vectors 2.7 Factors 2.8 Generating Vectors", " Chapter 2 Data Structures 2.1 Vectors Vectors are the building blocks of data programming in R, so they are extremely important concepts. Very loosely speaking a vector is a set of numbers or words. For example, [ 1, 2, 3] or [ apple, orange, pear ]. In social science, a vector is a variable in a dataset. You will spend a lot of time creating data vectors, transforming variables, generating subsets, cleaning data, and adding new observations. These are all accomplished through operators (commands) that act on vectors. There are four primary vector types (“classes”) in R: Class Description numeric Typical variable of only numbers character A vector of letters or words, always enclosed with quotes factor Categories which represent groups, like treatment and control logical A vector of TRUE and FALSE to designate which observations fit a criteria Each vector or dataset has a “class” that tells R the data type. These different vectors can be combined into three different types of datasets (data frames, matrices, and lists), which will be discussed below. x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) x4 &lt;- c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) x5 &lt;- x4 == &quot;treatment&quot; dat &lt;- data.frame( name=x2, sex=x3, treat=x4, is.treat=x5, strength=x1 ) name sex treat is.treat strength adam male treatment TRUE 167 jamal male control FALSE 185 linda female treatment TRUE 119 sriti female control FALSE 142 R keeps track of the data type of each object: class( x1 ) # c(167,185,119,142) #&gt; [1] &quot;numeric&quot; class( x2 ) # c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) #&gt; [1] &quot;character&quot; class( x3 ) # factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) #&gt; [1] &quot;factor&quot; class( x5 ) # c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) #&gt; [1] &quot;logical&quot; class( dat ) # data.frame( name=x2, sex=x3, treat=x4, is.treat=x5, strength=x1 ) #&gt; [1] &quot;data.frame&quot; We often need to know how many elements belong to a vector, which we find with the length() function. x1 #&gt; [1] 167 185 119 142 length( x1 ) #&gt; [1] 4 2.2 Casting You can easily move from one data type to another by casting a specific type as another type: x &lt;- 1:5 x #&gt; [1] 1 2 3 4 5 as.character(x) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; y &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) y #&gt; [1] TRUE FALSE TRUE TRUE FALSE as.numeric( y ) #&gt; [1] 1 0 1 1 0 as.character( y ) #&gt; [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; &quot;TRUE&quot; &quot;FALSE&quot; But in some cases it might not make sense to cast one variable type as another. z &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) z #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; as.numeric( z ) #&gt; [1] NA NA NA Casting will often be induced when you try to combine different types of data. For example, when you add a character element to a numeric vector, the whole vector will be cast as a character vector. x10 &lt;- 1:5 x10 #&gt; [1] 1 2 3 4 5 # a vector can only have one data type x11 &lt;- c( x10, &quot;a&quot;) x11 # all numbers silently recast as characters #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;a&quot; When you read data in from outside sources, the input functions often will cast numeric vectors as factors if they contain a low number of elements. See the section on factors below for special instrucdtions on moving from factors to numeric vectors. 2.3 The Combine Function We often need to combine several elements into a single vector, or two vectors to form one. This is done using the c() function. c(1,2,3) #&gt; [1] 1 2 3 c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; x &lt;- 1:5 y &lt;- 10:15 z &lt;- c(x,y) z #&gt; [1] 1 2 3 4 5 10 11 12 13 14 15 c(z,&quot;a&quot;) # combining data types #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;a&quot; 2.4 Numeric Vectors There are some specific things to note about each vector type. Math operators will only work on numeric vectors. summary( x1 ) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 119.0 136.2 154.5 153.2 171.5 185.0 Note that if we try to run this mathematicl function we get an error: sum( x2 ) # Error in sum(x2) : invalid &#39;type&#39; (character) of argument Many functions in R are sensitive to the data type of vectors. Mathematical functions, for example, do not make sense when applied to text (character vectors). In many cases R will give an error. In some cases R will silently re-cast the variable, then perform the operation. Be watchful for when silent re-casting occurs because it might have unwanted side effects, such as deleting data or re-coding group levels in the wrong way. 2.4.1 Integers Are Simple Numeric Vectors The integer vector is a special type of numeric vector. It is used to save memory since integers require less space than numbers that contain decimals points (you need to allocate space for the numbers to the left and the numbers to the right of the decimal). Google “computer memory allocation” if you are interested in the specifics. If you are doing advanced programming you will be more sensitive to memory allocation and the speed of your code, but in the intro class we will not differentiate between the two types of number vectors. In most cases they result in the same results, unless you are doing advanced numerical analysis where rounding errors matter. n &lt;- 1:5 n #&gt; [1] 1 2 3 4 5 class( n ) #&gt; [1] &quot;integer&quot; n[ 2 ] &lt;- 2.01 n # all elements converted to decimals #&gt; [1] 1.00 2.01 3.00 4.00 5.00 class( n ) #&gt; [1] &quot;numeric&quot; 2.5 Character Vectors The most important rule to remember with this data type: when creating character vectors, all text must be enclosed by quotation marks. c( &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ) # this works #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # c( a, b, c ) # this doesn&#39;t work # Error: object &#39;a&#39; not found When you type characters surrounded by quotes then R knows you are creating new text (“strings” in programming speak). When you type characters that are not surrounded by quotes, R thinks that you are looking for an object in the environment, like the variables we have already created. It gets confused when it doesn’t find the object that you typed. In generate, you will use quotes when you are creating character vectors, and for arguments in functions. You do not use quotes when you are referencing an active object. ls() # these are all of the active objects #&gt; [1] &quot;dat&quot; &quot;Master&quot; &quot;n&quot; &quot;tab&quot; &quot;USArrests&quot; #&gt; [6] &quot;x&quot; &quot;x1&quot; &quot;x10&quot; &quot;x11&quot; &quot;x2&quot; #&gt; [11] &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; &quot;y&quot; &quot;z&quot; &quot;x1&quot; #&gt; [1] &quot;x1&quot; x1 #&gt; [1] 167 185 119 142 plot( rnorm(5), col=&quot;blue&quot;, pch=19, cex=4 ) # text used for color argument, use quotes # what if we want colors that represent groups in our data plot( 1:4, x1, col=x3, pch=19, cex=4 ) # object used for color argument, no quotes Recall that x3 is the sex of study participants, so the different colors represent the different genders in the study. 2.6 Logical Vectors Logical vectors are collections of a set of TRUE and FALSE statements. Logical statements allow us to define groups based upon criteria, then decide whether observations belong to the group. See the section on operators below for a complete list of logical statements. Logical vectors are important because organizing data into these sets is what drives all of the advanced data analytics (set theory is at the basis of mathematics and computer science). dat #&gt; name sex treat is.treat strength #&gt; 1 adam male treatment TRUE 167 #&gt; 2 jamal male control FALSE 185 #&gt; 3 linda female treatment TRUE 119 #&gt; 4 sriti female control FALSE 142 dat$name == &quot;sriti&quot; #&gt; [1] FALSE FALSE FALSE TRUE dat$sex == &quot;male&quot; #&gt; [1] TRUE TRUE FALSE FALSE dat$strength &gt; 180 #&gt; [1] FALSE TRUE FALSE FALSE Typically logical vectors are used in combination with subset operators to identify specific groups in the data. dat #&gt; name sex treat is.treat strength #&gt; 1 adam male treatment TRUE 167 #&gt; 2 jamal male control FALSE 185 #&gt; 3 linda female treatment TRUE 119 #&gt; 4 sriti female control FALSE 142 # isolate data on all of the females in the dataset dat[ dat$sex == &quot;female&quot; , ] #&gt; name sex treat is.treat strength #&gt; 3 linda female treatment TRUE 119 #&gt; 4 sriti female control FALSE 142 When defining logical vectors, you can use the abbreviated versions of T for TRUE and F for FALSE. z1 &lt;- c(T,T,F,T,F,F) z1 #&gt; [1] TRUE TRUE FALSE TRUE FALSE FALSE Note how NAs affect complex logical statements: TRUE &amp; TRUE #&gt; [1] TRUE TRUE &amp; FALSE #&gt; [1] FALSE TRUE &amp; NA #&gt; [1] NA FALSE &amp; NA #&gt; [1] FALSE If one condition is TRUE, and another is NA, R does not want to throw out the data because the state of the missing value is unclear. As a result, it will preserve the observation, but it will replace all of the data with missing values: dat #&gt; name sex treat is.treat strength #&gt; 1 adam male treatment TRUE 167 #&gt; 2 jamal male control FALSE 185 #&gt; 3 linda female treatment TRUE 119 #&gt; 4 sriti female control FALSE 142 keep.these &lt;- c(T,F,NA,F) dat[ keep.these , ] #&gt; name sex treat is.treat strength #&gt; 1 adam male treatment TRUE 167 #&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA To remove these rows, replace all NAs in your selector vector with FALSE: keep.these[ is.na(keep.these) ] &lt;- FALSE dat[ keep.these , ] #&gt; name sex treat is.treat strength #&gt; 1 adam male treatment TRUE 167 2.7 Factors When there are categorical variables within our data, or groups, then we use a special vector to keep track of these groups. We could just use numbers (1=female, 0=male) or characters (“male”,“female”), but factors are useful for two reasons. First, it saves memory. Text is very “expensive” in terms of memory allocation and processing speed, so using simpler data structure makes R faster. Second, when a variable is set as a factor, R recognizes that it represents a group and it can deploy object-oriented functionality. When you use a factor in analysis, R knows that you want to split the analysis up by groups. height &lt;- c( 70, 68, 62, 64, 72, 69, 58, 63 ) strength &lt;- c(167,185,119,142,175,204,124,117) sex &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot; ) ) plot( height, strength ) # simple scatter plot plot( sex, strength ) # box and whisker plot Factors are more memory efficient than character vectors because they store the underlying data as a numeric vector instead of a categorical (text) vector. Each group in the data is assigned a number, and when printing items the program only has to remember which group corresponds to which number: sex #&gt; [1] male male female female male male female female #&gt; Levels: female male as.numeric( sex ) #&gt; [1] 2 2 1 1 2 2 1 1 # male = 2 # female = 1 If you print a factor, the computer just replaces each category designation with its name (2 would be replaced with “male” in this example). These replacements can be done in real time without clogging the memory of your computer as they don’t need to be saved. In some instances a categorical variable might be represented by numbers. For example, grades 9-12 for high school kids. The very important rule to remember with factors is you can’t move directly from the factor to numeric using the as.numeric() casting function. This will give you the underlying data structure, but will not give you the category names. To get these, you need the as.character casting function. grades &lt;- sample( x=9:12, size=10, replace=T ) grades #&gt; [1] 10 11 11 12 11 9 9 9 11 12 grades &lt;- as.factor( grades ) grades #&gt; [1] 10 11 11 12 11 9 9 9 11 12 #&gt; Levels: 9 10 11 12 as.numeric( grades ) #&gt; [1] 2 3 3 4 3 1 1 1 3 4 as.character( grades ) #&gt; [1] &quot;10&quot; &quot;11&quot; &quot;11&quot; &quot;12&quot; &quot;11&quot; &quot;9&quot; &quot;9&quot; &quot;9&quot; &quot;11&quot; &quot;12&quot; # to get back to the original numeric vector as.numeric( as.character( grades )) #&gt; [1] 10 11 11 12 11 9 9 9 11 12 Note that when subsetting a factor, it will retain all of the original levels, even when they are not in use. In this example, there are 37 teams in the Lahman dataset (some of them defunct) and 16 teams in the National League in 2002. But after applying the year and league subsets you will still have 37 levels. # there are only 16 teams in the NL in 2002 sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) #&gt; [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; #&gt; [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; #&gt; [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; #&gt; [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; #&gt; [45] &quot;WAS&quot; &quot;WSN&quot; After applying a subset, in order to remove the unused factor levels you need to apply either droplevels(), or else recast your factor as a new factor. For example: sals.2002 &lt;- Salaries [Salaries$yearID==&quot;2002&quot;, ] nl.sals &lt;- sals.2002 [ sals.2002$lgID == &quot;NL&quot;,] levels( nl.sals$teamID ) #&gt; [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; #&gt; [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; #&gt; [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; #&gt; [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; #&gt; [45] &quot;WAS&quot; &quot;WSN&quot; # fix in one of two equivalent ways: # # nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) # nl.sals$teamID &lt;- factor( nl.sals$teamID ) levels( nl.sals$teamID ) #&gt; [1] &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BOS&quot; &quot;CAL&quot; &quot;CHA&quot; &quot;CHC&quot; &quot;CHN&quot; &quot;CHW&quot; &quot;CIN&quot; #&gt; [12] &quot;CLE&quot; &quot;COL&quot; &quot;DET&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;KCA&quot; &quot;KCR&quot; &quot;LAA&quot; &quot;LAD&quot; &quot;LAN&quot; &quot;MIA&quot; #&gt; [23] &quot;MIL&quot; &quot;MIN&quot; &quot;ML4&quot; &quot;MON&quot; &quot;NYA&quot; &quot;NYM&quot; &quot;NYN&quot; &quot;NYY&quot; &quot;OAK&quot; &quot;PHI&quot; &quot;PIT&quot; #&gt; [34] &quot;SDN&quot; &quot;SDP&quot; &quot;SEA&quot; &quot;SFG&quot; &quot;SFN&quot; &quot;SLN&quot; &quot;STL&quot; &quot;TBA&quot; &quot;TBR&quot; &quot;TEX&quot; &quot;TOR&quot; #&gt; [45] &quot;WAS&quot; &quot;WSN&quot; nl.sals$teamID &lt;- droplevels( nl.sals$teamID ) levels( nl.sals$teamID ) #&gt; [1] &quot;ARI&quot; &quot;ATL&quot; &quot;CHN&quot; &quot;CIN&quot; &quot;COL&quot; &quot;FLO&quot; &quot;HOU&quot; &quot;LAN&quot; &quot;MIL&quot; &quot;MON&quot; &quot;NYN&quot; #&gt; [12] &quot;PHI&quot; &quot;PIT&quot; &quot;SDN&quot; &quot;SFN&quot; &quot;SLN&quot; 2.8 Generating Vectors You will often need to generate vectors for data transformations or simulations. Here are the most common functions that will be helpful. # repeat a number, or series of numbers rep( x=9, times=5 ) #&gt; [1] 9 9 9 9 9 rep( x=c(5,7), times=5 ) #&gt; [1] 5 7 5 7 5 7 5 7 5 7 rep( x=c(5,7), each=5 ) #&gt; [1] 5 5 5 5 5 7 7 7 7 7 rep( x=c(&quot;treatment&quot;,&quot;control&quot;), each=5 ) # also works to create categories #&gt; [1] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; #&gt; [6] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; # create a sequence of numbers seq( from=1, to=15, by=1 ) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 seq( from=1, to=15, by=3 ) #&gt; [1] 1 4 7 10 13 1:15 # shorthand if by=1 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # create a random sample hat &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;f&quot;) sample( x=hat, size=3, replace=FALSE ) #&gt; [1] &quot;b&quot; &quot;c&quot; &quot;f&quot; sample( x=hat, size=3, replace=FALSE ) #&gt; [1] &quot;f&quot; &quot;c&quot; &quot;b&quot; sample( x=hat, size=3, replace=FALSE ) #&gt; [1] &quot;c&quot; &quot;a&quot; &quot;f&quot; # for multiple samples use replacement sample( x=hat, size=10, replace=TRUE ) #&gt; [1] &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;a&quot; &quot;f&quot; &quot;a&quot; &quot;b&quot; # create data that follows a normal curve iq &lt;- rnorm( n=1000, mean=100, sd=15 ) hist( iq, col=&quot;gray&quot; ) "],
["operators.html", "Chapter 3 Operators 3.1 Datasets 3.2 Subsets 3.3 Variable Transformations 3.4 Missing Values: NA’s 3.5 The ‘attach’ Function", " Chapter 3 Operators Logical operators are the most basic type of data programming and the core of many types of data analysis. Most of the time we are not conducting fancy statistics, we just want to identify members of a group (print all of the females from the study), or describe things that belong to a subset of the data (compare the average price of houses with garages to houses without garages). In order to accomplish these simple tasks we need to use logic statements. A logic statement answers the question, does an observation belong to a group. Many times groups are simple. Show me all of the professions that make over $100k a year, for example. Sometimes groups are complex. Identify the African American children from a specific zip code in Chicago that live in households with single mothers. You will use nine basic logical operators: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to x | y x OR y x &amp; y x AND y ! opposite of Logical operators create logical vectors, a vector that contains only TRUE or FALSE. The TRUE means that the observation belongs to the group, FALSE means it does not. x1 &lt;- c(7,9,1,2) x2 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x3 &lt;- c(&quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) x1 &gt; 7 #&gt; [1] FALSE TRUE FALSE FALSE x1 &gt;= 7 #&gt; [1] TRUE TRUE FALSE FALSE x1 == 9 #&gt; [1] FALSE TRUE FALSE FALSE x1 = 9 # don&#39;t use a single equals operator! it overwrites your variable x1 #&gt; [1] 9 x1 &lt;- c(7,9,1,2) x1 == 9 | x1 == 1 #&gt; [1] FALSE TRUE TRUE FALSE # x2 == male # this will not work because male is not enclosed with quotes x2 == &quot;female&quot; #&gt; [1] FALSE FALSE TRUE TRUE x2 == &quot;female&quot; &amp; x3 == &quot;treatment&quot; #&gt; [1] FALSE FALSE TRUE FALSE Note that we use operators to create logical vectors where TRUE designates observation that belong to the defined group, and FALSE designates observations outside the group. We use these logical vectors in three ways: We can create a selector variable that is used for subsets. When a logical vector is passed to the subset function it will keep all observations with a TRUE value, and drop observations with a FALSE value. x1 #&gt; [1] 7 9 1 2 x1 &gt; 5 #&gt; [1] TRUE TRUE FALSE FALSE keep.these &lt;- x1 &gt; 5 x1[ keep.these ] #&gt; [1] 7 9 # you can create a selector variable with one variable, and apply it to another x2[ keep.these ] # sex of observations where x1 &gt; 5 #&gt; [1] &quot;male&quot; &quot;male&quot; Logical vectors give us an easy way to count things within defined groups. We can apply a sum() function to a logical vector, and the result will be a tally of all of the TRUE cases. # how many females do we have in our study? sum( x2 == &quot;female&quot; ) #&gt; [1] 2 # how many females do we have in our treatment group? sum( x2 == &quot;female&quot; &amp; x3 == &quot;treatment&quot; ) #&gt; [1] 1 We use selector variables to replace observations with new values using the assignment operator. This is similar to a find and replace operation. x7 &lt;- c( &quot;mole&quot;,&quot;mouse&quot;,&quot;shrew&quot;,&quot;mouse&quot;,&quot;rat&quot;,&quot;shrew&quot;) # the lab assistant incorrectly identified the shrews x7 #&gt; [1] &quot;mole&quot; &quot;mouse&quot; &quot;shrew&quot; &quot;mouse&quot; &quot;rat&quot; &quot;shrew&quot; x7[ x7 == &quot;shrew&quot; ] &lt;- &quot;possum&quot; x7 #&gt; [1] &quot;mole&quot; &quot;mouse&quot; &quot;possum&quot; &quot;mouse&quot; &quot;rat&quot; &quot;possum&quot; # we don&#39;t know if linda received the treatment x3 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x4 &lt;- c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot;) x4[ x3 == &quot;linda&quot; ] &lt;- NA x4 #&gt; [1] &quot;treatment&quot; &quot;control&quot; NA &quot;control&quot; The ! operator is a special case, where it is not used to define a new logical vector, but rather it swaps the values of an existing logical vector. x1 #&gt; [1] 7 9 1 2 these &lt;- x1 &gt; 5 these #&gt; [1] TRUE TRUE FALSE FALSE ! these #&gt; [1] FALSE FALSE TRUE TRUE ! TRUE #&gt; [1] FALSE ! FALSE #&gt; [1] TRUE 3.1 Datasets When we combine multiple vectors together, we now have a dataset. There are three main types that we will use in this class. Class Description data frame A typical data set comprised of several variables matrix A data set comprised of only numbers, used for matrix math list The grab bag of data structures - several vectors held together 3.1.1 Data Frames The most familiar spreadsheet-type data structure is called a data frame in R. It consists of rows, which represent observations, and columns, which represent variables. data( USArrests ) dim( USArrests ) # number of rows by number of columns #&gt; [1] 50 4 names( USArrests ) # variable names or column names #&gt; [1] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; row.names( USArrests ) #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; #&gt; [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; #&gt; [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; #&gt; [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; #&gt; [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; #&gt; [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; #&gt; [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; #&gt; [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; #&gt; [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; #&gt; [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; #&gt; [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; #&gt; [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; #&gt; [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; head( USArrests ) # print first six rows of the data #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 #&gt; Colorado 7.9 204 78 38.7 3.1.2 Matrices A matrix is also a rectangular data object that consists of collections of vectors, but it is special in the sense that it only has numeric vectors and no variable names. mat &lt;- matrix( 1:20, nrow=5 ) mat #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 6 11 16 #&gt; [2,] 2 7 12 17 #&gt; [3,] 3 8 13 18 #&gt; [4,] 4 9 14 19 #&gt; [5,] 5 10 15 20 names( mat ) #&gt; NULL dim( mat ) #&gt; [1] 5 4 as.data.frame( mat ) # creates variable names #&gt; V1 V2 V3 V4 #&gt; 1 1 6 11 16 #&gt; 2 2 7 12 17 #&gt; 3 3 8 13 18 #&gt; 4 4 9 14 19 #&gt; 5 5 10 15 20 These are used almost exclusively for matrix algebra operations, which are fundamental to mathematical statistics. We will not use matrices in this course. 3.1.3 Lists The list is the most flexible data structure. It is created by sticking a bunch of unrelated vectors or datasets together. For example, when you run a regression you generate a bunch of interesting information. This information is saved as a list. x &lt;- 1:100 y &lt;- 2*x + rnorm( 100, 0, 10) m.01 &lt;- lm( y ~ x ) names( m.01 ) #&gt; [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; #&gt; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; #&gt; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; m.01$coefficients #&gt; (Intercept) x #&gt; 3.773543 1.935404 m.01$residuals #&gt; 1 2 3 4 5 #&gt; -8.13310718 -8.23789681 21.11447524 -4.09429834 1.67129012 #&gt; 6 7 8 9 10 #&gt; 7.72914917 1.93295585 3.00194707 1.11441266 -12.77126023 #&gt; 11 12 13 14 15 #&gt; -5.33103909 -15.54988910 -14.15916638 19.00214971 -13.19821251 #&gt; 16 17 18 19 20 #&gt; 16.38708809 14.13895094 10.58562304 3.67784817 1.95310457 #&gt; 21 22 23 24 25 #&gt; -6.63552339 -5.77635509 -1.08261898 -22.29576346 20.45115670 #&gt; 26 27 28 29 30 #&gt; -12.13049695 -16.20396455 10.85106555 -19.38767440 -2.78035771 #&gt; 31 32 33 34 35 #&gt; 10.72046885 -10.36124446 10.08180496 1.11527076 5.55531934 #&gt; 36 37 38 39 40 #&gt; 12.98604093 -14.77633549 -13.20817437 7.42295194 -3.00534367 #&gt; 41 42 43 44 45 #&gt; -5.91139248 0.03175416 3.18516519 -2.12720835 8.97747037 #&gt; 46 47 48 49 50 #&gt; 7.72667900 17.47640120 4.41530965 6.71772100 13.64770429 #&gt; 51 52 53 54 55 #&gt; 10.15017522 9.87583433 -8.57122205 -3.45911127 -14.55826444 #&gt; 56 57 58 59 60 #&gt; -2.59825733 -9.01403984 -11.38818194 18.56702308 -1.75778320 #&gt; 61 62 63 64 65 #&gt; -6.62425092 7.23762327 2.27691947 3.53475127 -3.13081624 #&gt; 66 67 68 69 70 #&gt; -2.80966400 -8.90775281 -2.37148254 8.96207117 -0.17165983 #&gt; 71 72 73 74 75 #&gt; -6.62835284 6.84068452 -9.57892907 9.66837769 3.22127840 #&gt; 76 77 78 79 80 #&gt; 9.91951468 -7.95690100 1.69498729 -13.42842784 9.40544153 #&gt; 81 82 83 84 85 #&gt; -21.51471314 -0.17932205 8.29534673 15.48899827 0.44537734 #&gt; 86 87 88 89 90 #&gt; -4.92296395 -5.22518368 -7.27599025 5.82769287 0.38998126 #&gt; 91 92 93 94 95 #&gt; -7.25393258 -8.41427247 -2.76180581 -6.00048236 -7.88964854 #&gt; 96 97 98 99 100 #&gt; 13.37684695 9.48790667 -3.61552411 0.37448603 6.45366250 m.01$call #&gt; lm(formula = y ~ x) These output are all related to the model we have run, so they are kept organized by the list so they can be used for various further steps like comparing models or checking for model fit. A data frame is a bit more rigid that a list in that you cannot combine elements that do not have the same dimsions. # new.dataframe &lt;- data.frame( m.01$coefficients, m.01$residuals, m.01$call ) # # these will fail because the vectors have different lengths 3.2 Subsets The subset operators [ ] are one of the most common you will use in R. The primary rule of subsets is to use a data operator to create a logical selector vector, and use that to generate subsets. Any observation that corresponds to TRUE will be retained, any observation that corresponds to FALSE will be dropped. For vectors, you need to specify a single dimension. x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- factor( c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) x4 &lt;- c( &quot;treatment&quot;,&quot;control&quot;,&quot;treatment&quot;,&quot;control&quot; ) dat &lt;- data.frame( name=x2, sex=x3, treat=x4, strength=x1 ) these &lt;- x1 &gt; 140 # selector vector these #&gt; [1] TRUE TRUE FALSE TRUE x1[ these ] #&gt; [1] 167 185 142 x2[ these ] #&gt; [1] &quot;adam&quot; &quot;jamal&quot; &quot;sriti&quot; For data frames, you need two dimensions (rows and columns). The two dimensions are seperated by a comma, and if you leave one blank you will not drop anything. # dat[ row position , column position ] dat #&gt; name sex treat strength #&gt; 1 adam male treatment 167 #&gt; 2 jamal male control 185 #&gt; 3 linda female treatment 119 #&gt; 4 sriti female control 142 these &lt;- dat$treat == &quot;treatment&quot; dat[ these , ] # all data in the treatment group #&gt; name sex treat strength #&gt; 1 adam male treatment 167 #&gt; 3 linda female treatment 119 dat[ , c(&quot;name&quot;,&quot;sex&quot;) ] # select two columns of data #&gt; name sex #&gt; 1 adam male #&gt; 2 jamal male #&gt; 3 linda female #&gt; 4 sriti female # to keep a subset as a separate dataset dat.women &lt;- dat[ dat$sex == &quot;female&quot; , ] dat.women #&gt; name sex treat strength #&gt; 3 linda female treatment 119 #&gt; 4 sriti female control 142 Note the rules listed above about subsetting factors. After applying a subset, they will retain all of the original levels, even when they are not longer useful. You need to drop the unused levels if you would like them to be omitted from functions that use the factor levels for analysis. df &lt;- data.frame( letters=LETTERS[1:5], numbers=seq(1:5) ) levels( df$letters ) #&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; sub.df &lt;- df[ 1:3, ] sub.df$letters #&gt; [1] A B C #&gt; Levels: A B C D E levels( sub.df$letters ) #&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; droplevels( sub.df$letters ) #&gt; [1] A B C #&gt; Levels: A B C sub.df$letters &lt;- droplevels( sub.df$letters ) 3.3 Variable Transformations When we create a new variable from existing variables, it is called a ‘transformation’. This is very common in data science. Crime is measures by the number of assults per 100,000 people, for example (crime / pop). A batting average is the number of hits divided by the number of at bats. In R, mathematical operations are vectorized, which means that operations are performed on the entire vector all at once. This makes transformations fast and easy. x &lt;- 1:10 x + 5 #&gt; [1] 6 7 8 9 10 11 12 13 14 15 x * 5 #&gt; [1] 5 10 15 20 25 30 35 40 45 50 R uses a convention called “recycling”, which means that it will re-use elements of a vector if necessary. In the example below the x vector has 10 elements, but the y vector only has 5 elements. When we run out of y, we just start over from the beginning. This is powerful in some instances, but can be dangerous in others if you don’t realize that that it is happening. x &lt;- 1:10 y &lt;- 1:5 x + y #&gt; [1] 2 4 6 8 10 7 9 11 13 15 x * y #&gt; [1] 1 4 9 16 25 6 14 24 36 50 # the colors are recycled plot( 1:5, 1:5, col=c(&quot;red&quot;,&quot;blue&quot;), pch=19, cex=3 ) Here is an example of recycling gone wrong: x1 &lt;- c(167,185,119,142) x2 &lt;- c(&quot;adam&quot;,&quot;jamal&quot;,&quot;linda&quot;,&quot;sriti&quot;) x3 &lt;- c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) x4 &lt;- c( &quot;treatment&quot;,&quot;contro&quot;,&quot;treatment&quot;,&quot;control&quot; ) dat &lt;- data.frame( name=x2, sex=x3, treat=x4, strength=x1 ) # create a subset of data of all female study participants dat$sex == &quot;female&quot; #&gt; [1] FALSE FALSE TRUE TRUE these &lt;- dat$sex == &quot;female&quot; dat[ these, ] # correct subset #&gt; name sex treat strength #&gt; 3 linda female treatment 119 #&gt; 4 sriti female control 142 # same thing, but i mess is up - the female element is recycled in the overwrite dat$sex = &quot;female&quot; # whoops just over-wrote my data! should be double equal these &lt;- dat$sex == &quot;female&quot; dat[ these , ] #&gt; name sex treat strength #&gt; 1 adam female treatment 167 #&gt; 2 jamal female contro 185 #&gt; 3 linda female treatment 119 #&gt; 4 sriti female control 142 3.4 Missing Values: NA’s Missing values are coded differently in each data analysis program. SPSS uses a period, for example. In R, missing values are coded as “NA”. The important thing to note is that R wants to make sure you know there are missing values if you are conducting analysis. As a result, it will give you the answer of “NA” when you try to do math with a vector that includes a missing value. You have to ask it explicitly to ignore the missing value. x5 &lt;- c( 1, 2, 3, 4 ) x5 #&gt; [1] 1 2 3 4 sum( x5 ) #&gt; [1] 10 mean( x5 ) #&gt; [1] 2.5 x5 &lt;- c( 1, 2, NA, 4 ) x5 #&gt; [1] 1 2 NA 4 # should missing values be treated as zeros or dropped? sum( x5 ) #&gt; [1] NA mean( x5 ) #&gt; [1] NA sum( x5, na.rm=T ) # na.rm=T argument drops missing values #&gt; [1] 7 mean( x5, na.rm=T ) # na.rm=T argument drops missing values #&gt; [1] 2.333333 You cannot use the == operator to identify missing values in a dataset. There is a special is.na() function to locate all of the missing values in a vector. x5 #&gt; [1] 1 2 NA 4 x5 == NA # this does not do what you want #&gt; [1] NA NA NA NA is.na( x5 ) # much better #&gt; [1] FALSE FALSE TRUE FALSE ! is.na( x5 ) # if you want to create a selector vector to drop missing values #&gt; [1] TRUE TRUE FALSE TRUE x5[ ! is.na(x5) ] #&gt; [1] 1 2 4 x5[ is.na(x5) ] &lt;- 0 # replace missing values with zero 3.5 The ‘attach’ Function Never Use This! This is a convenient function for making variable names easily accessible, but it is problematic because of: scope conflicting variable names x &lt;- 1:5 y &lt;- 6:10 dat &lt;- data.frame(x,y) rm(x) rm(y) # I want to transform x in my dataset attach( dat ) 2*x #&gt; [1] 2 4 6 8 10 x &lt;- 2*x detach( dat ) x #&gt; [1] 2 4 6 8 10 dat # whoops! I didn&#39;t save my work in the dataset #&gt; x y #&gt; 1 1 6 #&gt; 2 2 7 #&gt; 3 3 8 #&gt; 4 4 9 #&gt; 5 5 10 You will see the attach() function used on occassion, and it is tempting because you can write the variable names directly. But in general, try to avoid the attach() function and don’t form bad habits by using it now because when your scripts become more complicated then can cause problmes. "],
["merging-data.html", "Chapter 4 Merging Data 4.1 Packages Used in This Chapter 4.2 Relational Databases 4.3 Set Theory 4.4 Merging Data 4.5 Non-Unique Observations in ID Variables 4.6 The %in% function 4.7 The Match Function", " Chapter 4 Merging Data 4.1 Packages Used in This Chapter library( pander ) library( dplyr ) library( maps ) 4.2 Relational Databases Modern databases are huge - think about the amount of information stored at Amazon in the history of each transation, the database where Google logs every single search from every person around the world, or Twitter’s database of all of the tweets (millions each day). When databases become large, flat spreadsheet style formats are not useful because they create a lot of redundant information, are large to store, and are not efficient to search. Large datasets are instead stored in relational databases - sets of tables that contain unique IDs that allow them to be joined when necessary. For example, consider a simple customer database. We don’t want to store customer info with our transactions because we would be repeating their name and street address every time they make a new purchase. As a result, we store customer information and transaction information separately. Customer Database CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE 178 Alvaro Jaurez 123 Park Ave 57701 934 Janette Johnson 456 Candy Ln 57701 269 Latisha Shane 1600 Penn Ave 20500 Transactions Database CUSTOMER.ID PRODUCT PRICE 178 video 5.38 178 shovel 12 269 book 3.99 269 purse 8 934 mirror 7.64 If we want to make the information actionable then we need to combine these datasets. For example, perhaps we want to know the average purchase amount from an individual in the 57701 zip code. We cannot answer that question with either dataset since the zip code is in one dataset, and the price is in another. We need to merge the data. merge( customer.info, purchases ) #&gt; CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE #&gt; 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 #&gt; 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 #&gt; 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 #&gt; 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 #&gt; 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 full.dat &lt;- merge( customer.info, purchases ) full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] #&gt; [1] 5.38 12.00 7.64 mean( full.dat$PRICE[ full.dat$ZIP.CODE == &quot;57701&quot; ] ) #&gt; [1] 8.34 In reality, each purchase would have a purchase ID that is linked to shipping addresses, customer complaints, seller ratings, etc. Each seller would have their own data table with info. Each purchase would be tied to a payment type, which has its own data table. The system gets quite complex, which is why it is important to pay attention to the details of putting the data back together again. We will cover a few details of data merges that will help you avoid common and very subtle mistakes that can lead to incorrect inferences. 4.3 Set Theory In order to merge data correctly you need to understand some very basic principles of set theory. 4.3.1 Set Theory Functions Let’s assume we have two sets: set1=[A,B], set2=[B,C]. Each element in this set represents a group of observations that occurs in the dataset. So B represents people that occur in both datasets, A represents people that occur only in the first dataset, and C represents people that only occur in the second dataset. We can then describe membership through three operations: Operation Description union The universe of all elements across all both sets: [A,B,C] intersection The elements shared by both sets: [B] difference The elements in my first set, not in my second [A] or [C] Let’s see how this might work in practice with an example of members of a study: name &lt;- c(&quot;frank&quot;,&quot;wanda&quot;,&quot;sanjay&quot;,&quot;nancy&quot;) group &lt;- c(&quot;treat&quot;,&quot;treat&quot;,&quot;control&quot;,&quot;control&quot;) gender &lt;- c(&quot;male&quot;,&quot;female&quot;,&quot;male&quot;,&quot;female&quot;) data.frame( name, group, gender ) %&gt;% pander name group gender frank treat male wanda treat female sanjay control male nancy control female For this example let’s define set 1 as the treatment group, and set 2 as all women in the study. Note that set membership is always defined as binary (you are in the set or out), but it can include multiple criteria (the set of animals can contains cats, dogs, and mice). treated &lt;- name[ group == &quot;treat&quot; ] treated #&gt; [1] &quot;frank&quot; &quot;wanda&quot; females &lt;- name[ gender == &quot;female&quot; ] females #&gt; [1] &quot;wanda&quot; &quot;nancy&quot; Now we can specify group belonging using some convenient set theory functions: union(), setdiff(), and intersect(). union( treated, females ) #&gt; [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; intersect( treated, females ) #&gt; [1] &quot;wanda&quot; setdiff( treated, females ) #&gt; [1] &quot;frank&quot; setdiff( females, treated ) #&gt; [1] &quot;nancy&quot; It is very important to note that union() and intersect() are symmetric functions, meaning intersect(x,y) will give you the same result as intersect(y,x). The setdiff() function is not symmetric, however. 4.3.2 Set Theory Using Logical Operators Typically you will define your groups using logical operators, which perform the exact same funciton as set theory functions but are a little more expressive and flexible. Let’s use the same example above where x=“treatment” and y=“female”, then consider these cases: Who belongs in each group? # x name[ group == &quot;treat&quot; ] #&gt; [1] &quot;frank&quot; &quot;wanda&quot; # x &amp; y name[ group == &quot;treat&quot; &amp; gender == &quot;female&quot; ] #&gt; [1] &quot;wanda&quot; # x &amp; ! y name[ group == &quot;treat&quot; &amp; gender != &quot;female&quot; ] #&gt; [1] &quot;frank&quot; # x | y name[ group == &quot;treat&quot; | gender == &quot;female&quot; ] #&gt; [1] &quot;frank&quot; &quot;wanda&quot; &quot;nancy&quot; Who belongs in these groups? !x &amp; !y x &amp; ! ( x &amp; y ) ( x | y ) &amp; ! ( x &amp; y ) 4.4 Merging Data The Merge Function The merge function joins two datasets. The function requires two datasets as the arguments, and they need to share a unique ID variable. Recall the example from above: merge( customer.info, purchases ) #&gt; CUSTOMER.ID FIRST.NAME LAST.NAME ADDRESS ZIP.CODE PRODUCT PRICE #&gt; 1 178 Alvaro Jaurez 123 Park Ave 57701 video 5.38 #&gt; 2 178 Alvaro Jaurez 123 Park Ave 57701 shovel 12.00 #&gt; 3 269 Latisha Shane 1600 Penn Ave 20500 book 3.99 #&gt; 4 269 Latisha Shane 1600 Penn Ave 20500 purse 8.00 #&gt; 5 934 Janette Johnson 456 Candy Ln 57701 mirror 7.64 The important thing to keep in mind is that the default merge operation uses the intersection of the two datasets. It will drop all elements that don’t occur in both datasets. We may want to fine-tune this as to not lose valuable data and potentially bias our analysis. As an example, no illegal immigrants will have social security numbers, so if you are merging using the SSN, you will drop this group from the data, which could impact your results. With a little help from the set theory examples above, we can think about which portions of the data we wish to drop and which portions we wish to keep. Argument Usage all=F DEFAULT - new dataset contains intersection of X and Y (B only) all=T New dataset contains union of X and Y (A, B &amp; C) all.x=T New dataset contains A and B, not C all.y=T New dataset contains B and C, not A Here is some demonstrations with examples adapted from the R help file. authors #&gt; surname nationality deceased #&gt; 1 Tukey US yes #&gt; 2 Tierney US no #&gt; 3 Ripley UK no #&gt; 4 McNeil Australia no #&gt; 5 Shakespeare England yes books #&gt; name title #&gt; 1 Tukey Exploratory Data Analysis #&gt; 2 Venables Modern Applied Statistics #&gt; 3 Ripley Spatial Statistics #&gt; 4 Ripley Stochastic Simulation #&gt; 5 McNeil Interactive Data Analysis #&gt; 6 R Core Team An Introduction to R # adding books to the author bios dataset ( set B only ) merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;) #&gt; surname nationality deceased title #&gt; 1 McNeil Australia no Interactive Data Analysis #&gt; 2 Ripley UK no Spatial Statistics #&gt; 3 Ripley UK no Stochastic Simulation #&gt; 4 Tukey US yes Exploratory Data Analysis # adding author bios to the books dataset ( set B only ) merge(books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;) #&gt; name title nationality deceased #&gt; 1 McNeil Interactive Data Analysis Australia no #&gt; 2 Ripley Spatial Statistics UK no #&gt; 3 Ripley Stochastic Simulation UK no #&gt; 4 Tukey Exploratory Data Analysis US yes # keep books without author bios, lose authors without books ( sets A and B ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.x=T ) #&gt; name title nationality deceased #&gt; 1 McNeil Interactive Data Analysis Australia no #&gt; 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; #&gt; 3 Ripley Spatial Statistics UK no #&gt; 4 Ripley Stochastic Simulation UK no #&gt; 5 Tukey Exploratory Data Analysis US yes #&gt; 6 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; # keep authors without book listed, lose books without author bios ( sets B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all.y=T ) #&gt; name title nationality deceased #&gt; 1 McNeil Interactive Data Analysis Australia no #&gt; 2 Ripley Spatial Statistics UK no #&gt; 3 Ripley Stochastic Simulation UK no #&gt; 4 Shakespeare &lt;NA&gt; England yes #&gt; 5 Tierney &lt;NA&gt; US no #&gt; 6 Tukey Exploratory Data Analysis US yes # dont&#39; throw out any data ( sets A and B and C ) merge( books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;, all=T ) #&gt; name title nationality deceased #&gt; 1 McNeil Interactive Data Analysis Australia no #&gt; 2 R Core Team An Introduction to R &lt;NA&gt; &lt;NA&gt; #&gt; 3 Ripley Spatial Statistics UK no #&gt; 4 Ripley Stochastic Simulation UK no #&gt; 5 Shakespeare &lt;NA&gt; England yes #&gt; 6 Tierney &lt;NA&gt; US no #&gt; 7 Tukey Exploratory Data Analysis US yes #&gt; 8 Venables Modern Applied Statistics &lt;NA&gt; &lt;NA&gt; Also note that the order of your datasets in the argument list will impact the inclusion or exclusion of elements. merge( x, y, all=F ) EQUALS merge( y, x, all=F ) merge( x, y, all.x=T ) DOES NOT EQUAL merge( y, x, all.x=T ) 4.4.1 The by.x and by.y Arguments When you use the default merge() function without specifying the variables to merge upon, the function will check for common variable names across the two datasets. If there are multiple, it will join the shared variables to create a new unique key. This might be problematic if that was not the intent. Take the example of combining fielding and salary data in the Lahman package. If we are not explicit about the merge variable, we may get odd results. Note that they two datasets share four ID variables. library( Lahman ) data( Fielding ) data( Salaries ) intersect( names(Fielding), names(Salaries) ) #&gt; [1] &quot;playerID&quot; &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; # merge id int &lt;- intersect( names(Fielding), names(Salaries) ) paste( int[1],int[2],int[3],int[4], sep=&quot;.&quot; ) #&gt; [1] &quot;playerID.yearID.teamID.lgID&quot; To avoid problems, be explicit using the by.x and by.x arguments to control which variable is used for the merge. head( merge( Salaries, Fielding ) ) #&gt; yearID teamID lgID playerID salary stint POS G GS InnOuts PO A E DP #&gt; 1 1985 ATL NL barkele01 870000 1 P 20 18 221 2 9 1 0 #&gt; 2 1985 ATL NL bedrost01 550000 1 P 37 37 620 13 23 4 3 #&gt; 3 1985 ATL NL benedbr01 545000 1 C 70 67 1698 314 35 4 1 #&gt; 4 1985 ATL NL campri01 633333 1 P 66 2 383 7 13 4 3 #&gt; 5 1985 ATL NL ceronri01 625000 1 C 91 76 2097 384 48 6 4 #&gt; 6 1985 ATL NL chambch01 800000 1 1B 39 27 814 299 25 1 31 #&gt; PB WP SB CS ZR #&gt; 1 NA NA NA NA NA #&gt; 2 NA NA NA NA NA #&gt; 3 1 9 65 24 1 #&gt; 4 NA NA NA NA NA #&gt; 5 6 20 69 29 1 #&gt; 6 NA NA NA NA NA head( merge( Salaries, Fielding, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) ) #&gt; playerID yearID.x teamID.x lgID.x salary yearID.y stint teamID.y #&gt; 1 aardsda01 2010 SEA AL 2750000 2009 1 SEA #&gt; 2 aardsda01 2010 SEA AL 2750000 2015 1 ATL #&gt; 3 aardsda01 2010 SEA AL 2750000 2006 1 CHN #&gt; 4 aardsda01 2010 SEA AL 2750000 2008 1 BOS #&gt; 5 aardsda01 2010 SEA AL 2750000 2013 1 NYN #&gt; 6 aardsda01 2010 SEA AL 2750000 2012 1 NYA #&gt; lgID.y POS G GS InnOuts PO A E DP PB WP SB CS ZR #&gt; 1 AL P 73 0 214 2 5 0 1 NA NA NA NA NA #&gt; 2 NL P 33 0 92 0 1 1 0 NA NA NA NA NA #&gt; 3 NL P 45 0 159 1 5 0 1 NA NA NA NA NA #&gt; 4 AL P 47 0 146 3 6 0 0 NA NA NA NA NA #&gt; 5 NL P 43 0 119 1 5 0 0 NA NA NA NA NA #&gt; 6 AL P 1 0 3 0 0 0 0 NA NA NA NA NA 4.5 Non-Unique Observations in ID Variables In some rare instances, you will need to merge to datasets that have non-singular elements in the unique key ID variables, meaning each observation / individual appears more than one time in the data. Note that in this case, for each occurance of an observation / individual in your X dataset, you will merge once with each occurance of the same observation / individual in the Y dataset. The result will be a multiplicative expansion of the size of your dataset. For example, if John appears on four separate rows of X, and three seperate rows of Y, the new dataset will contain 12 rows of John (4 x 3 = 12). dataset X contains four separate instances of an individual [ X1, X2, X3, X4 ] dataset Y contains three separate instances of an individual [ Y1, Y2, Y3 ] After the merge we have one row for each pair: X1-Y1 X1-Y2 X1-Y3 X2-Y1 X2-Y2 X2-Y3 X3-Y1 X3-Y2 X3-Y3 X4-Y1 X4-Y2 X4-Y3 For example, perhaps a sales company has a database that keeps track of biographical data, and sales performance. Perhaps we want to see if there is peak age for sales performance. We need to merge these datasets. bio &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), age=c(43,44,45) ) performance &lt;- data.frame( name=c(&quot;John&quot;,&quot;John&quot;,&quot;John&quot;), year=c(2000,2001,2002), sales=c(&quot;15k&quot;,&quot;20k&quot;,&quot;17k&quot;) ) # correct merge merge( bio, performance, by.x=c(&quot;name&quot;,&quot;year&quot;), by.y=c(&quot;name&quot;,&quot;year&quot;) ) #&gt; name year age sales #&gt; 1 John 2000 43 15k #&gt; 2 John 2001 44 20k #&gt; 3 John 2002 45 17k # incorrect merge merge( bio, performance, by.x=c(&quot;name&quot;), by.y=c(&quot;name&quot;) ) #&gt; name year.x age year.y sales #&gt; 1 John 2000 43 2000 15k #&gt; 2 John 2000 43 2001 20k #&gt; 3 John 2000 43 2002 17k #&gt; 4 John 2001 44 2000 15k #&gt; 5 John 2001 44 2001 20k #&gt; 6 John 2001 44 2002 17k #&gt; 7 John 2002 45 2000 15k #&gt; 8 John 2002 45 2001 20k #&gt; 9 John 2002 45 2002 17k It is good practice to check the size (number of rows) of your dataset before and after a merge. If it has expanded, chances are you either used the wrong unique IDs, or your dataset contains duplicates. 4.5.1 Example of Incorrect Merge Here is a tangible example using the Lahman baseball dataset. Perhaps we want to examine the relationship between fielding position and salary. The Fielding dataset contains fielding position information, and the Salaries dataset contains salary information. We can merge these two datasets using the playerID field. If we are not thoughtful about this, however, we will end up causing problems. Let’s look at an example using Kirby Pucket. kirby.fielding &lt;- Fielding[ Fielding$playerID == &quot;puckeki01&quot; , ] head( kirby.fielding ) #&gt; playerID yearID stint teamID lgID POS G GS InnOuts PO A E DP #&gt; 83848 puckeki01 1984 1 MIN AL OF 128 128 3377 438 16 3 4 #&gt; 85157 puckeki01 1985 1 MIN AL OF 161 160 4213 465 19 8 5 #&gt; 86489 puckeki01 1986 1 MIN AL OF 160 157 4155 429 8 6 3 #&gt; 87896 puckeki01 1987 1 MIN AL OF 147 147 3820 341 8 5 2 #&gt; 89264 puckeki01 1988 1 MIN AL OF 158 157 4049 450 12 3 4 #&gt; 90685 puckeki01 1989 1 MIN AL OF 157 154 3985 438 13 4 3 #&gt; PB WP SB CS ZR #&gt; 83848 NA NA NA NA NA #&gt; 85157 NA NA NA NA NA #&gt; 86489 NA NA NA NA NA #&gt; 87896 NA NA NA NA NA #&gt; 89264 NA NA NA NA NA #&gt; 90685 NA NA NA NA NA nrow( kirby.fielding ) #&gt; [1] 21 kirby.salary &lt;- Salaries[ Salaries$playerID == &quot;puckeki01&quot; , ] head( kirby.salary ) #&gt; yearID teamID lgID playerID salary #&gt; 280 1985 MIN AL puckeki01 130000 #&gt; 917 1986 MIN AL puckeki01 255000 #&gt; 1610 1987 MIN AL puckeki01 465000 #&gt; 2244 1988 MIN AL puckeki01 1090000 #&gt; 2922 1989 MIN AL puckeki01 2000000 #&gt; 3717 1990 MIN AL puckeki01 2816667 nrow( kirby.salary ) #&gt; [1] 13 kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=&quot;playerID&quot;, by.y=&quot;playerID&quot; ) head( select( kirby.field.salary, yearID.x, yearID.y, POS, G, GS, salary ) ) #&gt; yearID.x yearID.y POS G GS salary #&gt; 1 1984 1985 OF 128 128 130000 #&gt; 2 1984 1986 OF 128 128 255000 #&gt; 3 1984 1987 OF 128 128 465000 #&gt; 4 1984 1988 OF 128 128 1090000 #&gt; 5 1984 1989 OF 128 128 2000000 #&gt; 6 1984 1990 OF 128 128 2816667 nrow( kirby.field.salary ) #&gt; [1] 273 21*13 #&gt; [1] 273 What we have done here is taken each year of fielding data, and matched it to every year of salary data. We can see that we have 21 fielding observations and 13 years of salary data, so our resulting dataset is 273 observation pairs. This merge also makes it difficult to answer the question of the relationship between fielding position and salary if players change positions over time. The correct merge in this case would be a merge on a playerID-yearID pair. We can create a unique key by combining playerID and yearID using paste(): head( paste( kirby.fielding$playerID, kirby.fielding$yearID, sep=&quot;.&quot;) ) #&gt; [1] &quot;puckeki01.1984&quot; &quot;puckeki01.1985&quot; &quot;puckeki01.1986&quot; &quot;puckeki01.1987&quot; #&gt; [5] &quot;puckeki01.1988&quot; &quot;puckeki01.1989&quot; But there is a simple solution as the merge function also allows for multiple variables to be used for a merge() command. kirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=c(&quot;playerID&quot;,&quot;yearID&quot;), by.y=c(&quot;playerID&quot;,&quot;yearID&quot;) ) nrow( kirby.field.salary ) #&gt; [1] 20 4.6 The %in% function Since we are talking about intersections and matches, I want to briefly introduce the %in% function. It is a combination of the two. The intersect() function returns a list of unique matches between two vectors. data(Salaries) data(Fielding) intersect( names(Salaries), names(Fielding) ) #&gt; [1] &quot;yearID&quot; &quot;teamID&quot; &quot;lgID&quot; &quot;playerID&quot; The match() function returns the position of matched elements. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) match( x, y ) #&gt; [1] 3 1 NA 1 The %in% function returns a logical vector, where TRUE signifies that the element in y also occurs in x. In other words, does a specific element in y belong to the intersection of x,y. This is very useful for creating subsets of data that belong to both sets. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;B&quot;,&quot;F&quot;,&quot;B&quot;) y %in% x # does each element of y occur anywhere in x? #&gt; [1] TRUE FALSE TRUE TRUE FALSE TRUE y[ y %in% x] # keep only data that occurs in both #&gt; [1] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; 4.7 The Match Function Often times we do not need to merge data, we may just need sort data in one dataset so that it matches the order of another dataset. This is accomplished using the match() function. Note that we can rearrange the order of a dataset by referencing the desired position. x &lt;- c(&quot;Second&quot;,&quot;Third&quot;,&quot;First&quot;) x #&gt; [1] &quot;Second&quot; &quot;Third&quot; &quot;First&quot; x[ c(3,1,2) ] #&gt; [1] &quot;First&quot; &quot;Second&quot; &quot;Third&quot; The match() function returns the positions of matches of its first vector to the second vector listed in the arguments. Or in other words, the order that vector 2 would need to follow to match vector 1. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;) cbind( x, y ) #&gt; x y #&gt; [1,] &quot;A&quot; &quot;B&quot; #&gt; [2,] &quot;B&quot; &quot;D&quot; #&gt; [3,] &quot;C&quot; &quot;A&quot; match( x, y ) #&gt; [1] 3 1 NA match( y, x) # not a symmetric operation! #&gt; [1] 2 NA 1 # In the y vector: # # [3]=A # [1]=B # [NA]=D (no match) order.y &lt;- match( x, y ) y[ order.y ] #&gt; [1] &quot;A&quot; &quot;B&quot; NA We can see that match() returns the correct order to put y in so that it matches the order of x. In the re-ordered vector, the first element is the original third element A, the second element is the original first element B, and there is no third element because D did not match anything in x. Note the order of arguments in the function: match( data I want to match to , data I need to re-order ) We can use this position information to re-order y as follows: x &lt;- sample( LETTERS[1:15], size=10 ) y &lt;- sample( LETTERS[1:15], size=10 ) cbind( x, y ) #&gt; x y #&gt; [1,] &quot;B&quot; &quot;F&quot; #&gt; [2,] &quot;C&quot; &quot;G&quot; #&gt; [3,] &quot;L&quot; &quot;O&quot; #&gt; [4,] &quot;F&quot; &quot;I&quot; #&gt; [5,] &quot;O&quot; &quot;J&quot; #&gt; [6,] &quot;N&quot; &quot;B&quot; #&gt; [7,] &quot;D&quot; &quot;H&quot; #&gt; [8,] &quot;E&quot; &quot;E&quot; #&gt; [9,] &quot;M&quot; &quot;M&quot; #&gt; [10,] &quot;I&quot; &quot;A&quot; order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) #&gt; x y.new #&gt; [1,] &quot;B&quot; &quot;B&quot; #&gt; [2,] &quot;C&quot; NA #&gt; [3,] &quot;L&quot; NA #&gt; [4,] &quot;F&quot; &quot;F&quot; #&gt; [5,] &quot;O&quot; &quot;O&quot; #&gt; [6,] &quot;N&quot; NA #&gt; [7,] &quot;D&quot; NA #&gt; [8,] &quot;E&quot; &quot;E&quot; #&gt; [9,] &quot;M&quot; &quot;M&quot; #&gt; [10,] &quot;I&quot; &quot;I&quot; # Note the result if you confuse the order or arguments order.y &lt;- match( y, x ) y.new &lt;- y[ order.y ] cbind( x, y.new ) #&gt; x y.new #&gt; [1,] &quot;B&quot; &quot;I&quot; #&gt; [2,] &quot;C&quot; NA #&gt; [3,] &quot;L&quot; &quot;J&quot; #&gt; [4,] &quot;F&quot; &quot;A&quot; #&gt; [5,] &quot;O&quot; NA #&gt; [6,] &quot;N&quot; &quot;F&quot; #&gt; [7,] &quot;D&quot; NA #&gt; [8,] &quot;E&quot; &quot;E&quot; #&gt; [9,] &quot;M&quot; &quot;M&quot; #&gt; [10,] &quot;I&quot; NA This comes in handy when we are matching information between two tables. For example, in GIS the map regions follow a specific order but your data does not. Create a color scheme for levels of your data, and then re-order the colors so they match the correct region on the map. In this example, we will look at unemployment levels by county. library( maps ) data( county.fips ) data( unemp ) map( database=&quot;county&quot; ) # assign a color to each level of unemployment, red = high, gray = medium, blue = low color.function &lt;- colorRampPalette( c(&quot;steelblue&quot;, &quot;gray70&quot;, &quot;firebrick&quot;) ) color.vector &lt;- cut( rank(unemp$unemp), breaks=7, labels=color.function( 7 ) ) color.vector &lt;- as.character( color.vector ) head( color.vector ) #&gt; [1] &quot;#B28282&quot; &quot;#B28282&quot; &quot;#B22222&quot; &quot;#B25252&quot; &quot;#B28282&quot; &quot;#B22222&quot; # doesn&#39;t look quite right map( database=&quot;county&quot;, col=color.vector, fill=T, lty=0 ) # what went wrong here? # our unemployment data (and thus the color vector) follows a different order cbind( map.id=county.fips$fips, data.id=unemp$fips, color.vector )[ 2500:2510 , ] #&gt; map.id data.id color.vector #&gt; [1,] &quot;48011&quot; &quot;47149&quot; &quot;#B28282&quot; #&gt; [2,] &quot;48013&quot; &quot;47151&quot; &quot;#B22222&quot; #&gt; [3,] &quot;48015&quot; &quot;47153&quot; &quot;#B22222&quot; #&gt; [4,] &quot;48017&quot; &quot;47155&quot; &quot;#B28282&quot; #&gt; [5,] &quot;48019&quot; &quot;47157&quot; &quot;#B28282&quot; #&gt; [6,] &quot;48021&quot; &quot;47159&quot; &quot;#B22222&quot; #&gt; [7,] &quot;48023&quot; &quot;47161&quot; &quot;#B25252&quot; #&gt; [8,] &quot;48025&quot; &quot;47163&quot; &quot;#B3B3B3&quot; #&gt; [9,] &quot;48027&quot; &quot;47165&quot; &quot;#B28282&quot; #&gt; [10,] &quot;48029&quot; &quot;47167&quot; &quot;#B25252&quot; #&gt; [11,] &quot;48031&quot; &quot;47169&quot; &quot;#B25252&quot; # place the color vector in the correct order this.order &lt;- match( county.fips$fips, unemp$fips ) color.vec.ordered &lt;- color.vector[ this.order ] # colors now match their correct counties map( database=&quot;county&quot;, col=color.vec.ordered, fill=T, lty=0 ) title( main=&quot;Unemployment Levels by County in 2009&quot;) Note that elements can be recycled from your y vector: x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;) y &lt;- c(&quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;F&quot;) cbind( x, y ) #&gt; x y #&gt; [1,] &quot;A&quot; &quot;B&quot; #&gt; [2,] &quot;B&quot; &quot;D&quot; #&gt; [3,] &quot;C&quot; &quot;A&quot; #&gt; [4,] &quot;B&quot; &quot;F&quot; match( x, y ) #&gt; [1] 3 1 NA 1 order.y &lt;- match( x, y ) y.new &lt;- y[ order.y ] cbind( x, y.new ) #&gt; x y.new #&gt; [1,] &quot;A&quot; &quot;A&quot; #&gt; [2,] &quot;B&quot; &quot;B&quot; #&gt; [3,] &quot;C&quot; NA #&gt; [4,] &quot;B&quot; &quot;B&quot; "],
["analysis-with-groups-in-r.html", "Chapter 5 Analysis with Groups in R 5.1 Group Structure 5.2 Analysis by Group", " Chapter 5 Analysis with Groups in R 5.0.1 Packages Used in this Chapter library( pander ) library( dplyr ) library( tidyr ) library( reshape2 ) library( scales ) library( ggplot2 ) library( Lahman ) 5.0.2 Hypothetical Experimental Data We will demonstrate some functions using this hypothetical dataset: head( d ) %&gt;% pander id race blood.type gender age study.group speed 1 white A female 40 treatment 677.1 2 white A female 37 treatment 616.8 3 white B female 64 treatment 503.6 4 asian A male 50 treatment 555.5 5 black A female 32 treatment 671 6 black A female 28 treatment 644.2 5.1 Group Structure The two most important skills as you first learn a data programming language are: Translating English phrases into computer code using logical statements Organizing your data into groups This lecture focuses on efficienty splitting your data into groups, and then analyzing your data by group. 5.1.1 What Are Groups? A group represents a set of elements with identical characteristics - mice all belong to one group and elephants belong to another. Easy enough, right? In data analysis, it is a little more complicated because a group is defined by a set of features. Each group still represents a set of elements with identical characteristics, but when we have multiple features there is a unique group for each combination of features. The simple way to think about this is that the cross-tab of features generates a grid (table), and each cell represents a unique group: We might be interested in simple groups (treatment cases versus control cases), or complex groups (does the treatment effect women and men differently?). In previous lectures you have learned to identify a group with a logical statement, and analyze that group discretely. mean( speed[ study.group == &quot;treatment&quot; &amp; gender==&quot;female&quot; ] ) #&gt; [1] 618.9414 In this lecture you will learn to define a group structure, then analyze all of your data using that structure. tapply( speed, INDEX = list( study.group, gender ), FUN = mean ) female male control 468.3 361.4 treatment 618.9 519.8 5.1.2 Main Take-Away R has been designed to do efficient data analysis by defining a group structure, then quickly applying a function to all unique members. The base R packages do this with a set of functions in the apply() family. The tapply() function allows you to specify an outcome to analyze and a group, then ask for results from a function. tapply( X=speed, INDEX=list( study.group, gender ), FUN=mean ) female male control 468.3 361.4 treatment 618.9 519.8 The dplyr package makes this process easier using some simple verbs and the “pipe” operator. dat %&gt;% group_by( study.group, gender ) %&gt;% summarize( ave.speed = mean(speed) ) study.group gender ave.speed control male 361.4 control female 468.3 treatment male 519.8 treatment female 618.9 5.1.3 Example Let’s think about a study looking at reading speed. The treatment is a workshop that teaches some speed-reading techniques. In this study we have data on: gender (male,female) race (black,white,asian) blood.type (A,B) age (from 18 to 93) Examining descriptive statistics we can see that reading speed varies by gender and the treatment group, but not by race or blood type: The question is, how many unique groups can we create with these four factors? Each individual factor contains a small number of levels (only 2 or 3 in this case), which makes the group structure look deceptively simple at first glance. When we start to examine combinations of factors we see that group structure can get complicated pretty quickly. If we look at gender alone, we have two levels: male and female. So we have two groups. If we look at our study groups alone we have two groups: treatment and control. If we look at gender and the study groups together, we now have a 2 x 2 grid, or four unique groups. If the race factor has three levels, how many unique groups will we have considering the study design, gender, and race together? We can calculate the size of the grid by multiplying number of levels for each factor. We see here we have 12 unique groups: nlevels( gender ) * nlevels( study.group ) * nlevels( race ) #&gt; [1] 12 If we add blood type, a factor with two levels (A and B), we now have 24 unique groups: p + facet_grid( race + study.group ~ gender + blood.type) What about age? It is a continuous variable, so it’s a little more tricky. We can certainly analyze the relationship between age and speed using correlation tools. plot( age, speed, bty=&quot;n&quot;, main=&quot;Age&quot; ) But we can also incorporate this independent variable into a group structure. We can treat each distinct age as a separate group. The ages in this study range from 18 to 93, so we have 65 distinct ages represented. plot( factor(age), speed, las=2, frame.plot=F, outline=F, main=&quot;Age&quot;, xaxt=&quot;n&quot; ) If we think about the overall group structure, then, we have unique groups defined by gender, race, blood type, and study design, and another 65 age groups. So in total we now have 24 x 65 = 1,560 groups! That is getting complicated. This group design is problematic for two reasons. From a pragmatic standpoint, we can’t report results from 1,500 groups in a table. From a more substantive perspective, we although we have 1,500 distinct cells in our grid, many may not include observations that represent the unique combination of all factors. So this group design is not very practical. A similar problem arises if our data includes time. If our data includes the time of events recorded by hours, days of the week, months, and years, we can have generate complicated group structures if we try to analyze every unique combination. We can simplify our analysis by thinking about age ranges instead of ages, or in other words by binning our continuous data. If we split it into five-year ranges, for example, we have gone from 65 distinct ages to 12 distinct age groups. age.group &lt;- cut( age, breaks=seq(from=20,to=80,by=5), labels=paste( seq(from=20,to=75,by=5), &quot;to&quot;, seq(from=25,to=80,by=5) ) ) group.structure &lt;- formula( speed ~ age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group&quot; ) We have now simplified our analysis from 1,560 to 288 possible groups. Combinations of groups will also be easier: group.structure &lt;- formula( speed ~ gender * age.group ) boxplot( group.structure, las=2, frame.plot=F, outline=F, main=&quot;Age Group by Gender&quot;, col=c(&quot;firebrick&quot;,&quot;steelblue&quot;), xaxt=&quot;n&quot; ) 5.2 Analysis by Group Let’s demonstrate some analysis of groups using the Lahman package and some dplyr verbs. Let’s do some analysis of player salaries (Salaries dataset), and start with a simple group structure - teams in the National League and time. Which team has the highest average player salary? Which team has the most players paid over $5 million a season? Which team has raised it’s pay the most over the past decade? Let’s start by thinking about group structure. We have teams, and we have seasons. Teams is stored as a factor, and seasons as a numeric value, so we can consider group for each by counting levels and unique values: nlevels( Salaries$teamID ) #&gt; [1] 46 length( unique( Salaries$yearID ) ) #&gt; [1] 32 So we can potentially calculate 32 x 46 = 1,472 average player salaries. 5.2.1 Highest Ave Player Salary For our first question, we will select only teams from the National League. Let’s use the most recent year of data to calculate average pay. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID) %&gt;% summarize( Ave_Salary = mean(salary) ) #&gt; # A tibble: 15 x 2 #&gt; teamID Ave_Salary #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 ARI 3363041. #&gt; 2 ATL 2362010. #&gt; 3 CHC 5312678. #&gt; 4 CIN 3066899. #&gt; 5 COL 3413487 #&gt; 6 LAD 6322525. #&gt; # ... with 9 more rows Since the salaries are large, they are a little hard to read. Let’s clean up the table a bit. Salaries %&gt;% filter( lgID == &quot;NL&quot;, yearID == 2016 ) %&gt;% group_by( teamID ) %&gt;% summarize( Ave_Salary=dollar( mean(salary,na.rm=T) ) ) %&gt;% arrange( desc(Ave_Salary) ) %&gt;% pander() teamID Ave_Salary SFG $6,890,151 LAD $6,322,525 WSN $5,448,179 CHC $5,312,678 NYM $4,958,857 STL $4,614,629 SDP $3,756,475 PIT $3,706,387 COL $3,413,487 ARI $3,363,041 CIN $3,066,899 MIA $2,761,222 ATL $2,362,010 MIL $2,292,508 PHI $2,033,793 5.2.2 Most Players Paid Over $5 Million This question requires you to utilize a logical statement in order to translate from the question to code. We need to inspect each salary, determine whether it is over the $5m threshold, then count all of the cases. The operation will look something like this: sum( Salaries$salary &gt; 5000000 ) #&gt; [1] 3175 It gets a little trickier when we want to do the operation simultaneously across groups. Our team group structure is already defined, so let’s define our logical vector and count cases that match: dat.NL &lt;- filter( Salaries, yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() gt.5m &lt;- dat.NL$salary &gt; 5000000 table( dat.NL$teamID, gt.5m ) #&gt; gt.5m #&gt; FALSE TRUE #&gt; ARI 23 3 #&gt; ATL 21 6 #&gt; CHN 19 8 #&gt; CIN 21 5 #&gt; COL 23 6 #&gt; FLO 23 4 #&gt; HOU 24 4 #&gt; LAN 20 7 #&gt; MIL 25 4 #&gt; NYN 19 9 #&gt; PHI 18 10 #&gt; PIT 27 0 #&gt; SDN 25 1 #&gt; SFN 21 7 #&gt; SLN 19 6 #&gt; WAS 26 4 This solution works, but the table provides too much information. We can use dply to simplify and format the table nicely for our report: Salaries %&gt;% filter( yearID == 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID ) %&gt;% summarise( gt_five_million = sum( salary &gt; 5000000 ) ) %&gt;% arrange( desc(gt_five_million) ) %&gt;% pander teamID gt_five_million PHI 10 NYN 9 CHN 8 LAN 7 SFN 7 ATL 6 COL 6 SLN 6 CIN 5 FLO 4 HOU 4 MIL 4 WAS 4 ARI 3 SDN 1 PIT 0 5.2.3 Fielding Positions Which fielding position is the highest paid? merge( Salaries, Fielding ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( POS ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander POS Mean_Salary 1B $5,570,032 2B $3,162,075 3B $3,579,088 C $2,521,903 OF $3,546,115 P $3,401,676 SS $2,510,833 5.2.4 Country of Birth Which country has produced the highest paid baseball players? merge( Salaries, Master ) %&gt;% filter( yearID == 2016 ) %&gt;% group_by( birthCountry ) %&gt;% summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% pander birthCountry Mean_Salary Aruba $650,000 Australia $523,400 Brazil $1,548,792 CAN $7,854,167 Colombia $3,125,289 Cuba $5,532,484 Curacao $5,724,167 D.R. $5,102,318 Germany $511,500 Japan $8,247,012 Mexico $4,617,038 Netherlands $2,425,000 Nicaragua $2,375,000 P.R. $3,241,378 Panama $2,946,550 Saudi Arabia $522,500 South Korea $5,326,190 Taiwan $6,750,000 USA $4,189,640 V.I. $507,500 Venezuela $4,521,051 5.2.5 Pay Raises To examine pay raises, we will now use more than one year of data. Since the question asks about pay raises over the past decade, we will filter the last ten years of data. And how since we are looking at patterns over teams and over time, we need to define a group structure with two variables: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean= dollar(mean(salary)) ) %&gt;% head( 20 ) %&gt;% pander teamID yearID mean ARI 2007 $1,859,555 ARI 2008 $2,364,383 ARI 2009 $2,812,141 ARI 2010 $2,335,314 ARI 2011 $1,986,660 ARI 2012 $2,733,512 ARI 2013 $3,004,400 ARI 2014 $3,763,904 ARI 2015 $2,034,250 ARI 2016 $3,363,041 ATL 2007 $3,117,530 ATL 2008 $3,412,189 ATL 2009 $3,335,385 ATL 2010 $3,126,802 ATL 2011 $3,346,257 ATL 2012 $2,856,205 ATL 2013 $3,254,501 ATL 2014 $4,067,042 ATL 2015 $2,990,885 ATL 2016 $2,362,010 This might seem like an odd format. We might expect something that looks more like our grid structure: dat.NL &lt;- filter( Salaries, yearID &gt; 2010 &amp; lgID == &quot;NL&quot; ) %&gt;% droplevels() tapply( dat.NL$salary, INDEX=list(dat.NL$teamID, dat.NL$yearID), FUN=mean, na.rm=T ) %&gt;% pander 2011 2012 2013 2014 2015 2016 ARI 1986660 2733512 3004400 3763904 2034250 3363041 ATL 3346257 2856205 3254501 4067042 2990885 2362010 CHC NA NA NA NA NA 5312678 CHN 5001893 3392194 3867989 2426759 4138547 NA CIN 2531571 2935843 4256178 3864911 4187862 3066899 CLE NA NA NA 4500000 NA NA COL 3390310 2692054 2976363 3180117 3827544 3413487 FLO 2190154 NA NA NA NA NA HOU 2437724 2332731 NA NA NA NA LAD NA NA NA NA NA 6322525 LAN 3472967 3171453 6980069 6781706 7441103 NA MIA NA 4373259 1400079 1549515 2835688 2761222 MIL 2849911 3755921 3077881 3748778 3477586 2292508 NYM NA NA NA NA NA 4958857 NYN 4401752 3457555 1648278 3168777 3870667 NA PHI 5765879 5817965 6533200 5654530 4295885 2033793 PIT 1553345 2248286 2752214 2756357 3065259 3706387 SDN 1479650 1973025 2342339 2703061 4555435 NA SDP NA NA NA NA NA 3756475 SFG NA NA NA NA NA 6890151 SFN 4377716 3920689 5006441 5839649 6100056 NA SLN 3904947 3939317 3295004 4310464 4586212 NA STL NA NA NA NA NA 4614629 WAS 2201963 2695171 4548131 4399456 5365085 NA WSN NA NA NA NA NA 5448179 Later on we will look at the benefits of “tidy data”, but the basic idea is that you can “facet” your analysis easily when your groups are represented as factors instead of arranged as a table. For example, here is a time series graph that is faceted by teams: Salaries %&gt;% filter( yearID &gt; 2000 &amp; lgID == &quot;AL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( Mean_Player_Salary=mean(salary) ) -&gt; t1 qplot( data=t1, x=yearID, y=Mean_Player_Salary, geom=c(&quot;point&quot;, &quot;smooth&quot;) ) + facet_wrap( ~ teamID, ncol=5 ) Now you can quickly see that Detroit is the team that has raised salaries most aggressively. If we need to, we can easily convert a tidy dataset into something that looks like a table using the spread() function: Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=yearID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% na.omit() %&gt;% pander teamID yearID_2007 yearID_2008 yearID_2009 yearID_2010 yearID_2011 ARI $1,859,555 $2,364,383 $2,812,141 $2,335,314 $1,986,660 ATL $3,117,530 $3,412,189 $3,335,385 $3,126,802 $3,346,257 CHN $3,691,494 $4,383,179 $5,392,360 $5,429,963 $5,001,893 CIN $2,210,483 $2,647,061 $3,198,196 $2,760,059 $2,531,571 COL $2,078,500 $2,640,596 $2,785,222 $2,904,379 $3,390,310 FLO $984,097 $660,955 $1,315,500 $2,112,212 $2,190,154 HOU $3,250,333 $3,293,719 $3,814,682 $3,298,411 $2,437,724 LAN $3,739,811 $4,089,260 $4,016,584 $3,531,778 $3,472,967 MIL $2,629,130 $2,790,948 $3,083,942 $2,796,837 $2,849,911 NYN $3,841,055 $4,593,113 $5,334,785 $4,800,819 $4,401,752 PHI $2,980,940 $3,495,710 $4,185,335 $5,068,871 $5,765,879 PIT $1,427,327 $1,872,684 $1,872,808 $1,294,185 $1,553,345 SDN $2,235,022 $2,376,697 $1,604,952 $1,453,819 $1,479,650 SFN $3,469,964 $2,641,190 $2,965,230 $3,522,905 $4,377,716 SLN $3,224,529 $3,018,923 $3,278,830 $3,741,630 $3,904,947 WAS $1,319,554 $1,895,207 $2,140,286 $2,046,667 $2,201,963 Salaries %&gt;% filter( yearID &gt; 2006 &amp; lgID == &quot;NL&quot; ) %&gt;% group_by( teamID, yearID ) %&gt;% summarize( mean = dollar(mean(salary)) ) %&gt;% spread( key=teamID, value=mean, sep=&quot;_&quot; ) %&gt;% select( 1:6 ) %&gt;% pander yearID teamID_ARI teamID_ATL teamID_CHC teamID_CHN teamID_CIN 2007 $1,859,555 $3,117,530 NA $3,691,494 $2,210,483 2008 $2,364,383 $3,412,189 NA $4,383,179 $2,647,061 2009 $2,812,141 $3,335,385 NA $5,392,360 $3,198,196 2010 $2,335,314 $3,126,802 NA $5,429,963 $2,760,059 2011 $1,986,660 $3,346,257 NA $5,001,893 $2,531,571 2012 $2,733,512 $2,856,205 NA $3,392,194 $2,935,843 2013 $3,004,400 $3,254,501 NA $3,867,989 $4,256,178 2014 $3,763,904 $4,067,042 NA $2,426,759 $3,864,911 2015 $2,034,250 $2,990,885 NA $4,138,547 $4,187,862 2016 $3,363,041 $2,362,010 $5,312,678 NA $3,066,899 "]
]
